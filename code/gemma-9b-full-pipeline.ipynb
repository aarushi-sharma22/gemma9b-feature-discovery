{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjcSLtbWTkmSpGA4CiS8mE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarushi-sharma22/gemma9b-feature-discovery/blob/main/code/gemma-9b-full-pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQd7zThvo9e6"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "\n",
        "!pip install -q transformer_lens sae_lens torch transformers accelerate\n",
        "!pip install -q pandas numpy scipy scikit-learn matplotlib seaborn tqdm\n",
        "!pip install -q statsmodels\n",
        "\n",
        "print(\"Installation complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and setup\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Optional, Any, Set\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr, mannwhitneyu, kruskal, wilcoxon\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "from transformer_lens import HookedTransformer\n",
        "from sae_lens import SAE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "4YBtX0YjpCHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data mappings and file paths\n",
        "\n",
        "import os\n",
        "\n",
        "COUNTRY_MAP = {\n",
        "    \"AT\": \"Austria\", \"BE\": \"Belgium\", \"CZ\": \"Czechia\",\n",
        "    \"FI\": \"Finland\", \"FR\": \"France\", \"GB\": \"United Kingdom\",\n",
        "    \"HU\": \"Hungary\", \"IS\": \"Iceland\", \"PL\": \"Poland\",\n",
        "    \"PT\": \"Portugal\", \"SI\": \"Slovenia\"\n",
        "}\n",
        "\n",
        "GENDER_MAP = {1: \"man\", 2: \"woman\", 9: None}\n",
        "\n",
        "INCOME_MAP = {\n",
        "    1: \"living comfortably on present income\",\n",
        "    2: \"coping on present income\",\n",
        "    3: \"finding it difficult on present income\",\n",
        "    4: \"finding it very difficult on present income\",\n",
        "    7: None, 8: None, 9: None\n",
        "}\n",
        "\n",
        "EDUCATION_MAP = {\n",
        "    1: \"less than lower secondary education\",\n",
        "    2: \"lower secondary education\",\n",
        "    3: \"lower tier upper secondary education\",\n",
        "    4: \"upper tier upper secondary education\",\n",
        "    5: \"advanced vocational or sub-degree education\",\n",
        "    6: \"a bachelor's degree\",\n",
        "    7: \"a master's degree or higher\",\n",
        "    0: None, 55: None, 77: None, 88: None, 99: None,\n",
        "}\n",
        "\n",
        "EDUCATION_TEXT = {\n",
        "    1: \"You did not complete high school.\",\n",
        "    2: \"You completed middle school.\",\n",
        "    3: \"You have some high school education.\",\n",
        "    4: \"You completed high school.\",\n",
        "    5: \"You have vocational or technical training.\",\n",
        "    6: \"You have a bachelor's degree.\",\n",
        "    7: \"You have a master's degree or higher.\",\n",
        "}\n",
        "\n",
        "# Behavioural self-report items excluded from normative analysis\n",
        "BEHAVIOR_QUESTIONS = {\"w4hq17\", \"w4hq20\"}\n",
        "\n",
        "def get_domain(question_id: str) -> str:\n",
        "    \"\"\"Map question ID prefix to thematic domain.\"\"\"\n",
        "    qid = question_id.lower()\n",
        "    if qid.startswith(\"w4g\"):\n",
        "        return \"climate\"\n",
        "    elif qid.startswith(\"w4h\"):\n",
        "        return \"health\"\n",
        "    elif qid.startswith(\"w4d\"):\n",
        "        return \"digital\"\n",
        "    elif qid.startswith(\"w4e\"):\n",
        "        return \"equality\"\n",
        "    return \"unknown\"\n",
        "\n",
        "# File paths — update these for your environment\n",
        "DEMOGRAPHICS_FILE = \"/content/stratified_sample_200_blank.csv\"\n",
        "CODEBOOK_FILE = \"/content/codebook_updated.json\"\n",
        "\n",
        "assert os.path.exists(DEMOGRAPHICS_FILE), f\"Not found: {DEMOGRAPHICS_FILE}\"\n",
        "assert os.path.exists(CODEBOOK_FILE), f\"Not found: {CODEBOOK_FILE}\"\n",
        "\n",
        "print(f\"Demographics: {DEMOGRAPHICS_FILE}\")\n",
        "print(f\"Codebook: {CODEBOOK_FILE}\")\n",
        "print(\"Mappings ready\")"
      ],
      "metadata": {
        "id": "uvGYL-l6pEue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face authentication\n",
        "\n",
        "import os\n",
        "from huggingface_hub import login, whoami, model_info, repo_info\n",
        "\n",
        "HF_TOKEN = \"YOUR_TOKEN_HERE\"  # replace with your token, or use getpass\n",
        "\n",
        "login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "try:\n",
        "    info = whoami()\n",
        "    print(f\"Authenticated as: {info.get('name', 'Unknown')}\")\n",
        "\n",
        "    mi = model_info(\"google/gemma-2-9b-it\")\n",
        "    print(\"Access to Gemma-2-9B-IT confirmed\")\n",
        "\n",
        "    sae_info = repo_info(\"google/gemma-scope-9b-pt-res\")\n",
        "    print(\"Access to gemma-scope SAE repo confirmed\")\n",
        "\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    if \"gemma\" in error_msg.lower():\n",
        "        raise RuntimeError(\n",
        "            f\"Gemma access denied: {e}\\n\"\n",
        "            f\"Accept the license at: https://huggingface.co/google/gemma-2-9b-it\"\n",
        "        )\n",
        "    else:\n",
        "        raise RuntimeError(f\"Auth failed: {e}\")\n",
        "\n",
        "print(\"\\nAll access verified\")"
      ],
      "metadata": {
        "id": "taucCJRypTgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration and model/SAE loading\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Verified L0 values per layer — confirmed via HuggingFace API\n",
        "# repo: \"pt\" = gemma-scope-9b-pt-res, \"it\" = gemma-scope-9b-it-res\n",
        "LAYER_SAE_CONFIG = {\n",
        "    5:  {\"l0\": 77,  \"repo\": \"pt\", \"depth_pct\": 12},\n",
        "    9:  {\"l0\": 51,  \"repo\": \"pt\", \"depth_pct\": 22},\n",
        "    14: {\"l0\": 67,  \"repo\": \"pt\", \"depth_pct\": 34},\n",
        "    18: {\"l0\": 71,  \"repo\": \"pt\", \"depth_pct\": 44},\n",
        "    20: {\"l0\": 47,  \"repo\": \"it\", \"depth_pct\": 49},  # IT SAE\n",
        "    27: {\"l0\": 65,  \"repo\": \"pt\", \"depth_pct\": 66},\n",
        "    32: {\"l0\": 61,  \"repo\": \"pt\", \"depth_pct\": 78},\n",
        "    36: {\"l0\": 61,  \"repo\": \"pt\", \"depth_pct\": 88},\n",
        "}\n",
        "\n",
        "LAYER_L0_MAP = {layer: cfg[\"l0\"] for layer, cfg in LAYER_SAE_CONFIG.items()}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CausalExtractionConfig:\n",
        "    \"\"\"Configuration for the expanded depth analysis across 8 layers.\"\"\"\n",
        "\n",
        "    model_name: str = \"google/gemma-2-9b-it\"\n",
        "    sae_repo_pt: str = \"google/gemma-scope-9b-pt-res\"\n",
        "    sae_repo_it: str = \"google/gemma-scope-9b-it-res\"\n",
        "    sae_width: str = \"16k\"\n",
        "    d_sae: int = 16384\n",
        "\n",
        "    candidate_layers: List[int] = field(\n",
        "        default_factory=lambda: [5, 9, 14, 18, 20, 27, 32, 36]\n",
        "    )\n",
        "    original_layers: List[int] = field(\n",
        "        default_factory=lambda: [18, 27, 36]\n",
        "    )\n",
        "    new_layers: List[int] = field(\n",
        "        default_factory=lambda: [5, 9, 14, 20, 32]\n",
        "    )\n",
        "\n",
        "    n_features: int = 50\n",
        "    min_effect_threshold: float = 0.3\n",
        "    batch_size: int = 16\n",
        "    dtype: torch.dtype = torch.bfloat16\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    output_dir: Path = field(default_factory=lambda: Path(\"./outputs_gemma_replication\"))\n",
        "\n",
        "    def hook_name(self, layer: int) -> str:\n",
        "        return f\"blocks.{layer}.hook_resid_post\"\n",
        "\n",
        "\n",
        "config = CausalExtractionConfig()\n",
        "config.output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Configuration\")\n",
        "print(f\"  Model: {config.model_name}\")\n",
        "print(f\"  Layers: {config.candidate_layers}\")\n",
        "for layer in config.candidate_layers:\n",
        "    cfg = LAYER_SAE_CONFIG[layer]\n",
        "    tag = \" (IT SAE)\" if cfg[\"repo\"] == \"it\" else \"\"\n",
        "    print(f\"    L{layer:>2} — {cfg['depth_pct']}% depth, L0={cfg['l0']}{tag}\")\n",
        "\n",
        "# Load model\n",
        "model = HookedTransformer.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=config.dtype,\n",
        "    device=config.device,\n",
        ")\n",
        "print(f\"\\nModel loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}\")\n",
        "\n",
        "tokenizer = model.tokenizer\n",
        "d_model = model.cfg.d_model\n",
        "\n",
        "\n",
        "class GemmaScopeSAE:\n",
        "    \"\"\"Gemma-Scope SAE with JumpReLU activation.\n",
        "\n",
        "    Loads from a verified (layer, L0, repo) combination.\n",
        "    \"\"\"\n",
        "\n",
        "    REPO_IDS = {\n",
        "        \"pt\": \"google/gemma-scope-9b-pt-res\",\n",
        "        \"it\": \"google/gemma-scope-9b-it-res\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, layer: int, device: str = \"cuda\"):\n",
        "        self.layer = layer\n",
        "        self.device = device\n",
        "\n",
        "        if layer not in LAYER_SAE_CONFIG:\n",
        "            raise ValueError(\n",
        "                f\"Layer {layer} not in config. Available: {list(LAYER_SAE_CONFIG.keys())}\"\n",
        "            )\n",
        "\n",
        "        cfg = LAYER_SAE_CONFIG[layer]\n",
        "        l0 = cfg[\"l0\"]\n",
        "        repo_id = self.REPO_IDS[cfg[\"repo\"]]\n",
        "        sae_path = f\"layer_{layer}/width_16k/average_l0_{l0}/params.npz\"\n",
        "        print(f\"  Loading: {repo_id} / {sae_path}\")\n",
        "\n",
        "        params_file = hf_hub_download(\n",
        "            repo_id=repo_id, filename=sae_path, repo_type=\"model\"\n",
        "        )\n",
        "        params = np.load(params_file)\n",
        "\n",
        "        W_enc_raw = params['W_enc']\n",
        "        W_dec_raw = params['W_dec']\n",
        "\n",
        "        # Determine encoder orientation: need [d_model, d_sae] for x @ W_enc\n",
        "        if W_enc_raw.shape[0] == d_model:\n",
        "            self.encode_transpose = False\n",
        "        elif W_enc_raw.shape[1] == d_model:\n",
        "            self.encode_transpose = True\n",
        "        else:\n",
        "            raise ValueError(f\"W_enc shape {W_enc_raw.shape} incompatible with d_model={d_model}\")\n",
        "\n",
        "        self.W_enc = torch.tensor(W_enc_raw, dtype=torch.bfloat16, device=device)\n",
        "        self.W_dec = torch.tensor(W_dec_raw, dtype=torch.bfloat16, device=device)\n",
        "        self.b_enc = torch.tensor(params['b_enc'], dtype=torch.bfloat16, device=device)\n",
        "        self.b_dec = torch.tensor(params['b_dec'], dtype=torch.bfloat16, device=device)\n",
        "\n",
        "        if 'threshold' in params:\n",
        "            self.threshold = torch.tensor(params['threshold'], dtype=torch.bfloat16, device=device)\n",
        "        elif 'log_threshold' in params:\n",
        "            self.threshold = torch.exp(\n",
        "                torch.tensor(params['log_threshold'], dtype=torch.bfloat16, device=device)\n",
        "            )\n",
        "        else:\n",
        "            self.threshold = None\n",
        "            print(f\"    No threshold found, falling back to ReLU\")\n",
        "\n",
        "        if self.encode_transpose:\n",
        "            self.d_in = self.W_enc.shape[1]\n",
        "            self.d_sae = self.W_enc.shape[0]\n",
        "        else:\n",
        "            self.d_in = self.W_enc.shape[0]\n",
        "            self.d_sae = self.W_enc.shape[1]\n",
        "\n",
        "        self.l0 = l0\n",
        "        self.repo_type = cfg[\"repo\"]\n",
        "\n",
        "        assert self.d_in == d_model, f\"SAE d_in={self.d_in} != d_model={d_model}\"\n",
        "        print(f\"    Loaded: d_sae={self.d_sae}, L0={l0}, repo={cfg['repo'].upper()}\")\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Encode activations to sparse features.\n",
        "\n",
        "        Note: we skip the standard b_dec subtraction before encoding.\n",
        "        When applying PT SAEs to IT activations, subtracting b_dec (which was\n",
        "        learned to center PT activations) actively decenters the IT activations,\n",
        "        degrading reconstruction (NMSE 0.50 vs 0.17 without subtraction).\n",
        "        \"\"\"\n",
        "        x = x.to(self.W_enc.dtype)\n",
        "        if self.encode_transpose:\n",
        "            pre_acts = x @ self.W_enc.T + self.b_enc\n",
        "        else:\n",
        "            pre_acts = x @ self.W_enc + self.b_enc\n",
        "        if self.threshold is not None:\n",
        "            return torch.where(pre_acts > self.threshold, pre_acts, torch.zeros_like(pre_acts))\n",
        "        else:\n",
        "            return torch.relu(pre_acts)\n",
        "\n",
        "    def decode(self, acts: torch.Tensor) -> torch.Tensor:\n",
        "        acts = acts.to(self.W_dec.dtype)\n",
        "        if self.W_dec.shape[0] == self.d_sae:\n",
        "            return acts @ self.W_dec + self.b_dec\n",
        "        else:\n",
        "            return acts @ self.W_dec.T + self.b_dec\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple:\n",
        "        acts = self.encode(x)\n",
        "        recon = self.decode(acts)\n",
        "        return recon, acts\n",
        "\n",
        "\n",
        "class GemmaSAEManager:\n",
        "    \"\"\"Loads one SAE at a time to conserve VRAM.\"\"\"\n",
        "\n",
        "    def __init__(self, device: str = \"cuda\"):\n",
        "        self.device = device\n",
        "        self._current_sae = None\n",
        "        self._current_layer = None\n",
        "\n",
        "    def load_sae(self, layer: int) -> GemmaScopeSAE:\n",
        "        if self._current_layer == layer and self._current_sae is not None:\n",
        "            return self._current_sae\n",
        "        self.unload()\n",
        "        self._current_sae = GemmaScopeSAE(layer, self.device)\n",
        "        self._current_layer = layer\n",
        "        return self._current_sae\n",
        "\n",
        "    def unload(self):\n",
        "        if self._current_sae is not None:\n",
        "            del self._current_sae\n",
        "            self._current_sae = None\n",
        "            self._current_layer = None\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# Quick verification\n",
        "sae_manager = GemmaSAEManager(device=config.device)\n",
        "\n",
        "print(\"\\nVerifying SAE loading...\")\n",
        "test_sae = sae_manager.load_sae(18)\n",
        "print(\"  Layer 18 (PT) OK\")\n",
        "\n",
        "test_sae = sae_manager.load_sae(20)\n",
        "print(\"  Layer 20 (IT) OK\")\n",
        "\n",
        "sae_manager.unload()\n",
        "\n",
        "print(f\"\\nSetup complete\")\n",
        "print(f\"  Model: {config.model_name}\")\n",
        "print(f\"  Layers: {config.candidate_layers}\")\n",
        "print(f\"  Output: {config.output_dir}\")"
      ],
      "metadata": {
        "id": "Yjs8PEoRpxHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vocabulary sensitivity check for top-K feature selection.\n",
        "\n",
        "Tests whether identified features track demographic semantics or surface lexical form,\n",
        "by comparing SAE feature responses across 3 paraphrase variants per demographic.\n",
        "\n",
        "For each demographic x domain, we compute:\n",
        "  - Spearman correlation of mean diff vectors across wordings\n",
        "  - Jaccard overlap of top-50 features\n",
        "  - Cosine similarity of mean diff vectors\n",
        "\n",
        "High correlations (>0.7) and overlap (>0.3) indicate features reflect demographics,\n",
        "not prompt surface form. Run in the same session as the main extraction notebook.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import cosine as cosine_dist\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# --- Config ---\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "TARGET_LAYER = 36\n",
        "HOOK_NAME = f\"blocks.{TARGET_LAYER}.hook_resid_post\"\n",
        "\n",
        "# Use validation respondents to avoid contaminating the training set\n",
        "N_SUBSAMPLE_RESPONDENTS = 30\n",
        "N_FEATURES_CHECK = 50\n",
        "N_VOCAB_VARIANTS = 3\n",
        "\n",
        "# Subset of questions spanning all five domains\n",
        "QUESTION_SUBSET = [\n",
        "    \"w4gq1\", \"w4gq2\", \"w4gq3\", \"w4gq10_a\",       # climate\n",
        "    \"w4hq1\", \"w4hq4\", \"w4hq8\", \"w4hq15\",           # health\n",
        "    \"w4dq8\", \"w4dq11\", \"w4dq15\", \"w4dq16\",          # digital\n",
        "    \"w4eq1\", \"w4eq2\", \"w4eq3\", \"w4eq13\",             # economy\n",
        "    \"w4sq1\", \"w4sq6\", \"w4sq14\", \"w4sq17\",            # values\n",
        "]\n",
        "\n",
        "INPUT_DIR = Path(\"./outputs_gemma_replication\")\n",
        "OUTPUT_DIR = INPUT_DIR / \"vocab_sensitivity\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "CODEBOOK_FILE = Path(\"./codebook_updated.json\")\n",
        "DEMOGRAPHICS_FILE = Path(\"./stratified_sample_200_blank.csv\")\n",
        "\n",
        "ALL_DEMOGRAPHICS = ['income', 'age', 'gender', 'education', 'vote']\n",
        "\n",
        "\n",
        "# --- Alternative phrasings (3 variants per demographic, per condition) ---\n",
        "\n",
        "VOCAB_VARIANTS = {\n",
        "    'income': {\n",
        "        'prompts_a': [\n",
        "            \"You are financially wealthy. You own multiple properties, have substantial savings and diverse investments, and never worry about money. You can afford luxuries and expensive experiences without thinking twice about the cost.\",\n",
        "            \"You have a very high income and significant personal wealth. Your financial situation is extremely comfortable — you have extensive savings, property holdings, and investment portfolios. Money has never been a source of concern for you.\",\n",
        "            \"You are rich. You have more money than you could ever spend, with large savings, multiple homes, and a portfolio of investments. Financial worries are completely foreign to you.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are financially poor. You struggle to afford rent, have accumulated significant debt, and constantly worry about paying for basic necessities. Money is a persistent source of stress and limits your daily choices.\",\n",
        "            \"You have a very low income and almost no savings. Making ends meet is a constant struggle — you worry about rent, bills, and whether you can afford groceries. Debt weighs heavily on you every day.\",\n",
        "            \"You are poor. You live paycheck to paycheck with mounting debts, and basic expenses like housing and food are a constant source of anxiety. Financial security feels impossibly out of reach.\",\n",
        "        ],\n",
        "    },\n",
        "    'age': {\n",
        "        'prompts_a': [\n",
        "            \"You are 75 years old. You are retired after a long career, have lived through decades of social and technological change, and remember life before computers and mobile phones. You have accumulated a lifetime of experiences.\",\n",
        "            \"You are an elderly person of 75. You've been retired for years after a full working life. You witnessed the entire digital revolution unfold and remember a world without the internet. Decades of experience have shaped your worldview.\",\n",
        "            \"You are 75 and in your retirement years. You've seen enormous societal changes over your long life — from a pre-digital world to today. Your career is behind you and you draw on a lifetime of accumulated wisdom.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are 22 years old. You are early in your career, recently finished education, and grew up as a digital native with smartphones and social media from childhood. You have your whole adult life ahead of you.\",\n",
        "            \"You are a young adult of 22, just starting out in your professional life. Technology has been part of your world since birth — you can't remember life without the internet. Your future stretches ahead with many possibilities.\",\n",
        "            \"You are 22 and recently entered the workforce. Smartphones and social media have been constant companions throughout your life. You're at the very beginning of your adult journey with everything still ahead.\",\n",
        "        ],\n",
        "    },\n",
        "    'gender': {\n",
        "        'prompts_a': [\n",
        "            \"You are a man. You have lived your entire life experiencing society as a man, with male social expectations, relationships, and career experiences. Your perspective has been shaped by masculine social norms and experiences.\",\n",
        "            \"You are male. Throughout your life, you have navigated the world as a man — from boyhood through adulthood. Societal expectations of masculinity have influenced your relationships, career path, and daily interactions.\",\n",
        "            \"You are a man who has experienced life through a male lens. Masculine norms and expectations have shaped how you relate to others, how you approach work, and how society has treated you throughout your life.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are a woman. You have lived your entire life experiencing society as a woman, with female social expectations, relationships, and career experiences. Your perspective has been shaped by feminine social norms and experiences.\",\n",
        "            \"You are female. Throughout your life, you have navigated the world as a woman — from girlhood through adulthood. Societal expectations of femininity have influenced your relationships, career path, and daily interactions.\",\n",
        "            \"You are a woman who has experienced life through a female lens. Feminine norms and expectations have shaped how you relate to others, how you approach work, and how society has treated you throughout your life.\",\n",
        "        ],\n",
        "    },\n",
        "    'education': {\n",
        "        'prompts_a': [\n",
        "            \"You have a PhD and spent over 20 years in formal education. You work in a professional field requiring advanced expertise and critical analysis. Academic thinking and research methodology are second nature to you.\",\n",
        "            \"You are highly educated with a doctoral degree. After more than two decades of academic study, you work in a field that demands deep expertise. Rigorous analytical thinking and scholarly methods come naturally to you.\",\n",
        "            \"You hold a PhD after 20+ years of formal education. Your career requires advanced intellectual skills and critical reasoning. You are thoroughly trained in research methods and think naturally in academic frameworks.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You did not complete high school. You left formal education early and learned through practical work experience rather than academic study. You've built knowledge through hands-on learning and real-world problem solving.\",\n",
        "            \"You have minimal formal education, having left school before finishing secondary level. What you know, you learned through working and doing — practical experience rather than textbooks shaped your understanding of the world.\",\n",
        "            \"You dropped out before completing high school. Your education came from the school of life — years of hands-on work and practical problem-solving rather than formal academic training.\",\n",
        "        ],\n",
        "    },\n",
        "    'vote': {\n",
        "        'prompts_a': [\n",
        "            \"You are a regular voter who participates in every election. You believe voting is a civic duty and fundamental to democracy. You stay informed about candidates and issues, and always make time to cast your ballot.\",\n",
        "            \"You vote consistently in every election without exception. Democratic participation is a core value for you — you research candidates, follow political developments, and consider voting an essential civic responsibility.\",\n",
        "            \"You are a committed voter who never misses an election. Casting your ballot is something you see as a fundamental obligation of citizenship. You keep up with political issues and candidates as a matter of principle.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are a non-voter who doesn't participate in elections. You feel disconnected from the political system and don't believe your vote makes a difference. Electoral politics seems distant from your daily life concerns.\",\n",
        "            \"You don't vote in elections. The political system feels irrelevant to your life — you see no point in casting a ballot when it won't change anything. Politics and elections are not things you pay attention to.\",\n",
        "            \"You have chosen not to participate in elections. Voting feels meaningless to you — the political system seems disconnected from the issues that actually affect your daily life, and you don't believe one vote matters.\",\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# --- Token handling and EV computation ---\n",
        "\n",
        "def _build_token_info():\n",
        "    info = {}\n",
        "    for i in range(0, 11):\n",
        "        encoded = tokenizer.encode(str(i), add_special_tokens=False)\n",
        "        if len(encoded) == 1:\n",
        "            info[i] = {'type': 'single', 'token_id': encoded[0]}\n",
        "        else:\n",
        "            info[i] = {\n",
        "                'type': 'multi',\n",
        "                'token_ids': encoded,\n",
        "                'first_token_id': encoded[0],\n",
        "                'second_token_id': encoded[1],\n",
        "            }\n",
        "    return info\n",
        "\n",
        "\n",
        "def compute_response_metrics(logits, scale_min, scale_max):\n",
        "    \"\"\"Compute expected value over the model's response distribution.\n",
        "\n",
        "    For 11-point scales (0-10), the value '10' shares its first token with '1'.\n",
        "    We split the probability mass equally between them. See paper Section 4.2.\n",
        "    \"\"\"\n",
        "    has_multi = scale_max >= 10 and TOKEN_INFO.get(10, {}).get('type') == 'multi'\n",
        "\n",
        "    if not has_multi:\n",
        "        tokens = [TOKEN_INFO[i]['token_id'] for i in range(scale_min, scale_max + 1)]\n",
        "        logits_subset = logits[tokens].float()\n",
        "        probs = F.softmax(logits_subset, dim=0).cpu().numpy()\n",
        "        values = np.arange(scale_min, scale_max + 1)\n",
        "        return float(np.dot(values, probs))\n",
        "\n",
        "    single_max = min(scale_max, 9)\n",
        "    single_tokens = [TOKEN_INFO[i]['token_id'] for i in range(scale_min, single_max + 1)]\n",
        "    all_logits = logits[single_tokens].float()\n",
        "    all_probs = F.softmax(all_logits, dim=0).cpu().numpy()\n",
        "\n",
        "    values = list(range(scale_min, single_max + 1))\n",
        "    probs_list = list(all_probs)\n",
        "\n",
        "    # Split shared first-token probability between 1 and 10\n",
        "    if 1 in values:\n",
        "        idx_of_1 = values.index(1)\n",
        "        p1_raw = probs_list[idx_of_1]\n",
        "        probs_list[idx_of_1] = p1_raw / 2.0\n",
        "        values.append(10)\n",
        "        probs_list.append(p1_raw / 2.0)\n",
        "    else:\n",
        "        values.append(10)\n",
        "        probs_list.append(0.0)\n",
        "\n",
        "    values = np.array(values)\n",
        "    probs = np.array(probs_list)\n",
        "    probs = probs / probs.sum()\n",
        "    return float(np.dot(values, probs))\n",
        "\n",
        "\n",
        "# --- Prompt construction ---\n",
        "# These mirror the main extraction pipeline's prompt format.\n",
        "\n",
        "def get_real_gender_description(gndr_code):\n",
        "    if gndr_code == 1:\n",
        "        return \"You are a man.\"\n",
        "    elif gndr_code == 2:\n",
        "        return \"You are a woman.\"\n",
        "    return \"You are an adult.\"\n",
        "\n",
        "\n",
        "def get_real_age_description(age):\n",
        "    if pd.isna(age) or age > 900:\n",
        "        return \"You are an adult.\"\n",
        "    age = int(age)\n",
        "    if age < 25: return f\"You are {age} years old, a young adult just starting out.\"\n",
        "    elif age < 35: return f\"You are {age} years old, in your late twenties to early thirties.\"\n",
        "    elif age < 45: return f\"You are {age} years old, in your mid-thirties to early forties.\"\n",
        "    elif age < 55: return f\"You are {age} years old, in middle age.\"\n",
        "    elif age < 65: return f\"You are {age} years old, in your late fifties to early sixties.\"\n",
        "    elif age < 75: return f\"You are {age} years old, in your sixties to early seventies.\"\n",
        "    return f\"You are {age} years old, a senior citizen.\"\n",
        "\n",
        "\n",
        "def get_real_education_description(eisced_code):\n",
        "    d = {\n",
        "        1: \"You have less than lower secondary education. You left school early.\",\n",
        "        2: \"You completed lower secondary education (middle school equivalent).\",\n",
        "        3: \"You completed upper secondary education (high school).\",\n",
        "        4: \"You have post-secondary non-tertiary education (vocational training).\",\n",
        "        5: \"You have a short-cycle tertiary degree (associate's or similar).\",\n",
        "        6: \"You have a bachelor's degree from university.\",\n",
        "        7: \"You have a master's degree or higher (including PhD).\",\n",
        "    }\n",
        "    return d.get(eisced_code, \"You have completed some formal education.\")\n",
        "\n",
        "\n",
        "def get_real_income_description(hincfel_code):\n",
        "    d = {\n",
        "        1: \"You live comfortably on your current income.\",\n",
        "        2: \"You cope on your current income.\",\n",
        "        3: \"You find it difficult on your current income.\",\n",
        "        4: \"You find it very difficult on your current income.\",\n",
        "    }\n",
        "    return d.get(hincfel_code, \"You have a moderate income.\")\n",
        "\n",
        "\n",
        "def get_real_vote_description(vote_code):\n",
        "    d = {\n",
        "        1: \"You voted in the last national election.\",\n",
        "        2: \"You did not vote in the last national election.\",\n",
        "        3: \"You were not eligible to vote in the last national election.\",\n",
        "    }\n",
        "    return d.get(vote_code, \"You have varying levels of political engagement.\")\n",
        "\n",
        "\n",
        "def build_context(row, tested_demo, tested_prompt, country):\n",
        "    parts = []\n",
        "\n",
        "    if tested_demo == 'gender':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_gender_description(row.get('gndr', 0)))\n",
        "\n",
        "    if tested_demo == 'age':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        age_val = row.get('agea', row.get('age', 999))\n",
        "        parts.append(get_real_age_description(age_val))\n",
        "\n",
        "    parts.append(f\"You live in {country}.\")\n",
        "\n",
        "    if tested_demo == 'education':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_education_description(row.get('eisced', 0)))\n",
        "\n",
        "    if tested_demo == 'income':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_income_description(row.get('hincfel', 0)))\n",
        "\n",
        "    if tested_demo == 'vote':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_vote_description(row.get('vote', 0)))\n",
        "\n",
        "    return \" \".join(parts)\n",
        "\n",
        "\n",
        "def format_gemma_prompt(context, question, scale_info):\n",
        "    user_content = f\"{context}\\n\\n{question} {scale_info}\\n\\nRESPOND WITH ONLY A SINGLE NUMBER.\"\n",
        "    return f\"<start_of_turn>user\\n{user_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "\n",
        "# --- Load data ---\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "if CODEBOOK_FILE.exists():\n",
        "    with open(CODEBOOK_FILE) as f:\n",
        "        codebook = json.load(f)\n",
        "    codebook_lookup = {v['variable_name']: v for v in codebook['variables']}\n",
        "    print(f\"  Codebook: {len(codebook_lookup)} variables\")\n",
        "else:\n",
        "    codebook_lookup = {}\n",
        "    print(f\"  Codebook not found at {CODEBOOK_FILE}\")\n",
        "\n",
        "prompts_df = pd.read_parquet(INPUT_DIR / 'prompts_selection.parquet')\n",
        "print(f\"  Prompts: {len(prompts_df):,}\")\n",
        "\n",
        "# Build question metadata from codebook\n",
        "QUESTION_INFO = {}\n",
        "for qid in QUESTION_SUBSET:\n",
        "    matches = prompts_df[prompts_df['question_id'] == qid]\n",
        "    if len(matches) == 0:\n",
        "        print(f\"  Warning: question {qid} not found in prompts\")\n",
        "        continue\n",
        "    row = matches.iloc[0]\n",
        "    q_var = codebook_lookup.get(qid, {})\n",
        "    q_text = q_var.get('question', qid)\n",
        "\n",
        "    if 'scale_range' in q_var:\n",
        "        min_label = q_var['scale_range'].get('min_label', str(row['scale_min']))\n",
        "        max_label = q_var['scale_range'].get('max_label', str(row['scale_max']))\n",
        "    elif 'values' in q_var:\n",
        "        labels = {v['code']: v['label'] for v in q_var['values'] if isinstance(v['code'], int)}\n",
        "        min_label = labels.get(int(row['scale_min']), str(row['scale_min']))\n",
        "        max_label = labels.get(int(row['scale_max']), str(row['scale_max']))\n",
        "    else:\n",
        "        min_label = str(row['scale_min'])\n",
        "        max_label = str(row['scale_max'])\n",
        "\n",
        "    QUESTION_INFO[qid] = {\n",
        "        'text': q_text,\n",
        "        'scale_min': int(row['scale_min']),\n",
        "        'scale_max': int(row['scale_max']),\n",
        "        'min_label': min_label,\n",
        "        'max_label': max_label,\n",
        "        'domain': row['domain'],\n",
        "    }\n",
        "\n",
        "print(f\"  Questions for check: {len(QUESTION_INFO)}\")\n",
        "\n",
        "# Load demographics and select validation respondents\n",
        "if not DEMOGRAPHICS_FILE.exists():\n",
        "    raise FileNotFoundError(f\"Demographics file not found: {DEMOGRAPHICS_FILE}\")\n",
        "\n",
        "demographics_df = pd.read_csv(DEMOGRAPHICS_FILE)\n",
        "print(f\"  Demographics: {len(demographics_df)} rows\")\n",
        "\n",
        "id_col = None\n",
        "for col in ['idno', 'respondent_id', 'id', 'ID']:\n",
        "    if col in demographics_df.columns:\n",
        "        id_col = col\n",
        "        break\n",
        "if id_col is None:\n",
        "    id_col = demographics_df.columns[0]\n",
        "    print(f\"  No standard ID column, using: {id_col}\")\n",
        "\n",
        "demographics_df = demographics_df.drop_duplicates(subset=id_col, keep='first')\n",
        "\n",
        "split_file = INPUT_DIR / 'train_val_split.json'\n",
        "if split_file.exists():\n",
        "    with open(split_file) as f:\n",
        "        split = json.load(f)\n",
        "    val_ids = split.get('validation_ids', [])\n",
        "    if len(val_ids) > 0:\n",
        "        resp_df = demographics_df[demographics_df[id_col].isin(val_ids)]\n",
        "        print(f\"  Using {len(resp_df)} validation respondents\")\n",
        "    else:\n",
        "        resp_df = demographics_df.head(N_SUBSAMPLE_RESPONDENTS)\n",
        "        print(f\"  No validation IDs in split, using first {len(resp_df)}\")\n",
        "else:\n",
        "    resp_df = demographics_df.head(N_SUBSAMPLE_RESPONDENTS)\n",
        "    print(f\"  No split file, using first {len(resp_df)}\")\n",
        "\n",
        "if len(resp_df) == 0:\n",
        "    raise ValueError(\"No respondents found — check demographics file and split.\")\n",
        "\n",
        "resp_df = resp_df.rename(columns={id_col: 'idno'})\n",
        "TOKEN_INFO = _build_token_info()\n",
        "\n",
        "\n",
        "# --- Generate prompts for all 3 vocab variants ---\n",
        "\n",
        "print(f\"\\nGenerating prompts for {N_VOCAB_VARIANTS} vocab variants...\")\n",
        "\n",
        "all_prompts = []\n",
        "\n",
        "for _, row in resp_df.iterrows():\n",
        "    country = COUNTRY_MAP.get(row.get('cntry', ''), 'Europe')\n",
        "\n",
        "    for qid, q_info in QUESTION_INFO.items():\n",
        "        scale_str = (f\"({q_info['scale_min']} = {q_info['min_label']}, \"\n",
        "                     f\"{q_info['scale_max']} = {q_info['max_label']})\")\n",
        "\n",
        "        for demo_name in ALL_DEMOGRAPHICS:\n",
        "            for vocab_idx in range(N_VOCAB_VARIANTS):\n",
        "                for vt in ['a', 'b']:\n",
        "                    demo_prompt = VOCAB_VARIANTS[demo_name][f'prompts_{vt}'][vocab_idx]\n",
        "                    context = build_context(row, demo_name, demo_prompt, country)\n",
        "                    prompt = format_gemma_prompt(context, q_info['text'], scale_str)\n",
        "\n",
        "                    all_prompts.append({\n",
        "                        'respondent_id': row['idno'],\n",
        "                        'question_id': qid,\n",
        "                        'domain': q_info['domain'],\n",
        "                        'demographic': demo_name,\n",
        "                        'value_type': f'value_{vt}',\n",
        "                        'vocab_idx': vocab_idx,\n",
        "                        'scale_min': q_info['scale_min'],\n",
        "                        'scale_max': q_info['scale_max'],\n",
        "                        'prompt': prompt,\n",
        "                        'pair_key': f\"{row['idno']}_{qid}_{demo_name}_v{vocab_idx}\",\n",
        "                    })\n",
        "\n",
        "sensitivity_df = pd.DataFrame(all_prompts)\n",
        "n_per_vocab = len(sensitivity_df) // N_VOCAB_VARIANTS\n",
        "\n",
        "print(f\"  Total prompts: {len(sensitivity_df):,}\")\n",
        "print(f\"  Per variant: {n_per_vocab:,}\")\n",
        "print(f\"  Respondents: {len(resp_df)}, Questions: {len(QUESTION_INFO)}, \"\n",
        "      f\"Demographics: {len(ALL_DEMOGRAPHICS)}\")\n",
        "\n",
        "\n",
        "# --- Extract SAE activations ---\n",
        "\n",
        "print(f\"\\nExtracting SAE activations (Layer {TARGET_LAYER})...\")\n",
        "\n",
        "sae = sae_manager.load_sae(TARGET_LAYER)\n",
        "d_sae = sae.d_sae if hasattr(sae, 'd_sae') else 16384\n",
        "\n",
        "USE_TRANSFORMER_LENS = hasattr(model, 'run_with_cache')\n",
        "\n",
        "diff_storage = {}    # (vocab_idx, demo, domain) -> list of diff vectors\n",
        "activation_buffer = {}\n",
        "\n",
        "n = len(sensitivity_df)\n",
        "\n",
        "for batch_start in tqdm(range(0, n, BATCH_SIZE), desc=\"Extracting\"):\n",
        "    batch_end = min(batch_start + BATCH_SIZE, n)\n",
        "    batch_size = batch_end - batch_start\n",
        "\n",
        "    batch_rows = sensitivity_df.iloc[batch_start:batch_end]\n",
        "    batch_prompts = batch_rows['prompt'].tolist()\n",
        "\n",
        "    tokens = tokenizer(batch_prompts, return_tensors='pt',\n",
        "                       padding=True, truncation=True, max_length=512)\n",
        "    input_ids = tokens['input_ids'].to(DEVICE)\n",
        "    attention_mask = tokens['attention_mask'].to(DEVICE)\n",
        "    seq_lens = attention_mask.sum(dim=1) - 1\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        if USE_TRANSFORMER_LENS:\n",
        "            logits, cache = model.run_with_cache(\n",
        "                input_ids, names_filter=lambda n: HOOK_NAME in n\n",
        "            )\n",
        "            residuals = torch.stack([\n",
        "                cache[HOOK_NAME][j, seq_lens[j], :] for j in range(batch_size)\n",
        "            ]).float()\n",
        "            sae_acts = sae.encode(residuals).float().cpu().numpy()\n",
        "            del cache, residuals, logits\n",
        "        else:\n",
        "            activations = {}\n",
        "            def hook_fn(module, inp, out):\n",
        "                activations['h'] = out[0].detach() if isinstance(out, tuple) else out.detach()\n",
        "            hook = model.model.layers[TARGET_LAYER].register_forward_hook(hook_fn)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            hook.remove()\n",
        "            last_residuals = torch.stack([\n",
        "                activations['h'][j, seq_lens[j], :] for j in range(batch_size)\n",
        "            ]).float()\n",
        "            sae_acts = sae.encode(last_residuals).float().cpu().numpy()\n",
        "            del activations\n",
        "\n",
        "    # Match pairs and compute diffs\n",
        "    for j in range(batch_size):\n",
        "        idx = batch_start + j\n",
        "        row = batch_rows.iloc[j]\n",
        "        pk = row['pair_key']\n",
        "        vtype = row['value_type']\n",
        "\n",
        "        if pk not in activation_buffer:\n",
        "            activation_buffer[pk] = {\n",
        "                'demo': row['demographic'],\n",
        "                'domain': row['domain'],\n",
        "                'vocab_idx': row['vocab_idx'],\n",
        "            }\n",
        "        activation_buffer[pk][vtype] = sae_acts[j]\n",
        "\n",
        "        if 'value_a' in activation_buffer[pk] and 'value_b' in activation_buffer[pk]:\n",
        "            info = activation_buffer[pk]\n",
        "            key = (info['vocab_idx'], info['demo'], info['domain'])\n",
        "            if key not in diff_storage:\n",
        "                diff_storage[key] = []\n",
        "            diff_storage[key].append(info['value_a'] - info['value_b'])\n",
        "            del activation_buffer[pk]\n",
        "\n",
        "    if (batch_start // BATCH_SIZE) % 20 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "del sae\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "n_orphans = len(activation_buffer)\n",
        "if n_orphans > 0:\n",
        "    print(f\"  {n_orphans} orphan pairs (unmatched)\")\n",
        "activation_buffer.clear()\n",
        "\n",
        "print(f\"  Groups extracted: {len(diff_storage)}\")\n",
        "\n",
        "\n",
        "# --- Compute sensitivity metrics ---\n",
        "\n",
        "print(\"\\nComputing sensitivity metrics...\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for demo in ALL_DEMOGRAPHICS:\n",
        "    for domain in set(q['domain'] for q in QUESTION_INFO.values()):\n",
        "        mean_diffs = {}\n",
        "        for v_idx in range(N_VOCAB_VARIANTS):\n",
        "            key = (v_idx, demo, domain)\n",
        "            if key in diff_storage and len(diff_storage[key]) >= 5:\n",
        "                diffs = np.stack(diff_storage[key])\n",
        "                mean_diffs[v_idx] = np.mean(diffs, axis=0)\n",
        "\n",
        "        if len(mean_diffs) < 2:\n",
        "            continue\n",
        "\n",
        "        top_k = {}\n",
        "        for v_idx, md in mean_diffs.items():\n",
        "            top_k[v_idx] = set(np.argsort(np.abs(md))[::-1][:N_FEATURES_CHECK].tolist())\n",
        "\n",
        "        for vi, vj in [(0, 1), (0, 2), (1, 2)]:\n",
        "            if vi not in mean_diffs or vj not in mean_diffs:\n",
        "                continue\n",
        "\n",
        "            md_i, md_j = mean_diffs[vi], mean_diffs[vj]\n",
        "\n",
        "            rho, p_rho = spearmanr(md_i, md_j)\n",
        "            cos_sim = 1.0 - cosine_dist(md_i, md_j)\n",
        "            pearson_r = np.corrcoef(md_i, md_j)[0, 1]\n",
        "            jaccard = len(top_k[vi] & top_k[vj]) / len(top_k[vi] | top_k[vj])\n",
        "            overlap_count = len(top_k[vi] & top_k[vj])\n",
        "\n",
        "            # Rank correlation among the union of top features\n",
        "            union_feats = list(top_k[vi] | top_k[vj])\n",
        "            ranks_i = np.argsort(np.abs(md_i))[::-1]\n",
        "            ranks_j = np.argsort(np.abs(md_j))[::-1]\n",
        "            rank_map_i = {feat: rank for rank, feat in enumerate(ranks_i)}\n",
        "            rank_map_j = {feat: rank for rank, feat in enumerate(ranks_j)}\n",
        "            ri = [rank_map_i[f] for f in union_feats]\n",
        "            rj = [rank_map_j[f] for f in union_feats]\n",
        "            rank_rho, _ = spearmanr(ri, rj)\n",
        "\n",
        "            results.append({\n",
        "                'demographic': demo,\n",
        "                'domain': domain,\n",
        "                'vocab_i': vi,\n",
        "                'vocab_j': vj,\n",
        "                'spearman_rho': float(rho),\n",
        "                'spearman_p': float(p_rho),\n",
        "                'cosine_similarity': float(cos_sim),\n",
        "                'pearson_r': float(pearson_r),\n",
        "                'jaccard_top50': float(jaccard),\n",
        "                'overlap_count_top50': int(overlap_count),\n",
        "                'rank_rho_union': float(rank_rho),\n",
        "            })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# --- Summary ---\n",
        "\n",
        "print(f\"\\nResults ({len(results_df)} comparisons)\")\n",
        "print(f\"\\nOverall:\")\n",
        "for metric in ['spearman_rho', 'cosine_similarity', 'pearson_r',\n",
        "               'jaccard_top50', 'overlap_count_top50', 'rank_rho_union']:\n",
        "    vals = results_df[metric].dropna()\n",
        "    print(f\"  {metric}: mean={vals.mean():.3f}, median={vals.median():.3f}, \"\n",
        "          f\"range=[{vals.min():.3f}, {vals.max():.3f}]\")\n",
        "\n",
        "print(f\"\\nBy demographic:\")\n",
        "for demo in ALL_DEMOGRAPHICS:\n",
        "    sub = results_df[results_df['demographic'] == demo]\n",
        "    if len(sub) == 0:\n",
        "        continue\n",
        "    print(f\"  {demo}:\")\n",
        "    print(f\"    Spearman rho:     {sub['spearman_rho'].mean():.3f} \"\n",
        "          f\"(min={sub['spearman_rho'].min():.3f})\")\n",
        "    print(f\"    Cosine sim:       {sub['cosine_similarity'].mean():.3f}\")\n",
        "    print(f\"    Jaccard top-50:   {sub['jaccard_top50'].mean():.3f} \"\n",
        "          f\"({sub['overlap_count_top50'].mean():.1f} features)\")\n",
        "    print(f\"    Rank rho (union): {sub['rank_rho_union'].mean():.3f}\")\n",
        "\n",
        "# Interpretation thresholds\n",
        "print(\"\\nInterpretation:\")\n",
        "mean_rho = results_df['spearman_rho'].mean()\n",
        "mean_jac = results_df['jaccard_top50'].mean()\n",
        "if mean_rho > 0.7 and mean_jac > 0.3:\n",
        "    print(\"  -> Features track demographic semantics (robust to rephrasing)\")\n",
        "elif mean_rho > 0.3 and mean_jac > 0.1:\n",
        "    print(\"  -> Partial robustness — some features stable, others wording-dependent\")\n",
        "else:\n",
        "    print(\"  -> Features may be prompt-specific — interpret encoding claims with caution\")\n",
        "\n",
        "\n",
        "# --- Save ---\n",
        "\n",
        "results_df.to_csv(OUTPUT_DIR / 'vocab_sensitivity_results.csv', index=False)\n",
        "\n",
        "summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'layer': TARGET_LAYER,\n",
        "    'n_respondents': len(resp_df),\n",
        "    'n_questions': len(QUESTION_INFO),\n",
        "    'n_vocab_variants': N_VOCAB_VARIANTS,\n",
        "    'k_features': N_FEATURES_CHECK,\n",
        "    'overall': {\n",
        "        'spearman_rho': float(results_df['spearman_rho'].mean()),\n",
        "        'cosine_similarity': float(results_df['cosine_similarity'].mean()),\n",
        "        'jaccard_top50': float(results_df['jaccard_top50'].mean()),\n",
        "        'overlap_count_top50': float(results_df['overlap_count_top50'].mean()),\n",
        "    },\n",
        "    'by_demographic': {},\n",
        "}\n",
        "for demo in ALL_DEMOGRAPHICS:\n",
        "    sub = results_df[results_df['demographic'] == demo]\n",
        "    if len(sub) > 0:\n",
        "        summary['by_demographic'][demo] = {\n",
        "            'spearman_rho': float(sub['spearman_rho'].mean()),\n",
        "            'cosine_similarity': float(sub['cosine_similarity'].mean()),\n",
        "            'jaccard_top50': float(sub['jaccard_top50'].mean()),\n",
        "            'overlap_count_top50': float(sub['overlap_count_top50'].mean()),\n",
        "        }\n",
        "\n",
        "with open(OUTPUT_DIR / 'vocab_sensitivity_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved to {OUTPUT_DIR}/\")"
      ],
      "metadata": {
        "id": "0ADv8lYipzdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt generation — contrastive pairs with real respondent demographics\n",
        "#\n",
        "# Each pair varies one demographic between extreme values while grounding all\n",
        "# other attributes in the respondent's real ESS data. This ensures activation\n",
        "# differences are attributable to the manipulated demographic.\n",
        "#\n",
        "# 110 questions across 5 domains, aligned with Llama pipeline.\n",
        "# Exclusions: w4dq4_4, w4dq13_7 (refusal codes); w4dq9/10 (demographic\n",
        "# comparison items); w4dq14 (not opinion-based); w4hq16-20 (behavioral).\n",
        "#\n",
        "# Scale handling: 0-10 scales retain all 11 response options. The downstream\n",
        "# EV computation handles '10' as a multi-token value.\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "SEED = 42\n",
        "N_SELECTION = 120\n",
        "N_VALIDATION = 30\n",
        "N_VOCAB = 1  # single wording; robustness to paraphrase validated separately\n",
        "\n",
        "OUTPUT_DIR = Path(\"./outputs_gemma_replication\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "MAX_SCALE = 10  # preserves full 0-10 range (previously capped at 9)\n",
        "\n",
        "\n",
        "# --- Load and split respondents ---\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "demographics_df = pd.read_csv(DEMOGRAPHICS_FILE)\n",
        "demographics_df = demographics_df.drop_duplicates(subset='idno', keep='first')\n",
        "AGE_COL = 'agea' if 'agea' in demographics_df.columns else 'age'\n",
        "\n",
        "print(f\"  Raw: {len(demographics_df)} respondents\")\n",
        "\n",
        "valid_df = demographics_df[\n",
        "    (demographics_df['gndr'].isin([1, 2])) &\n",
        "    (demographics_df['hincfel'].isin([1, 2, 3, 4])) &\n",
        "    (demographics_df[AGE_COL] < 900) &\n",
        "    (demographics_df['eisced'].isin([1, 2, 3, 4, 5, 6, 7])) &\n",
        "    (demographics_df['vote'].isin([1, 2, 3]))\n",
        "].copy()\n",
        "\n",
        "print(f\"  Valid: {len(valid_df)}\")\n",
        "\n",
        "valid_df = valid_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "selection_df = valid_df.iloc[:N_SELECTION].copy()\n",
        "validation_df = valid_df.iloc[N_SELECTION:N_SELECTION + N_VALIDATION].copy()\n",
        "\n",
        "assert len(set(selection_df['idno']) & set(validation_df['idno'])) == 0, \"Train/val overlap!\"\n",
        "\n",
        "selection_ids = selection_df['idno'].tolist()\n",
        "validation_ids = validation_df['idno'].tolist()\n",
        "\n",
        "print(f\"  Selection: {len(selection_ids)}, Validation: {len(validation_ids)}\")\n",
        "\n",
        "with open(OUTPUT_DIR / 'train_val_split.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'selection_ids': selection_ids,\n",
        "        'validation_ids': validation_ids,\n",
        "        'seed': SEED,\n",
        "    }, f)\n",
        "\n",
        "\n",
        "# --- Contrastive demographic descriptions ---\n",
        "\n",
        "DEMOGRAPHICS = {\n",
        "    'income': {\n",
        "        'value_a': 'wealthy',\n",
        "        'value_b': 'poor',\n",
        "        'prompts_a': [\n",
        "            \"You are financially wealthy. You own multiple properties, have substantial \"\n",
        "            \"savings and diverse investments, and never worry about money. You can afford \"\n",
        "            \"luxuries and expensive experiences without thinking twice about the cost.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are financially poor. You struggle to afford rent, have accumulated \"\n",
        "            \"significant debt, and constantly worry about paying for basic necessities. \"\n",
        "            \"Money is a persistent source of stress and limits your daily choices.\",\n",
        "        ],\n",
        "    },\n",
        "    'age': {\n",
        "        'value_a': 'old',\n",
        "        'value_b': 'young',\n",
        "        'prompts_a': [\n",
        "            \"You are 75 years old. You are retired after a long career, have lived through \"\n",
        "            \"decades of social and technological change, and remember life before computers \"\n",
        "            \"and mobile phones. You have accumulated a lifetime of experiences.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are 22 years old. You are early in your career, recently finished education, \"\n",
        "            \"and grew up as a digital native with smartphones and social media from childhood. \"\n",
        "            \"You have your whole adult life ahead of you.\",\n",
        "        ],\n",
        "    },\n",
        "    'gender': {\n",
        "        'value_a': 'male',\n",
        "        'value_b': 'female',\n",
        "        'prompts_a': [\n",
        "            \"You are a man. You have lived your entire life experiencing society as a man, \"\n",
        "            \"with male social expectations, relationships, and career experiences. Your \"\n",
        "            \"perspective has been shaped by masculine social norms and experiences.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are a woman. You have lived your entire life experiencing society as a woman, \"\n",
        "            \"with female social expectations, relationships, and career experiences. Your \"\n",
        "            \"perspective has been shaped by feminine social norms and experiences.\",\n",
        "        ],\n",
        "    },\n",
        "    'education': {\n",
        "        'value_a': 'high_education',\n",
        "        'value_b': 'low_education',\n",
        "        'prompts_a': [\n",
        "            \"You have a PhD and spent over 20 years in formal education. You work in a \"\n",
        "            \"professional field requiring advanced expertise and critical analysis. Academic \"\n",
        "            \"thinking and research methodology are second nature to you.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You did not complete high school. You left formal education early and learned \"\n",
        "            \"through practical work experience rather than academic study. You've built \"\n",
        "            \"knowledge through hands-on learning and real-world problem solving.\",\n",
        "        ],\n",
        "    },\n",
        "    'vote': {\n",
        "        'value_a': 'voter',\n",
        "        'value_b': 'non_voter',\n",
        "        'prompts_a': [\n",
        "            \"You are a regular voter who participates in every election. You believe voting \"\n",
        "            \"is a civic duty and fundamental to democracy. You stay informed about candidates \"\n",
        "            \"and issues, and always make time to cast your ballot.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are a non-voter who doesn't participate in elections. You feel disconnected \"\n",
        "            \"from the political system and don't believe your vote makes a difference. \"\n",
        "            \"Electoral politics seems distant from your daily life concerns.\",\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# --- Real demographic descriptions (for non-tested attributes) ---\n",
        "\n",
        "def get_real_gender_description(gndr_code):\n",
        "    if gndr_code == 1:\n",
        "        return \"You are a man.\"\n",
        "    elif gndr_code == 2:\n",
        "        return \"You are a woman.\"\n",
        "    return \"You are an adult.\"\n",
        "\n",
        "\n",
        "def get_real_age_description(age):\n",
        "    if age < 25: return f\"You are {age} years old, a young adult just starting out.\"\n",
        "    elif age < 35: return f\"You are {age} years old, in your late twenties to early thirties.\"\n",
        "    elif age < 45: return f\"You are {age} years old, in your mid-thirties to early forties.\"\n",
        "    elif age < 55: return f\"You are {age} years old, in middle age.\"\n",
        "    elif age < 65: return f\"You are {age} years old, in your late fifties to early sixties.\"\n",
        "    elif age < 75: return f\"You are {age} years old, in your sixties to early seventies.\"\n",
        "    return f\"You are {age} years old, a senior citizen.\"\n",
        "\n",
        "\n",
        "def get_real_education_description(eisced_code):\n",
        "    d = {\n",
        "        1: \"You have less than lower secondary education. You left school early.\",\n",
        "        2: \"You completed lower secondary education (middle school equivalent).\",\n",
        "        3: \"You completed upper secondary education (high school).\",\n",
        "        4: \"You have post-secondary non-tertiary education (vocational training).\",\n",
        "        5: \"You have a short-cycle tertiary degree (associate's or similar).\",\n",
        "        6: \"You have a bachelor's degree from university.\",\n",
        "        7: \"You have a master's degree or higher (including PhD).\",\n",
        "    }\n",
        "    return d.get(eisced_code, \"You have completed some formal education.\")\n",
        "\n",
        "\n",
        "def get_real_income_description(hincfel_code):\n",
        "    d = {\n",
        "        1: \"You live comfortably on your current income.\",\n",
        "        2: \"You cope on your current income.\",\n",
        "        3: \"You find it difficult on your current income.\",\n",
        "        4: \"You find it very difficult on your current income.\",\n",
        "    }\n",
        "    return d.get(hincfel_code, \"You have a moderate income.\")\n",
        "\n",
        "\n",
        "def get_real_vote_description(vote_code):\n",
        "    d = {\n",
        "        1: \"You voted in the last national election.\",\n",
        "        2: \"You did not vote in the last national election.\",\n",
        "        3: \"You were not eligible to vote in the last national election.\",\n",
        "    }\n",
        "    return d.get(vote_code, \"You have varying levels of political engagement.\")\n",
        "\n",
        "\n",
        "# --- Question definitions: 110 items across 5 domains ---\n",
        "\n",
        "VALID_QUESTIONS = {\n",
        "    # Climate (25)\n",
        "    \"w4gq1\": (0, 10, \"scale11\", \"climate\", \"Responsibility of current generations to reduce climate change\"),\n",
        "    \"w4gq2\": (0, 10, \"scale11\", \"climate\", \"Responsibility of future generations to reduce climate change\"),\n",
        "    \"w4gq3\": (1, 5, \"likert5\", \"climate\", \"Personal efforts to reduce climate change\"),\n",
        "    \"w4gq4\": (1, 5, \"likert5\", \"climate\", \"National government's efforts to reduce climate change\"),\n",
        "    \"w4gq5\": (1, 5, \"likert5\", \"climate\", \"Business and industry's efforts to reduce climate change\"),\n",
        "    \"w4gq6\": (1, 5, \"likert5\", \"climate\", \"Current generations' efforts to reduce climate change\"),\n",
        "    \"w4gq7\": (1, 5, \"likert5\", \"climate\", \"Confidence in fair outcomes of climate policies\"),\n",
        "    \"w4gq8\": (1, 5, \"likert5\", \"climate\", \"Confidence climate policies consider everyone's views\"),\n",
        "    \"w4gq9\": (1, 5, \"likert5\", \"climate\", \"Confidence climate policies are unbiased\"),\n",
        "    \"w4gq10_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for higher taxes on petrol and diesel\"),\n",
        "    \"w4gq10_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for higher distance-based road taxes\"),\n",
        "    \"w4gq11_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for subsidies for electric vehicles\"),\n",
        "    \"w4gq11_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for cycle-to-work scheme subsidies\"),\n",
        "    \"w4gq12_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for banning petrol and diesel cars\"),\n",
        "    \"w4gq12_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for banning construction of new roads\"),\n",
        "    \"w4gq13_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for more EV charging points\"),\n",
        "    \"w4gq13_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for more cycling infrastructure\"),\n",
        "    \"w4gq14_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for higher taxes on fossil fuel energy\"),\n",
        "    \"w4gq14_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for higher taxes on inefficient appliances\"),\n",
        "    \"w4gq15_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for subsidies on low-carbon heating\"),\n",
        "    \"w4gq15_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for subsidies on home insulation\"),\n",
        "    \"w4gq16_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for banning gas and oil boilers\"),\n",
        "    \"w4gq16_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for banning inefficient appliances\"),\n",
        "    \"w4gq17_a\": (1, 5, \"likert5_policy\", \"climate\", \"Support for low-carbon heating in new homes\"),\n",
        "    \"w4gq17_b\": (1, 5, \"likert5_policy\", \"climate\", \"Support for high insulation standards in new homes\"),\n",
        "\n",
        "    # Health (15) — w4hq16-20 excluded (behavioral self-reports)\n",
        "    \"w4hq1\": (1, 5, \"likert5_policy\", \"health\", \"Ban alcohol sales in neighbourhood shops\"),\n",
        "    \"w4hq2\": (1, 5, \"likert5_policy\", \"health\", \"Reduce serving size of alcoholic drinks\"),\n",
        "    \"w4hq3\": (1, 5, \"likert5_policy\", \"health\", \"Graphic warning labels on alcohol\"),\n",
        "    \"w4hq4\": (1, 5, \"likert5_policy\", \"health\", \"Increase price of alcoholic drinks\"),\n",
        "    \"w4hq5\": (1, 5, \"likert5_policy\", \"health\", \"Ban tobacco sales in neighbourhood shops\"),\n",
        "    \"w4hq6\": (1, 5, \"likert5_policy\", \"health\", \"Ban tobacco sales to under 18s\"),\n",
        "    \"w4hq7\": (1, 5, \"likert5_policy\", \"health\", \"Reduce cigarettes per pack\"),\n",
        "    \"w4hq8\": (1, 5, \"likert5_policy\", \"health\", \"Increase price of cigarettes/tobacco\"),\n",
        "    \"w4hq9\": (1, 5, \"likert5_policy\", \"health\", \"Ban vape sales to under 18s\"),\n",
        "    \"w4hq10\": (1, 5, \"likert5_policy\", \"health\", \"Ban vape sales in neighbourhood shops\"),\n",
        "    \"w4hq11\": (1, 5, \"likert5_policy\", \"health\", \"Ban high calorie snacks in shops/vending\"),\n",
        "    \"w4hq12\": (1, 5, \"likert5_policy\", \"health\", \"Ban advertisement of high calorie snacks\"),\n",
        "    \"w4hq13\": (1, 5, \"likert5_policy\", \"health\", \"Reduce size of snack packets\"),\n",
        "    \"w4hq14\": (1, 5, \"likert5_policy\", \"health\", \"Graphic warnings on high calorie snacks\"),\n",
        "    \"w4hq15\": (1, 5, \"likert5_policy\", \"health\", \"Tax to increase price of high calorie snacks\"),\n",
        "\n",
        "    # Digital (35) — excludes w4dq4_4, w4dq13_7 (refusal codes),\n",
        "    #                w4dq9/10 (demographic comparison), w4dq14 (not opinion)\n",
        "    \"w4dq1\": (1, 4, \"cat4\", \"digital\", \"Who should protect personal data\"),\n",
        "    \"w4dq2\": (1, 4, \"cat4\", \"digital\", \"Most trusted to protect personal data\"),\n",
        "    \"w4dq3\": (1, 4, \"cat4\", \"digital\", \"Government right to monitor internet use\"),\n",
        "    \"w4dq4_1\": (0, 1, \"binary\", \"digital\", \"Employers access device data: improve processes\"),\n",
        "    \"w4dq4_2\": (0, 1, \"binary\", \"digital\", \"Employers access device data: evaluate performance\"),\n",
        "    \"w4dq4_3\": (0, 1, \"binary\", \"digital\", \"Employers access device data: no right\"),\n",
        "    \"w4dq5_a\": (1, 2, \"binary\", \"digital\", \"Share social media with public admin for payment\"),\n",
        "    \"w4dq5_b\": (1, 2, \"binary\", \"digital\", \"Share browser history with public admin, no pay\"),\n",
        "    \"w4dq5_c\": (1, 2, \"binary\", \"digital\", \"Share social media with public admin for profile\"),\n",
        "    \"w4dq5_d\": (1, 2, \"binary\", \"digital\", \"Share browser history with private co for payment\"),\n",
        "    \"w4dq5_e\": (1, 2, \"binary\", \"digital\", \"Share browser history with public admin for payment\"),\n",
        "    \"w4dq5_f\": (1, 2, \"binary\", \"digital\", \"Share browser history with public admin for profile\"),\n",
        "    \"w4dq6_a\": (1, 2, \"binary\", \"digital\", \"Share GPS with private co for profile\"),\n",
        "    \"w4dq6_b\": (1, 2, \"binary\", \"digital\", \"Share GPS with private co for payment\"),\n",
        "    \"w4dq6_c\": (1, 2, \"binary\", \"digital\", \"Share GPS with public admin, no pay\"),\n",
        "    \"w4dq6_d\": (1, 2, \"binary\", \"digital\", \"Share GPS with private co, no pay\"),\n",
        "    \"w4dq6_e\": (1, 2, \"binary\", \"digital\", \"Share browser history with private co, no pay\"),\n",
        "    \"w4dq6_f\": (1, 2, \"binary\", \"digital\", \"Share social media with private co for profile\"),\n",
        "    \"w4dq7_a\": (1, 2, \"binary\", \"digital\", \"Share social media with private co, no pay\"),\n",
        "    \"w4dq7_b\": (1, 2, \"binary\", \"digital\", \"Share social media with public admin, no pay\"),\n",
        "    \"w4dq7_c\": (1, 2, \"binary\", \"digital\", \"Share GPS with public admin for profile\"),\n",
        "    \"w4dq7_d\": (1, 2, \"binary\", \"digital\", \"Share browser history with private co for profile\"),\n",
        "    \"w4dq7_e\": (1, 2, \"binary\", \"digital\", \"Share social media with private co for payment\"),\n",
        "    \"w4dq7_f\": (1, 2, \"binary\", \"digital\", \"Share GPS with public admin for payment\"),\n",
        "    \"w4dq8\": (1, 5, \"likert5\", \"digital\", \"How well adapt to technology\"),\n",
        "    \"w4dq11\": (1, 5, \"likert5\", \"digital\", \"Tech advancements positive or negative for society\"),\n",
        "    \"w4dq12\": (1, 4, \"cat4\", \"digital\", \"Who should regulate AI\"),\n",
        "    \"w4dq13_1\": (0, 1, \"binary\", \"digital\", \"Employers use AI for hiring\"),\n",
        "    \"w4dq13_2\": (0, 1, \"binary\", \"digital\", \"Employers use AI for performance evaluation\"),\n",
        "    \"w4dq13_3\": (0, 1, \"binary\", \"digital\", \"Employers use AI for training\"),\n",
        "    \"w4dq13_4\": (0, 1, \"binary\", \"digital\", \"Employers use AI for discipline/termination\"),\n",
        "    \"w4dq13_5\": (0, 1, \"binary\", \"digital\", \"Employers use AI for admin support\"),\n",
        "    \"w4dq13_6\": (0, 1, \"binary\", \"digital\", \"Employers should not use AI internally\"),\n",
        "    \"w4dq15\": (0, 10, \"scale11\", \"digital\", \"Most people can be trusted\"),\n",
        "    \"w4dq16\": (0, 10, \"scale11\", \"digital\", \"How optimistic are you in general\"),\n",
        "\n",
        "    # Economy (17)\n",
        "    \"w4eq1\": (0, 10, \"scale11\", \"economy\", \"Personal wealth compared to others\"),\n",
        "    \"w4eq2\": (0, 10, \"scale11\", \"economy\", \"How fair are wealth differences\"),\n",
        "    \"w4eq3\": (0, 10, \"scale11\", \"economy\", \"Government responsibility for affordable housing\"),\n",
        "    \"w4eq4\": (1, 5, \"likert5_importance\", \"economy\", \"Importance of inheritance for accumulating wealth\"),\n",
        "    \"w4eq5\": (1, 5, \"likert5_importance\", \"economy\", \"Importance of hard work for accumulating wealth\"),\n",
        "    \"w4eq6\": (1, 5, \"likert5_importance\", \"economy\", \"Importance of luck for accumulating wealth\"),\n",
        "    \"w4eq7\": (1, 5, \"likert5_importance\", \"economy\", \"Importance of knowing right people for wealth\"),\n",
        "    \"w4eq8\": (1, 4, \"cat4\", \"economy\", \"Confidence in financial security for retirement\"),\n",
        "    \"w4eq9\": (1, 2, \"binary\", \"economy\", \"Ever received substantial inheritance\"),\n",
        "    \"w4eq10\": (1, 2, \"binary\", \"economy\", \"Expect to receive inheritance in future\"),\n",
        "    \"w4eq11\": (1, 5, \"likert5_importance\", \"economy\", \"Importance of leaving inheritance\"),\n",
        "    \"w4eq12\": (1, 5, \"likert5\", \"economy\", \"Likelihood of leaving inheritance\"),\n",
        "    \"w4eq13\": (1, 5, \"likert5_policy\", \"economy\", \"Support inheritance tax\"),\n",
        "    \"w4eq14\": (1, 5, \"likert5_policy\", \"economy\", \"Support annual wealth tax\"),\n",
        "    \"w4eq15\": (1, 5, \"likert5_policy\", \"economy\", \"Support capital gains taxed as income\"),\n",
        "    \"w4eq16\": (1, 5, \"likert5\", \"economy\", \"Capital gains tax level too low/high\"),\n",
        "    \"w4eq17\": (1, 5, \"likert5\", \"economy\", \"Real estate tax level too low/high\"),\n",
        "\n",
        "    # Values (18)\n",
        "    \"w4sq1\": (1, 5, \"likert5_importance\", \"values\", \"Importance of work in life\"),\n",
        "    \"w4sq2\": (1, 5, \"likert5_importance\", \"values\", \"Importance of family in life\"),\n",
        "    \"w4sq3\": (1, 5, \"likert5_importance\", \"values\", \"Importance of leisure in life\"),\n",
        "    \"w4sq4\": (1, 5, \"likert5_importance\", \"values\", \"Importance of politics in life\"),\n",
        "    \"w4sq5\": (1, 5, \"likert5_importance\", \"values\", \"Importance of religion in life\"),\n",
        "    \"w4sq6\": (0, 10, \"scale11\", \"values\", \"Freedom of choice and control over life\"),\n",
        "    \"w4sq7\": (1, 5, \"likert5_importance\", \"values\", \"Importance of faithfulness in marriage\"),\n",
        "    \"w4sq8\": (1, 5, \"likert5_importance\", \"values\", \"Importance of adequate income in marriage\"),\n",
        "    \"w4sq9\": (1, 5, \"likert5_importance\", \"values\", \"Importance of good accommodation in marriage\"),\n",
        "    \"w4sq10\": (1, 5, \"likert5_importance\", \"values\", \"Importance of sharing household chores\"),\n",
        "    \"w4sq11\": (1, 5, \"likert5_importance\", \"values\", \"Importance of children in marriage\"),\n",
        "    \"w4sq12\": (1, 5, \"likert5_importance\", \"values\", \"Importance of personal time in marriage\"),\n",
        "    \"w4sq13\": (1, 5, \"likert5_policy\", \"values\", \"Marriage is an outdated institution\"),\n",
        "    \"w4sq14\": (0, 10, \"scale11\", \"values\", \"Individual vs state responsibility\"),\n",
        "    \"w4sq15\": (0, 10, \"scale11\", \"values\", \"Community vs state responsibility\"),\n",
        "    \"w4sq16\": (0, 10, \"scale11\", \"values\", \"Private vs government ownership\"),\n",
        "    \"w4sq17\": (0, 10, \"scale11\", \"values\", \"Importance of living in democracy\"),\n",
        "    \"w4sq18\": (0, 10, \"scale11\", \"values\", \"How democratic is country today\"),\n",
        "}\n",
        "\n",
        "print(f\"Questions defined: {len(VALID_QUESTIONS)}\")\n",
        "\n",
        "\n",
        "# --- Load question text from codebook ---\n",
        "\n",
        "with open(CODEBOOK_FILE, 'r') as f:\n",
        "    codebook = json.load(f)\n",
        "\n",
        "codebook_lookup = {var['variable_name']: var for var in codebook['variables']}\n",
        "\n",
        "QUESTIONS = {}\n",
        "for qid, (min_v, max_v, qtype, domain, label) in VALID_QUESTIONS.items():\n",
        "    var = codebook_lookup.get(qid)\n",
        "\n",
        "    if var is None:\n",
        "        question_text = label\n",
        "        min_label = str(min_v)\n",
        "        max_label = str(max_v)\n",
        "    else:\n",
        "        question_text = var.get('question', label)\n",
        "        if 'scale_range' in var:\n",
        "            min_label = var['scale_range'].get('min_label', str(min_v))\n",
        "            max_label = var['scale_range'].get('max_label', str(max_v))\n",
        "        elif 'values' in var:\n",
        "            labels = {v['code']: v['label'] for v in var['values'] if isinstance(v['code'], int)}\n",
        "            min_label = labels.get(min_v, str(min_v))\n",
        "            max_label = labels.get(max_v, str(max_v))\n",
        "        else:\n",
        "            min_label = str(min_v)\n",
        "            max_label = str(max_v)\n",
        "\n",
        "    QUESTIONS[qid] = {\n",
        "        'text': question_text,\n",
        "        'scale_min': min_v,\n",
        "        'scale_max': min(max_v, MAX_SCALE),\n",
        "        'min_label': min_label,\n",
        "        'max_label': max_label,\n",
        "        'q_type': qtype,\n",
        "        'domain': domain,\n",
        "    }\n",
        "\n",
        "print(f\"Loaded {len(QUESTIONS)} questions from codebook\")\n",
        "\n",
        "domain_counts = {}\n",
        "for q in QUESTIONS.values():\n",
        "    domain_counts[q['domain']] = domain_counts.get(q['domain'], 0) + 1\n",
        "for domain, count in sorted(domain_counts.items()):\n",
        "    print(f\"  {domain}: {count}\")\n",
        "\n",
        "\n",
        "# --- Gemma 2 prompt formatting ---\n",
        "\n",
        "def format_gemma_prompt(context, question, scale_info):\n",
        "    user_content = f\"{context}\\n\\n{question} {scale_info}\\n\\nRESPOND WITH ONLY A SINGLE NUMBER.\"\n",
        "    return (\n",
        "        f\"<start_of_turn>user\\n\"\n",
        "        f\"{user_content}<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "def format_scale(q_info):\n",
        "    return f\"({q_info['scale_min']} = {q_info['min_label']}, {q_info['scale_max']} = {q_info['max_label']})\"\n",
        "\n",
        "\n",
        "def build_context_with_real_demographics(row, tested_demo, tested_prompt, country):\n",
        "    \"\"\"Build prompt context: tested demographic is contrastive, others are real ESS values.\"\"\"\n",
        "    parts = []\n",
        "\n",
        "    if tested_demo == 'gender':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_gender_description(row['gndr']))\n",
        "\n",
        "    if tested_demo == 'age':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_age_description(row[AGE_COL]))\n",
        "\n",
        "    parts.append(f\"You live in {country}.\")\n",
        "\n",
        "    if tested_demo == 'education':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_education_description(row['eisced']))\n",
        "\n",
        "    if tested_demo == 'income':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_income_description(row['hincfel']))\n",
        "\n",
        "    if tested_demo == 'vote':\n",
        "        parts.append(tested_prompt)\n",
        "    else:\n",
        "        parts.append(get_real_vote_description(row['vote']))\n",
        "\n",
        "    return \" \".join(parts)\n",
        "\n",
        "\n",
        "# --- Generate prompts ---\n",
        "\n",
        "def generate_prompts(resp_df, id_list, name):\n",
        "    print(f\"\\nGenerating {name} prompts...\")\n",
        "    prompts = []\n",
        "    df = resp_df[resp_df['idno'].isin(id_list)]\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        country = COUNTRY_MAP.get(row['cntry'], 'Europe')\n",
        "\n",
        "        for q_id, q_info in QUESTIONS.items():\n",
        "            for demo_name, demo in DEMOGRAPHICS.items():\n",
        "                for vocab_idx in range(N_VOCAB):\n",
        "                    pair_key = f\"{row['idno']}_{q_id}_{demo_name}_v{vocab_idx}\"\n",
        "\n",
        "                    for vt in ['a', 'b']:\n",
        "                        demo_prompt = demo[f'prompts_{vt}'][vocab_idx]\n",
        "                        context = build_context_with_real_demographics(\n",
        "                            row, demo_name, demo_prompt, country\n",
        "                        )\n",
        "                        prompt = format_gemma_prompt(context, q_info['text'], format_scale(q_info))\n",
        "\n",
        "                        prompts.append({\n",
        "                            'pair_key': pair_key,\n",
        "                            'respondent_id': row['idno'],\n",
        "                            'question_id': q_id,\n",
        "                            'question_type': q_info['q_type'],\n",
        "                            'domain': q_info['domain'],\n",
        "                            'demographic': demo_name,\n",
        "                            'value': demo[f'value_{vt}'],\n",
        "                            'value_type': f'value_{vt}',\n",
        "                            'vocab_idx': vocab_idx,\n",
        "                            'prompt': prompt,\n",
        "                            'scale_min': q_info['scale_min'],\n",
        "                            'scale_max': q_info['scale_max'],\n",
        "                        })\n",
        "\n",
        "    result = pd.DataFrame(prompts)\n",
        "    print(f\"  {name}: {len(result):,} prompts\")\n",
        "    return result\n",
        "\n",
        "\n",
        "prompts_selection = generate_prompts(valid_df, selection_ids, \"Selection\")\n",
        "prompts_validation = generate_prompts(valid_df, validation_ids, \"Validation\")\n",
        "\n",
        "prompts_selection.to_parquet(OUTPUT_DIR / 'prompts_selection.parquet', index=False)\n",
        "prompts_validation.to_parquet(OUTPUT_DIR / 'prompts_validation.parquet', index=False)\n",
        "\n",
        "print(f\"\\nSaved to {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "\n",
        "print(\"\\nSample prompts:\")\n",
        "\n",
        "sample_a = prompts_selection[\n",
        "    (prompts_selection['demographic'] == 'income') &\n",
        "    (prompts_selection['vocab_idx'] == 0) &\n",
        "    (prompts_selection['value_type'] == 'value_a')\n",
        "].iloc[0]\n",
        "print(f\"\\nIncome (wealthy):\")\n",
        "print(sample_a['prompt'])\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "sample_g = prompts_selection[\n",
        "    (prompts_selection['demographic'] == 'gender') &\n",
        "    (prompts_selection['vocab_idx'] == 0) &\n",
        "    (prompts_selection['value_type'] == 'value_a')\n",
        "].iloc[0]\n",
        "print(f\"\\nGender (male):\")\n",
        "print(sample_g['prompt'])\n",
        "\n",
        "# Verify 0-10 scales preserved\n",
        "scale11_sample = prompts_selection[prompts_selection['question_type'] == 'scale11'].iloc[0]\n",
        "assert scale11_sample['scale_max'] == 10, \"Scale 0-10 questions should have scale_max=10\"\n",
        "print(f\"\\nScale11 max check: {scale11_sample['scale_max']} (OK)\")\n",
        "\n",
        "\n",
        "# --- Summary ---\n",
        "\n",
        "n_questions = len(QUESTIONS)\n",
        "n_prompts_per_respondent = n_questions * len(DEMOGRAPHICS) * N_VOCAB * 2\n",
        "\n",
        "print(f\"\\nTotal: {len(prompts_selection) + len(prompts_validation):,} prompts\")\n",
        "print(f\"  Selection: {len(prompts_selection):,} ({N_SELECTION} respondents)\")\n",
        "print(f\"  Validation: {len(prompts_validation):,} ({N_VALIDATION} respondents)\")\n",
        "print(f\"  Per respondent: {n_prompts_per_respondent:,}\")\n",
        "print(f\"  Demographics: {len(DEMOGRAPHICS)}, Questions: {n_questions}, Vocab: {N_VOCAB}\")\n",
        "\n",
        "for domain in ['climate', 'health', 'digital', 'economy', 'values']:\n",
        "    n = len([q for q in QUESTIONS.values() if q['domain'] == domain])\n",
        "    print(f\"  {domain}: {n}\")"
      ],
      "metadata": {
        "id": "5poC6hJcqh8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocab sensitivity check — setup and prompt generation\n",
        "# Tests whether top-K feature selection is robust to prompt rephrasing.\n",
        "# Requires: model, tokenizer, sae_manager in scope\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial.distance import cosine as cosine_dist\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "TARGET_LAYER = 36\n",
        "HOOK_NAME = f\"blocks.{TARGET_LAYER}.hook_resid_post\"\n",
        "N_SUBSAMPLE_RESPONDENTS = 30\n",
        "N_FEATURES_CHECK = 50\n",
        "N_VOCAB_VARIANTS = 3\n",
        "ALL_DEMOGRAPHICS = ['income', 'age', 'gender', 'education', 'vote']\n",
        "\n",
        "QUESTION_SUBSET = [\n",
        "    \"w4gq1\", \"w4gq2\", \"w4gq3\", \"w4gq10_a\",   # climate\n",
        "    \"w4hq1\", \"w4hq4\", \"w4hq8\", \"w4hq15\",       # health\n",
        "    \"w4dq8\", \"w4dq11\", \"w4dq15\", \"w4dq16\",      # digital\n",
        "    \"w4eq1\", \"w4eq2\", \"w4eq3\", \"w4eq13\",         # economy\n",
        "    \"w4sq1\", \"w4sq6\", \"w4sq14\", \"w4sq17\",        # values\n",
        "]\n",
        "\n",
        "INPUT_DIR = Path(\"./outputs_gemma_replication\")\n",
        "OUTPUT_DIR = INPUT_DIR / \"vocab_sensitivity\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "CODEBOOK_FILE = Path(\"./codebook_updated.json\")\n",
        "DEMOGRAPHICS_FILE = Path(\"./stratified_sample_200_blank.csv\")\n",
        "\n",
        "# --- Vocab variants (3 wordings per demographic × value) ---\n",
        "\n",
        "VOCAB_VARIANTS = {\n",
        "    'income': {\n",
        "        'prompts_a': [\n",
        "            \"You are financially wealthy. You own multiple properties, have substantial savings and diverse investments, and never worry about money. You can afford luxuries and expensive experiences without thinking twice about the cost.\",\n",
        "            \"You have a very high income and significant personal wealth. Your financial situation is extremely comfortable — you have extensive savings, property holdings, and investment portfolios. Money has never been a source of concern for you.\",\n",
        "            \"You are rich. You have more money than you could ever spend, with large savings, multiple homes, and a portfolio of investments. Financial worries are completely foreign to you.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are financially poor. You struggle to afford rent, have accumulated significant debt, and constantly worry about paying for basic necessities. Money is a persistent source of stress and limits your daily choices.\",\n",
        "            \"You have a very low income and almost no savings. Making ends meet is a constant struggle — you worry about rent, bills, and whether you can afford groceries. Debt weighs heavily on you every day.\",\n",
        "            \"You are poor. You live paycheck to paycheck with mounting debts, and basic expenses like housing and food are a constant source of anxiety. Financial security feels impossibly out of reach.\",\n",
        "        ],\n",
        "    },\n",
        "    'age': {\n",
        "        'prompts_a': [\n",
        "            \"You are 75 years old. You are retired after a long career, have lived through decades of social and technological change, and remember life before computers and mobile phones. You have accumulated a lifetime of experiences.\",\n",
        "            \"You are an elderly person of 75. You've been retired for years after a full working life. You witnessed the entire digital revolution unfold and remember a world without the internet. Decades of experience have shaped your worldview.\",\n",
        "            \"You are 75 and in your retirement years. You've seen enormous societal changes over your long life — from a pre-digital world to today. Your career is behind you and you draw on a lifetime of accumulated wisdom.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are 22 years old. You are early in your career, recently finished education, and grew up as a digital native with smartphones and social media from childhood. You have your whole adult life ahead of you.\",\n",
        "            \"You are a young adult of 22, just starting out in your professional life. Technology has been part of your world since birth — you can't remember life without the internet. Your future stretches ahead with many possibilities.\",\n",
        "            \"You are 22 and recently entered the workforce. Smartphones and social media have been constant companions throughout your life. You're at the very beginning of your adult journey with everything still ahead.\",\n",
        "        ],\n",
        "    },\n",
        "    'gender': {\n",
        "        'prompts_a': [\n",
        "            \"You are a man. You have lived your entire life experiencing society as a man, with male social expectations, relationships, and career experiences. Your perspective has been shaped by masculine social norms and experiences.\",\n",
        "            \"You are male. Throughout your life, you have navigated the world as a man — from boyhood through adulthood. Societal expectations of masculinity have influenced your relationships, career path, and daily interactions.\",\n",
        "            \"You are a man who has experienced life through a male lens. Masculine norms and expectations have shaped how you relate to others, how you approach work, and how society has treated you throughout your life.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are a woman. You have lived your entire life experiencing society as a woman, with female social expectations, relationships, and career experiences. Your perspective has been shaped by feminine social norms and experiences.\",\n",
        "            \"You are female. Throughout your life, you have navigated the world as a woman — from girlhood through adulthood. Societal expectations of femininity have influenced your relationships, career path, and daily interactions.\",\n",
        "            \"You are a woman who has experienced life through a female lens. Feminine norms and expectations have shaped how you relate to others, how you approach work, and how society has treated you throughout your life.\",\n",
        "        ],\n",
        "    },\n",
        "    'education': {\n",
        "        'prompts_a': [\n",
        "            \"You have a PhD and spent over 20 years in formal education. You work in a professional field requiring advanced expertise and critical analysis. Academic thinking and research methodology are second nature to you.\",\n",
        "            \"You are highly educated with a doctoral degree. After more than two decades of academic study, you work in a field that demands deep expertise. Rigorous analytical thinking and scholarly methods come naturally to you.\",\n",
        "            \"You hold a PhD after 20+ years of formal education. Your career requires advanced intellectual skills and critical reasoning. You are thoroughly trained in research methods and think naturally in academic frameworks.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You did not complete high school. You left formal education early and learned through practical work experience rather than academic study. You've built knowledge through hands-on learning and real-world problem solving.\",\n",
        "            \"You have minimal formal education, having left school before finishing secondary level. What you know, you learned through working and doing — practical experience rather than textbooks shaped your understanding of the world.\",\n",
        "            \"You dropped out before completing high school. Your education came from the school of life — years of hands-on work and practical problem-solving rather than formal academic training.\",\n",
        "        ],\n",
        "    },\n",
        "    'vote': {\n",
        "        'prompts_a': [\n",
        "            \"You are a regular voter who participates in every election. You believe voting is a civic duty and fundamental to democracy. You stay informed about candidates and issues, and always make time to cast your ballot.\",\n",
        "            \"You vote consistently in every election without exception. Democratic participation is a core value for you — you research candidates, follow political developments, and consider voting an essential civic responsibility.\",\n",
        "            \"You are a committed voter who never misses an election. Casting your ballot is something you see as a fundamental obligation of citizenship. You keep up with political issues and candidates as a matter of principle.\",\n",
        "        ],\n",
        "        'prompts_b': [\n",
        "            \"You are a non-voter who doesn't participate in elections. You feel disconnected from the political system and don't believe your vote makes a difference. Electoral politics seems distant from your daily life concerns.\",\n",
        "            \"You don't vote in elections. The political system feels irrelevant to your life — you see no point in casting a ballot when it won't change anything. Politics and elections are not things you pay attention to.\",\n",
        "            \"You have chosen not to participate in elections. Voting feels meaningless to you — the political system seems disconnected from the issues that actually affect your daily life, and you don't believe one vote matters.\",\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "# --- Helper functions ---\n",
        "\n",
        "COUNTRY_MAP = {\n",
        "    'AT': 'Austria', 'BE': 'Belgium', 'CZ': 'Czechia', 'FI': 'Finland',\n",
        "    'FR': 'France', 'GB': 'United Kingdom', 'HU': 'Hungary', 'IS': 'Iceland',\n",
        "    'PL': 'Poland', 'PT': 'Portugal', 'SI': 'Slovenia'\n",
        "}\n",
        "\n",
        "def get_real_gender_description(code):\n",
        "    return {1: \"You are a man.\", 2: \"You are a woman.\"}.get(code, \"You are an adult.\")\n",
        "\n",
        "def get_real_age_description(age):\n",
        "    if pd.isna(age) or age > 900: return \"You are an adult.\"\n",
        "    age = int(age)\n",
        "    if age < 25: return f\"You are {age} years old, a young adult just starting out.\"\n",
        "    elif age < 35: return f\"You are {age} years old, in your late twenties to early thirties.\"\n",
        "    elif age < 45: return f\"You are {age} years old, in your mid-thirties to early forties.\"\n",
        "    elif age < 55: return f\"You are {age} years old, in middle age.\"\n",
        "    elif age < 65: return f\"You are {age} years old, in your late fifties to early sixties.\"\n",
        "    elif age < 75: return f\"You are {age} years old, in your sixties to early seventies.\"\n",
        "    return f\"You are {age} years old, a senior citizen.\"\n",
        "\n",
        "def get_real_education_description(code):\n",
        "    return {1: \"You have less than lower secondary education. You left school early.\",\n",
        "            2: \"You completed lower secondary education (middle school equivalent).\",\n",
        "            3: \"You completed upper secondary education (high school).\",\n",
        "            4: \"You have post-secondary non-tertiary education (vocational training).\",\n",
        "            5: \"You have a short-cycle tertiary degree (associate's or similar).\",\n",
        "            6: \"You have a bachelor's degree from university.\",\n",
        "            7: \"You have a master's degree or higher (including PhD).\"}.get(code, \"You have completed some formal education.\")\n",
        "\n",
        "def get_real_income_description(code):\n",
        "    return {1: \"You live comfortably on your current income.\",\n",
        "            2: \"You cope on your current income.\",\n",
        "            3: \"You find it difficult on your current income.\",\n",
        "            4: \"You find it very difficult on your current income.\"}.get(code, \"You have a moderate income.\")\n",
        "\n",
        "def get_real_vote_description(code):\n",
        "    return {1: \"You voted in the last national election.\",\n",
        "            2: \"You did not vote in the last national election.\",\n",
        "            3: \"You were not eligible to vote in the last national election.\"}.get(code, \"You have varying levels of political engagement.\")\n",
        "\n",
        "def build_context(row, tested_demo, tested_prompt, country):\n",
        "    parts = []\n",
        "    parts.append(tested_prompt if tested_demo == 'gender' else get_real_gender_description(row.get('gndr', 0)))\n",
        "    parts.append(tested_prompt if tested_demo == 'age' else get_real_age_description(row.get('agea', row.get('age', 999))))\n",
        "    parts.append(f\"You live in {country}.\")\n",
        "    parts.append(tested_prompt if tested_demo == 'education' else get_real_education_description(row.get('eisced', 0)))\n",
        "    parts.append(tested_prompt if tested_demo == 'income' else get_real_income_description(row.get('hincfel', 0)))\n",
        "    parts.append(tested_prompt if tested_demo == 'vote' else get_real_vote_description(row.get('vote', 0)))\n",
        "    return \" \".join(parts)\n",
        "\n",
        "def format_gemma_prompt(context, question, scale_info):\n",
        "    user_content = f\"{context}\\n\\n{question} {scale_info}\\n\\nRESPOND WITH ONLY A SINGLE NUMBER.\"\n",
        "    return f\"<start_of_turn>user\\n{user_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "def _build_token_info():\n",
        "    info = {}\n",
        "    for i in range(0, 11):\n",
        "        encoded = tokenizer.encode(str(i), add_special_tokens=False)\n",
        "        if len(encoded) == 1:\n",
        "            info[i] = {'type': 'single', 'token_id': encoded[0]}\n",
        "        else:\n",
        "            info[i] = {'type': 'multi', 'token_ids': encoded,\n",
        "                        'first_token_id': encoded[0], 'second_token_id': encoded[1]}\n",
        "    return info\n",
        "\n",
        "TOKEN_INFO = _build_token_info()\n",
        "\n",
        "# --- Load data ---\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "if CODEBOOK_FILE.exists():\n",
        "    with open(CODEBOOK_FILE) as f:\n",
        "        codebook = json.load(f)\n",
        "    codebook_lookup = {v['variable_name']: v for v in codebook['variables']}\n",
        "else:\n",
        "    codebook_lookup = {}\n",
        "\n",
        "prompts_df = pd.read_parquet(INPUT_DIR / 'prompts_selection.parquet')\n",
        "\n",
        "QUESTION_INFO = {}\n",
        "for qid in QUESTION_SUBSET:\n",
        "    matches = prompts_df[prompts_df['question_id'] == qid]\n",
        "    if len(matches) == 0: continue\n",
        "    row = matches.iloc[0]\n",
        "    q_var = codebook_lookup.get(qid, {})\n",
        "    q_text = q_var.get('question', qid)\n",
        "    if 'scale_range' in q_var:\n",
        "        min_label = q_var['scale_range'].get('min_label', str(row['scale_min']))\n",
        "        max_label = q_var['scale_range'].get('max_label', str(row['scale_max']))\n",
        "    elif 'values' in q_var:\n",
        "        labels = {v['code']: v['label'] for v in q_var['values'] if isinstance(v['code'], int)}\n",
        "        min_label = labels.get(int(row['scale_min']), str(row['scale_min']))\n",
        "        max_label = labels.get(int(row['scale_max']), str(row['scale_max']))\n",
        "    else:\n",
        "        min_label, max_label = str(row['scale_min']), str(row['scale_max'])\n",
        "    QUESTION_INFO[qid] = {\n",
        "        'text': q_text, 'scale_min': int(row['scale_min']), 'scale_max': int(row['scale_max']),\n",
        "        'min_label': min_label, 'max_label': max_label, 'domain': row['domain'],\n",
        "    }\n",
        "\n",
        "print(f\"  Questions: {len(QUESTION_INFO)}\")\n",
        "\n",
        "demographics_df = pd.read_csv(DEMOGRAPHICS_FILE)\n",
        "id_col = None\n",
        "for col in ['idno', 'respondent_id', 'id', 'ID']:\n",
        "    if col in demographics_df.columns: id_col = col; break\n",
        "if id_col is None: id_col = demographics_df.columns[0]\n",
        "demographics_df = demographics_df.drop_duplicates(subset=id_col, keep='first')\n",
        "\n",
        "split_file = INPUT_DIR / 'train_val_split.json'\n",
        "if split_file.exists():\n",
        "    with open(split_file) as f:\n",
        "        split = json.load(f)\n",
        "    val_ids = split.get('validation_ids', [])\n",
        "    resp_df = demographics_df[demographics_df[id_col].isin(val_ids)] if val_ids else demographics_df.head(N_SUBSAMPLE_RESPONDENTS)\n",
        "else:\n",
        "    resp_df = demographics_df.head(N_SUBSAMPLE_RESPONDENTS)\n",
        "\n",
        "resp_df = resp_df.rename(columns={id_col: 'idno'})\n",
        "print(f\"  Respondents: {len(resp_df)}\")\n",
        "\n",
        "# --- Generate prompts ---\n",
        "\n",
        "print(\"Generating prompts for 3 vocab variants...\")\n",
        "\n",
        "all_prompts = []\n",
        "for _, row in resp_df.iterrows():\n",
        "    country = COUNTRY_MAP.get(row.get('cntry', ''), 'Europe')\n",
        "    for qid, q_info in QUESTION_INFO.items():\n",
        "        scale_str = f\"({q_info['scale_min']} = {q_info['min_label']}, {q_info['scale_max']} = {q_info['max_label']})\"\n",
        "        for demo_name in ALL_DEMOGRAPHICS:\n",
        "            for vocab_idx in range(N_VOCAB_VARIANTS):\n",
        "                for vt in ['a', 'b']:\n",
        "                    demo_prompt = VOCAB_VARIANTS[demo_name][f'prompts_{vt}'][vocab_idx]\n",
        "                    context = build_context(row, demo_name, demo_prompt, country)\n",
        "                    prompt = format_gemma_prompt(context, q_info['text'], scale_str)\n",
        "                    all_prompts.append({\n",
        "                        'respondent_id': row['idno'], 'question_id': qid,\n",
        "                        'domain': q_info['domain'], 'demographic': demo_name,\n",
        "                        'value_type': f'value_{vt}', 'vocab_idx': vocab_idx,\n",
        "                        'scale_min': q_info['scale_min'], 'scale_max': q_info['scale_max'],\n",
        "                        'prompt': prompt,\n",
        "                        'pair_key': f\"{row['idno']}_{qid}_{demo_name}_v{vocab_idx}\",\n",
        "                    })\n",
        "\n",
        "sensitivity_df = pd.DataFrame(all_prompts)\n",
        "print(f\"  Total prompts: {len(sensitivity_df):,} ({len(sensitivity_df)//N_VOCAB_VARIANTS:,} per variant)\")"
      ],
      "metadata": {
        "id": "QMvpdwlv8H_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocab sensitivity — SAE extraction and metric computation\n",
        "\n",
        "print(f\"Extracting SAE activations (Layer {TARGET_LAYER})\")\n",
        "\n",
        "sae = sae_manager.load_sae(TARGET_LAYER)\n",
        "d_sae = sae.d_sae if hasattr(sae, 'd_sae') else 16384\n",
        "\n",
        "diff_storage = {}\n",
        "activation_buffer = {}\n",
        "n = len(sensitivity_df)\n",
        "\n",
        "for batch_start in tqdm(range(0, n, BATCH_SIZE), desc=\"Extracting\"):\n",
        "    batch_end = min(batch_start + BATCH_SIZE, n)\n",
        "    batch_size = batch_end - batch_start\n",
        "    batch_rows = sensitivity_df.iloc[batch_start:batch_end]\n",
        "\n",
        "    tokens = tokenizer(batch_rows['prompt'].tolist(), return_tensors='pt',\n",
        "                       padding=True, truncation=True, max_length=512)\n",
        "    input_ids = tokens['input_ids'].to(DEVICE)\n",
        "    attention_mask = tokens['attention_mask'].to(DEVICE)\n",
        "    seq_lens = attention_mask.sum(dim=1) - 1\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        logits, cache = model.run_with_cache(input_ids, names_filter=lambda n: HOOK_NAME in n)\n",
        "        residuals = torch.stack([cache[HOOK_NAME][j, seq_lens[j], :] for j in range(batch_size)]).float()\n",
        "        sae_acts = sae.encode(residuals).float().cpu().numpy()\n",
        "        del cache, residuals, logits\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        row = batch_rows.iloc[j]\n",
        "        pk = row['pair_key']\n",
        "\n",
        "        if pk not in activation_buffer:\n",
        "            activation_buffer[pk] = {'demo': row['demographic'], 'domain': row['domain'],\n",
        "                                      'vocab_idx': row['vocab_idx']}\n",
        "        activation_buffer[pk][row['value_type']] = sae_acts[j]\n",
        "\n",
        "        if 'value_a' in activation_buffer[pk] and 'value_b' in activation_buffer[pk]:\n",
        "            info = activation_buffer[pk]\n",
        "            key = (info['vocab_idx'], info['demo'], info['domain'])\n",
        "            if key not in diff_storage: diff_storage[key] = []\n",
        "            diff_storage[key].append(info['value_a'] - info['value_b'])\n",
        "            del activation_buffer[pk]\n",
        "\n",
        "    if (batch_start // BATCH_SIZE) % 20 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "del sae\n",
        "torch.cuda.empty_cache()\n",
        "activation_buffer.clear()\n",
        "\n",
        "print(f\"  Groups: {len(diff_storage)}, orphans: {len(activation_buffer)}\")\n",
        "\n",
        "# --- Compute metrics ---\n",
        "\n",
        "print(\"\\nComputing sensitivity metrics...\")\n",
        "\n",
        "results = []\n",
        "for demo in ALL_DEMOGRAPHICS:\n",
        "    for domain in set(q['domain'] for q in QUESTION_INFO.values()):\n",
        "        mean_diffs = {}\n",
        "        for v_idx in range(N_VOCAB_VARIANTS):\n",
        "            key = (v_idx, demo, domain)\n",
        "            if key in diff_storage and len(diff_storage[key]) >= 5:\n",
        "                mean_diffs[v_idx] = np.mean(np.stack(diff_storage[key]), axis=0)\n",
        "\n",
        "        if len(mean_diffs) < 2: continue\n",
        "\n",
        "        top_k = {v: set(np.argsort(np.abs(md))[::-1][:N_FEATURES_CHECK].tolist())\n",
        "                  for v, md in mean_diffs.items()}\n",
        "\n",
        "        for vi, vj in [(0, 1), (0, 2), (1, 2)]:\n",
        "            if vi not in mean_diffs or vj not in mean_diffs: continue\n",
        "            md_i, md_j = mean_diffs[vi], mean_diffs[vj]\n",
        "\n",
        "            rho, p_rho = spearmanr(md_i, md_j)\n",
        "            cos_sim = 1.0 - cosine_dist(md_i, md_j)\n",
        "            pearson_r = np.corrcoef(md_i, md_j)[0, 1]\n",
        "            jaccard = len(top_k[vi] & top_k[vj]) / len(top_k[vi] | top_k[vj])\n",
        "            overlap = len(top_k[vi] & top_k[vj])\n",
        "\n",
        "            # Rank correlation among top features\n",
        "            union_feats = list(top_k[vi] | top_k[vj])\n",
        "            rank_i = {f: r for r, f in enumerate(np.argsort(np.abs(md_i))[::-1])}\n",
        "            rank_j = {f: r for r, f in enumerate(np.argsort(np.abs(md_j))[::-1])}\n",
        "            rank_rho, _ = spearmanr([rank_i[f] for f in union_feats],\n",
        "                                     [rank_j[f] for f in union_feats])\n",
        "\n",
        "            results.append({\n",
        "                'demographic': demo, 'domain': domain, 'vocab_i': vi, 'vocab_j': vj,\n",
        "                'spearman_rho': float(rho), 'spearman_p': float(p_rho),\n",
        "                'cosine_similarity': float(cos_sim), 'pearson_r': float(pearson_r),\n",
        "                'jaccard_top50': float(jaccard), 'overlap_count_top50': int(overlap),\n",
        "                'rank_rho_union': float(rank_rho),\n",
        "            })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# --- Summary ---\n",
        "\n",
        "print(f\"\\nResults ({len(results_df)} comparisons):\")\n",
        "for metric in ['spearman_rho', 'cosine_similarity', 'jaccard_top50', 'rank_rho_union']:\n",
        "    vals = results_df[metric].dropna()\n",
        "    print(f\"  {metric}: mean={vals.mean():.3f}, median={vals.median():.3f}, \"\n",
        "          f\"min={vals.min():.3f}, max={vals.max():.3f}\")\n",
        "\n",
        "print(\"\\nBy demographic:\")\n",
        "for demo in ALL_DEMOGRAPHICS:\n",
        "    sub = results_df[results_df['demographic'] == demo]\n",
        "    if len(sub) == 0: continue\n",
        "    print(f\"  {demo}: ρ={sub['spearman_rho'].mean():.3f}, \"\n",
        "          f\"cos={sub['cosine_similarity'].mean():.3f}, \"\n",
        "          f\"J={sub['jaccard_top50'].mean():.3f} ({sub['overlap_count_top50'].mean():.0f} overlap)\")\n",
        "\n",
        "# --- Save ---\n",
        "\n",
        "results_df.to_csv(OUTPUT_DIR / 'vocab_sensitivity_results.csv', index=False)\n",
        "\n",
        "summary = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'layer': TARGET_LAYER,\n",
        "    'n_respondents': len(resp_df),\n",
        "    'n_questions': len(QUESTION_INFO),\n",
        "    'n_vocab_variants': N_VOCAB_VARIANTS,\n",
        "    'k_features': N_FEATURES_CHECK,\n",
        "    'overall': {\n",
        "        'spearman_rho': float(results_df['spearman_rho'].mean()),\n",
        "        'cosine_similarity': float(results_df['cosine_similarity'].mean()),\n",
        "        'jaccard_top50': float(results_df['jaccard_top50'].mean()),\n",
        "        'overlap_count_top50': float(results_df['overlap_count_top50'].mean()),\n",
        "    },\n",
        "    'by_demographic': {\n",
        "        demo: {\n",
        "            'spearman_rho': float(sub['spearman_rho'].mean()),\n",
        "            'cosine_similarity': float(sub['cosine_similarity'].mean()),\n",
        "            'jaccard_top50': float(sub['jaccard_top50'].mean()),\n",
        "        }\n",
        "        for demo in ALL_DEMOGRAPHICS\n",
        "        for sub in [results_df[results_df['demographic'] == demo]]\n",
        "        if len(sub) > 0\n",
        "    },\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / 'vocab_sensitivity_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved to {OUTPUT_DIR}/\")\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "4CEwrFtj8K9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction — all 8 layers in a single forward pass\n",
        "#\n",
        "# Extracts SAE activations and computes expected values for all contrastive\n",
        "# prompt pairs. Stores per-layer activation diffs grouped by (demographic, domain).\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "DEVICE = config.device\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "ANALYSIS_LAYERS = config.candidate_layers  # [5, 9, 14, 18, 20, 27, 32, 36]\n",
        "ALL_DEMOGRAPHICS = ['income', 'age', 'gender', 'education', 'vote']\n",
        "ALL_DOMAINS = ['climate', 'health', 'digital', 'economy', 'values']\n",
        "\n",
        "DOMAIN_MAP = {\n",
        "    'other': 'values', 'climate': 'climate', 'health': 'health',\n",
        "    'digital': 'digital', 'economy': 'economy', 'values': 'values',\n",
        "}\n",
        "\n",
        "INPUT_DIR = config.output_dir\n",
        "OUTPUT_DIR = INPUT_DIR / \"feature_extraction\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Layers: {ANALYSIS_LAYERS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "\n",
        "# --- Load prompts ---\n",
        "\n",
        "prompts_df = pd.read_parquet(INPUT_DIR / 'prompts_selection.parquet')\n",
        "if 'vocab_idx' in prompts_df.columns:\n",
        "    prompts_df = prompts_df[prompts_df['vocab_idx'] == 0].reset_index(drop=True)\n",
        "prompts_df['domain'] = prompts_df['domain'].map(DOMAIN_MAP).fillna(prompts_df['domain'])\n",
        "\n",
        "print(f\"Prompts: {len(prompts_df):,}\")\n",
        "\n",
        "pair_counts = prompts_df.groupby('pair_key').size()\n",
        "complete_pairs = (pair_counts == 2).sum()\n",
        "orphan_pairs = (pair_counts == 1).sum()\n",
        "print(f\"Complete pairs: {complete_pairs:,}\")\n",
        "if orphan_pairs > 0:\n",
        "    print(f\"  Orphan pairs: {orphan_pairs:,}\")\n",
        "\n",
        "\n",
        "# --- Token handling and EV computation ---\n",
        "\n",
        "def _build_token_info():\n",
        "    info = {}\n",
        "    for i in range(0, 11):\n",
        "        encoded = tokenizer.encode(str(i), add_special_tokens=False)\n",
        "        if len(encoded) == 1:\n",
        "            info[i] = {'type': 'single', 'token_id': encoded[0]}\n",
        "        else:\n",
        "            info[i] = {'type': 'multi', 'token_ids': encoded,\n",
        "                       'first_token_id': encoded[0], 'second_token_id': encoded[1]}\n",
        "    return info\n",
        "\n",
        "TOKEN_INFO = _build_token_info()\n",
        "\n",
        "\n",
        "def compute_response_metrics(logits, scale_min, scale_max):\n",
        "    \"\"\"Compute expected value, confidence, and top answer from logits.\"\"\"\n",
        "    has_multi = scale_max >= 10 and TOKEN_INFO.get(10, {}).get('type') == 'multi'\n",
        "\n",
        "    if not has_multi:\n",
        "        tokens = [TOKEN_INFO[i]['token_id'] for i in range(scale_min, scale_max + 1)]\n",
        "        logits_subset = logits[tokens].float()\n",
        "        probs = F.softmax(logits_subset, dim=0).cpu().numpy()\n",
        "        values = np.arange(scale_min, scale_max + 1)\n",
        "        ev = float(np.dot(values, probs))\n",
        "        return ev, float(probs.max()), int(values[probs.argmax()])\n",
        "\n",
        "    # Handle 0-10 scales where '10' tokenizes as two tokens\n",
        "    single_max = min(scale_max, 9)\n",
        "    single_tokens = [TOKEN_INFO[i]['token_id'] for i in range(scale_min, single_max + 1)]\n",
        "    all_logits = logits[single_tokens].float()\n",
        "    all_probs = F.softmax(all_logits, dim=0).cpu().numpy()\n",
        "\n",
        "    values = list(range(scale_min, single_max + 1))\n",
        "    probs_list = list(all_probs)\n",
        "\n",
        "    # Split shared first-token probability between 1 and 10\n",
        "    if 1 in values:\n",
        "        idx_of_1 = values.index(1)\n",
        "        p1_raw = probs_list[idx_of_1]\n",
        "        probs_list[idx_of_1] = p1_raw / 2.0\n",
        "        values.append(10)\n",
        "        probs_list.append(p1_raw / 2.0)\n",
        "    else:\n",
        "        values.append(10)\n",
        "        probs_list.append(0.0)\n",
        "\n",
        "    values = np.array(values)\n",
        "    probs = np.array(probs_list)\n",
        "    probs = probs / probs.sum()\n",
        "    ev = float(np.dot(values, probs))\n",
        "    return ev, float(probs.max()), int(values[probs.argmax()])\n",
        "\n",
        "\n",
        "# --- Extraction ---\n",
        "\n",
        "n = len(prompts_df)\n",
        "\n",
        "pair_keys = prompts_df['pair_key'].values\n",
        "demographics = prompts_df['demographic'].values\n",
        "domains = prompts_df['domain'].values\n",
        "value_types = prompts_df['value_type'].values\n",
        "scale_mins = prompts_df['scale_min'].values\n",
        "scale_maxs = prompts_df['scale_max'].values\n",
        "question_ids = prompts_df['question_id'].values\n",
        "respondent_ids = prompts_df['respondent_id'].values\n",
        "\n",
        "hook_names = [f\"blocks.{layer}.hook_resid_post\" for layer in ANALYSIS_LAYERS]\n",
        "\n",
        "def hook_filter(name):\n",
        "    return any(h in name for h in hook_names)\n",
        "\n",
        "all_evs = np.zeros(n, dtype=np.float32)\n",
        "all_confidences = np.zeros(n, dtype=np.float32)\n",
        "all_top_answers = np.zeros(n, dtype=np.int32)\n",
        "\n",
        "# Per-layer diff accumulation\n",
        "diff_data = {}\n",
        "activation_buffer = {}\n",
        "\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    diff_data[layer] = {}\n",
        "    for demo in ALL_DEMOGRAPHICS:\n",
        "        for domain in ALL_DOMAINS:\n",
        "            diff_data[layer][(demo, domain)] = {\n",
        "                'diffs': [], 'ev_a': [], 'ev_b': [],\n",
        "                'confidence_a': [], 'confidence_b': [], 'pairs': []\n",
        "            }\n",
        "\n",
        "# Load all SAEs upfront (~500MB each, 8 total fits in VRAM)\n",
        "print(\"\\nLoading all SAEs...\")\n",
        "saes = {}\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    saes[layer] = sae_manager.load_sae(layer)\n",
        "    sae_manager._current_sae = None\n",
        "    sae_manager._current_layer = None\n",
        "    print(f\"  Layer {layer}: d_sae={saes[layer].d_sae}\")\n",
        "\n",
        "print(f\"\\nStarting extraction ({len(saes)} layers per forward pass)...\")\n",
        "t0_total = datetime.now()\n",
        "n_batches = (n + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "for batch_start in tqdm(range(0, n, BATCH_SIZE), total=n_batches, desc=\"Extracting\"):\n",
        "    batch_end = min(batch_start + BATCH_SIZE, n)\n",
        "    batch_size = batch_end - batch_start\n",
        "\n",
        "    batch_prompts = prompts_df.iloc[batch_start:batch_end]['prompt'].tolist()\n",
        "    tokens = tokenizer(batch_prompts, return_tensors='pt',\n",
        "                       padding=True, truncation=True, max_length=512)\n",
        "    input_ids = tokens['input_ids'].to(DEVICE)\n",
        "    attention_mask = tokens['attention_mask'].to(DEVICE)\n",
        "    seq_lens = attention_mask.sum(dim=1) - 1\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        logits, cache = model.run_with_cache(\n",
        "            input_ids, names_filter=hook_filter\n",
        "        )\n",
        "\n",
        "        batch_logits = torch.stack([\n",
        "            logits[j, seq_lens[j], :] for j in range(batch_size)\n",
        "        ]).float().cpu()\n",
        "\n",
        "        layer_sae_acts = {}\n",
        "        for layer in ANALYSIS_LAYERS:\n",
        "            hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
        "            residuals = torch.stack([\n",
        "                cache[hook_name][j, seq_lens[j], :] for j in range(batch_size)\n",
        "            ]).float()\n",
        "            layer_sae_acts[layer] = saes[layer].encode(residuals).float().cpu().numpy()\n",
        "\n",
        "        del cache, logits\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        idx = batch_start + j\n",
        "\n",
        "        ev, conf, top_ans = compute_response_metrics(\n",
        "            batch_logits[j], scale_mins[idx], scale_maxs[idx]\n",
        "        )\n",
        "        all_evs[idx] = ev\n",
        "        all_confidences[idx] = conf\n",
        "        all_top_answers[idx] = top_ans\n",
        "\n",
        "        pk = pair_keys[idx]\n",
        "        vtype = value_types[idx]\n",
        "\n",
        "        if pk not in activation_buffer:\n",
        "            activation_buffer[pk] = {\n",
        "                'demo': demographics[idx],\n",
        "                'domain': domains[idx],\n",
        "                'question_id': question_ids[idx],\n",
        "                'respondent_id': respondent_ids[idx],\n",
        "                'scale_min': scale_mins[idx],\n",
        "                'scale_max': scale_maxs[idx],\n",
        "            }\n",
        "\n",
        "        activation_buffer[pk][vtype] = {'ev': ev, 'confidence': conf, 'idx': idx}\n",
        "        for layer in ANALYSIS_LAYERS:\n",
        "            activation_buffer[pk].setdefault(f'sae_{layer}', {})\n",
        "            activation_buffer[pk][f'sae_{layer}'][vtype] = layer_sae_acts[layer][j]\n",
        "\n",
        "        # Complete pair — compute diffs\n",
        "        if 'value_a' in activation_buffer[pk] and 'value_b' in activation_buffer[pk]:\n",
        "            info = activation_buffer[pk]\n",
        "            key = (info['demo'], info['domain'])\n",
        "\n",
        "            for layer in ANALYSIS_LAYERS:\n",
        "                sae_a = info[f'sae_{layer}']['value_a']\n",
        "                sae_b = info[f'sae_{layer}']['value_b']\n",
        "\n",
        "                diff_data[layer][key]['diffs'].append(sae_a - sae_b)\n",
        "                diff_data[layer][key]['ev_a'].append(info['value_a']['ev'])\n",
        "                diff_data[layer][key]['ev_b'].append(info['value_b']['ev'])\n",
        "                diff_data[layer][key]['confidence_a'].append(info['value_a']['confidence'])\n",
        "                diff_data[layer][key]['confidence_b'].append(info['value_b']['confidence'])\n",
        "                diff_data[layer][key]['pairs'].append({\n",
        "                    'pair_key': pk,\n",
        "                    'question_id': info['question_id'],\n",
        "                    'respondent_id': info['respondent_id'],\n",
        "                })\n",
        "\n",
        "            del activation_buffer[pk]\n",
        "\n",
        "    if (batch_start // BATCH_SIZE) % 200 == 0 and batch_start > 0:\n",
        "        elapsed = (datetime.now() - t0_total).total_seconds() / 60\n",
        "        pct = batch_start / n * 100\n",
        "        eta = elapsed / (pct / 100) - elapsed if pct > 0 else 0\n",
        "        print(f\"  {pct:.0f}% | {elapsed:.1f} min elapsed | ~{eta:.0f} min remaining | \"\n",
        "              f\"buffer: {len(activation_buffer)} pending\")\n",
        "\n",
        "n_orphans = len(activation_buffer)\n",
        "if n_orphans > 0:\n",
        "    print(f\"\\n  {n_orphans} orphan pairs skipped\")\n",
        "activation_buffer.clear()\n",
        "\n",
        "elapsed_total = (datetime.now() - t0_total).total_seconds() / 60\n",
        "print(f\"\\nExtraction complete: {elapsed_total:.1f} min\")\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    total_pairs = sum(len(d['diffs']) for d in diff_data[layer].values())\n",
        "    print(f\"  Layer {layer}: {total_pairs:,} pairs\")\n",
        "\n",
        "# Build prompt results (EVs are layer-independent)\n",
        "prompt_results_base = prompts_df[['pair_key', 'respondent_id', 'question_id',\n",
        "                                   'domain', 'demographic', 'value_type']].copy()\n",
        "prompt_results_base['ev'] = all_evs\n",
        "prompt_results_base['confidence'] = all_confidences\n",
        "prompt_results_base['top_answer'] = all_top_answers\n",
        "\n",
        "# Free SAEs\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    del saes[layer]\n",
        "del saes\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"SAEs unloaded\")"
      ],
      "metadata": {
        "id": "EJnWzi-criWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical feature selection — per layer, per (demographic, domain) group\n",
        "#\n",
        "# Three-stage filtering: noise filters, FDR-corrected t-tests, effect size threshold.\n",
        "# Top K=50 features selected per group, ranked by effect size magnitude.\n",
        "\n",
        "from scipy.stats import ttest_1samp, ttest_rel\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "N_FEATURES_SELECT = 50\n",
        "MIN_EFFECT_SIZE = 0.3\n",
        "FDR_ALPHA = 0.05\n",
        "\n",
        "# Noise filter thresholds\n",
        "SIGN_AGREEMENT_THRESHOLD = 0.60\n",
        "CV_THRESHOLD = 3.0\n",
        "OUTLIER_RATIO_THRESHOLD = 0.10\n",
        "\n",
        "\n",
        "def select_features_with_stats(layer_diff_data, layer):\n",
        "    \"\"\"Select top-K features per (demographic, domain) group.\n",
        "\n",
        "    Pipeline: noise filtering -> FDR-corrected t-tests -> effect size threshold.\n",
        "    Falls back to ranking by mean absolute diff if no features pass all criteria.\n",
        "    \"\"\"\n",
        "    selected_features = {}\n",
        "    all_stats = []\n",
        "    funnel_stats = []\n",
        "\n",
        "    for (demo, domain), data in layer_diff_data.items():\n",
        "        if len(data['diffs']) < 10:\n",
        "            continue\n",
        "\n",
        "        diffs = np.stack(data['diffs'])\n",
        "        n_pairs, n_feats = diffs.shape\n",
        "        mean_diffs = np.mean(diffs, axis=0)\n",
        "        std_diffs = np.std(diffs, axis=0, ddof=1)\n",
        "\n",
        "        # --- Noise filters ---\n",
        "        signs = np.sign(diffs)\n",
        "        mean_signs = np.sign(mean_diffs)\n",
        "        sign_agreement = np.mean(signs == mean_signs[np.newaxis, :], axis=0)\n",
        "        sign_consistent = sign_agreement >= SIGN_AGREEMENT_THRESHOLD\n",
        "\n",
        "        cv = np.full(n_feats, np.inf)\n",
        "        nonzero_mean = np.abs(mean_diffs) > 1e-10\n",
        "        cv[nonzero_mean] = std_diffs[nonzero_mean] / np.abs(mean_diffs[nonzero_mean])\n",
        "        low_noise = cv < CV_THRESHOLD\n",
        "\n",
        "        z_scores = np.abs((diffs - mean_diffs[np.newaxis, :]) / (std_diffs[np.newaxis, :] + 1e-10))\n",
        "        outlier_ratio = np.mean(z_scores > 3, axis=0)\n",
        "        few_outliers = outlier_ratio < OUTLIER_RATIO_THRESHOLD\n",
        "\n",
        "        not_noisy = sign_consistent & low_noise & few_outliers\n",
        "        is_noisy = ~not_noisy\n",
        "\n",
        "        # Cohen's d\n",
        "        cohens_d = np.zeros(n_feats)\n",
        "        has_variance = std_diffs > 1e-10\n",
        "        cohens_d[has_variance] = mean_diffs[has_variance] / std_diffs[has_variance]\n",
        "\n",
        "        # --- t-tests on clean features ---\n",
        "        testable = not_noisy & has_variance\n",
        "        t_stats = np.zeros(n_feats)\n",
        "        p_values = np.ones(n_feats)\n",
        "\n",
        "        testable_idx = np.where(testable)[0]\n",
        "        if len(testable_idx) > 0:\n",
        "            t_result, p_result = ttest_1samp(diffs[:, testable_idx], 0, axis=0)\n",
        "            valid_results = ~np.isnan(p_result)\n",
        "            valid_idx = testable_idx[valid_results]\n",
        "            t_stats[valid_idx] = t_result[valid_results]\n",
        "            p_values[valid_idx] = p_result[valid_results]\n",
        "\n",
        "        # --- FDR correction (per group) ---\n",
        "        valid_mask = not_noisy & (p_values < 1.0)\n",
        "        p_corrected = np.ones(n_feats)\n",
        "        if valid_mask.sum() > 0:\n",
        "            valid_p = p_values[valid_mask]\n",
        "            try:\n",
        "                rejected, corrected, _, _ = multipletests(valid_p, method='fdr_bh', alpha=FDR_ALPHA)\n",
        "                p_corrected[valid_mask] = corrected\n",
        "            except Exception:\n",
        "                p_corrected[valid_mask] = valid_p\n",
        "\n",
        "        # --- Selection ---\n",
        "        significant = p_corrected < FDR_ALPHA\n",
        "        large_effect = np.abs(cohens_d) >= MIN_EFFECT_SIZE\n",
        "        candidates = significant & large_effect & not_noisy\n",
        "        candidate_idx = np.where(candidates)[0]\n",
        "        selection_method = 'significant'\n",
        "\n",
        "        if len(candidate_idx) == 0:\n",
        "            # Fallback: rank by mean absolute diff among clean features\n",
        "            noisy_free_idx = np.where(not_noisy)[0]\n",
        "            if len(noisy_free_idx) > 0:\n",
        "                sorted_idx = noisy_free_idx[np.argsort(np.abs(mean_diffs[noisy_free_idx]))[::-1]]\n",
        "                candidate_idx = sorted_idx[:N_FEATURES_SELECT]\n",
        "                selection_method = 'fallback_noise_filtered'\n",
        "            else:\n",
        "                candidate_idx = np.argsort(np.abs(mean_diffs))[::-1][:N_FEATURES_SELECT]\n",
        "                selection_method = 'fallback_unfiltered'\n",
        "\n",
        "        sorted_candidates = candidate_idx[np.argsort(np.abs(mean_diffs[candidate_idx]))[::-1]]\n",
        "        top_features = sorted_candidates[:N_FEATURES_SELECT]\n",
        "\n",
        "        # Funnel diagnostics\n",
        "        funnel_stats.append({\n",
        "            'layer': layer,\n",
        "            'demographic': demo,\n",
        "            'domain': domain,\n",
        "            'n_total_features': int(n_feats),\n",
        "            'n_has_variance': int(has_variance.sum()),\n",
        "            'n_sign_consistent': int(sign_consistent.sum()),\n",
        "            'n_low_cv': int(low_noise.sum()),\n",
        "            'n_few_outliers': int(few_outliers.sum()),\n",
        "            'n_not_noisy': int(not_noisy.sum()),\n",
        "            'n_noisy': int(is_noisy.sum()),\n",
        "            'pct_noisy': round(float(is_noisy.mean() * 100), 2),\n",
        "            'n_testable': int(testable.sum()),\n",
        "            'n_fdr_significant': int(significant.sum()),\n",
        "            'n_large_effect': int(large_effect.sum()),\n",
        "            'n_large_effect_all': int((np.abs(cohens_d) >= MIN_EFFECT_SIZE).sum()),\n",
        "            'n_candidates': int(candidates.sum()),\n",
        "            'n_selected': int(len(top_features)),\n",
        "            'selection_method': selection_method,\n",
        "            'mean_abs_d_clean': float(np.abs(cohens_d[not_noisy]).mean()) if not_noisy.sum() > 0 else 0,\n",
        "            'mean_abs_d_noisy': float(np.abs(cohens_d[is_noisy]).mean()) if is_noisy.sum() > 0 else 0,\n",
        "            'mean_sign_agreement': float(sign_agreement.mean()),\n",
        "            'mean_cv_finite': float(cv[np.isfinite(cv)].mean()) if np.isfinite(cv).sum() > 0 else float('nan'),\n",
        "            'mean_outlier_ratio': float(outlier_ratio.mean()),\n",
        "        })\n",
        "\n",
        "        # Behavioral effect size\n",
        "        ev_a_arr = np.array(data['ev_a'])\n",
        "        ev_b_arr = np.array(data['ev_b'])\n",
        "        ev_diffs = ev_a_arr - ev_b_arr\n",
        "        behavioral_effect = float(np.mean(ev_diffs))\n",
        "\n",
        "        if len(ev_a_arr) >= 2 and np.std(ev_diffs) > 1e-10:\n",
        "            t_beh, p_beh = ttest_rel(ev_a_arr, ev_b_arr)\n",
        "            behavioral_t = float(t_beh)\n",
        "            behavioral_p = float(p_beh)\n",
        "        else:\n",
        "            behavioral_t = float('nan')\n",
        "            behavioral_p = float('nan')\n",
        "\n",
        "        selected_features[(demo, domain)] = {\n",
        "            'features': top_features.tolist(),\n",
        "            'mean_diffs': mean_diffs[top_features].tolist(),\n",
        "            'cohens_d': cohens_d[top_features].tolist(),\n",
        "            'p_values': p_values[top_features].tolist(),\n",
        "            'p_corrected': p_corrected[top_features].tolist(),\n",
        "            'n_pairs': n_pairs,\n",
        "            'behavioral_effect': behavioral_effect,\n",
        "            'behavioral_t': behavioral_t,\n",
        "            'behavioral_p': behavioral_p,\n",
        "            'n_significant': int(significant.sum()),\n",
        "            'n_candidates': int(candidates.sum()),\n",
        "            'n_not_noisy': int(not_noisy.sum()),\n",
        "            'selection_method': selection_method,\n",
        "            'layer': layer,\n",
        "        }\n",
        "\n",
        "        for i in top_features:\n",
        "            all_stats.append({\n",
        "                'demographic': demo, 'domain': domain,\n",
        "                'feature_idx': int(i),\n",
        "                'mean_diff': float(mean_diffs[i]),\n",
        "                'cohens_d': float(cohens_d[i]),\n",
        "                'p_value': float(p_values[i]),\n",
        "                'p_corrected': float(p_corrected[i]),\n",
        "                'significant': bool(significant[i]),\n",
        "                'large_effect': bool(large_effect[i]),\n",
        "                'sign_agreement': float(sign_agreement[i]),\n",
        "                'cv': float(cv[i]) if not np.isinf(cv[i]) else float('nan'),\n",
        "                'outlier_ratio': float(outlier_ratio[i]),\n",
        "                'is_noisy': bool(is_noisy[i]),\n",
        "                'feature_significant': bool(significant[i] & large_effect[i] & not_noisy[i]),\n",
        "                'selection_method': selection_method,\n",
        "                'layer': layer,\n",
        "            })\n",
        "\n",
        "        # Per-group status\n",
        "        beh_sig = (\"***\" if (not np.isnan(behavioral_p) and behavioral_p < 0.001) else\n",
        "                   \"**\" if (not np.isnan(behavioral_p) and behavioral_p < 0.01) else\n",
        "                   \"*\" if (not np.isnan(behavioral_p) and behavioral_p < 0.05) else \"ns\")\n",
        "        method_tag = \"\" if selection_method == 'significant' else f\" [{selection_method}]\"\n",
        "        print(f\"  ({demo}, {domain}): {n_pairs} pairs, {significant.sum()} sig, \"\n",
        "              f\"{candidates.sum()} cand -> {len(top_features)} sel | \"\n",
        "              f\"dEV={behavioral_effect:.3f} {beh_sig}{method_tag}\")\n",
        "\n",
        "    # Summary\n",
        "    method_counts = {}\n",
        "    for info in selected_features.values():\n",
        "        m = info['selection_method']\n",
        "        method_counts[m] = method_counts.get(m, 0) + 1\n",
        "    print(f\"\\n  Layer {layer} summary:\")\n",
        "    for method, count in sorted(method_counts.items()):\n",
        "        print(f\"    {method}: {count}\")\n",
        "\n",
        "    return selected_features, pd.DataFrame(all_stats), pd.DataFrame(funnel_stats)\n",
        "\n",
        "\n",
        "# --- Run selection for each layer ---\n",
        "\n",
        "all_selected_features = {}\n",
        "all_stats = []\n",
        "all_funnel = []\n",
        "\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    print(f\"\\nLayer {layer} ({LAYER_SAE_CONFIG[layer]['depth_pct']}% depth)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    selected, stats_df, funnel_df = select_features_with_stats(diff_data[layer], layer)\n",
        "    all_selected_features[str(layer)] = selected\n",
        "    all_stats.append(stats_df)\n",
        "    all_funnel.append(funnel_df)\n",
        "\n",
        "    del diff_data[layer]\n",
        "    gc.collect()\n",
        "\n",
        "del diff_data\n",
        "\n",
        "\n",
        "# --- Cross-layer summary ---\n",
        "\n",
        "print(\"\\nCross-layer summary:\")\n",
        "for layer_str, features_dict in all_selected_features.items():\n",
        "    layer = int(layer_str)\n",
        "    n_sig = sum(1 for v in features_dict.values() if v['selection_method'] == 'significant')\n",
        "    n_fb = sum(1 for v in features_dict.values() if v['selection_method'] != 'significant')\n",
        "    avg_cand = np.mean([v['n_candidates'] for v in features_dict.values()])\n",
        "    sig_beh = sum(1 for v in features_dict.values()\n",
        "                  if not np.isnan(v['behavioral_p']) and v['behavioral_p'] < 0.05)\n",
        "    depth = LAYER_SAE_CONFIG[layer]['depth_pct']\n",
        "    print(f\"  L{layer:>2} ({depth:>2}%): {n_sig} sig + {n_fb} fallback | \"\n",
        "          f\"avg cand: {avg_cand:.0f} | beh sig: {sig_beh}/{len(features_dict)}\")\n",
        "\n",
        "\n",
        "# --- Save ---\n",
        "\n",
        "def convert_for_json(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {(f\"{k[0]}_{k[1]}\" if isinstance(k, tuple) else str(k)): convert_for_json(v)\n",
        "                for k, v in obj.items()}\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        return [convert_for_json(i) for i in obj]\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, (np.int64, np.int32)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.float64, np.float32)):\n",
        "        return None if np.isnan(obj) else float(obj)\n",
        "    elif isinstance(obj, float) and np.isnan(obj):\n",
        "        return None\n",
        "    return obj\n",
        "\n",
        "with open(OUTPUT_DIR / 'selected_features.json', 'w') as f:\n",
        "    json.dump(convert_for_json(all_selected_features), f, indent=2)\n",
        "print(f\"Saved selected_features.json\")\n",
        "\n",
        "stats_combined = pd.concat(all_stats, ignore_index=True)\n",
        "stats_combined.to_csv(OUTPUT_DIR / 'feature_stats.csv', index=False)\n",
        "print(f\"Saved feature_stats.csv ({len(stats_combined):,} rows)\")\n",
        "\n",
        "funnel_combined = pd.concat(all_funnel, ignore_index=True)\n",
        "funnel_combined.to_csv(OUTPUT_DIR / 'filtering_funnel.csv', index=False)\n",
        "print(f\"Saved filtering_funnel.csv ({len(funnel_combined):,} rows)\")\n",
        "\n",
        "# Funnel summary\n",
        "print(\"\\nFiltering funnel (averaged per layer):\")\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    lf = funnel_combined[funnel_combined['layer'] == layer]\n",
        "    if len(lf) == 0:\n",
        "        continue\n",
        "    depth = LAYER_SAE_CONFIG[layer]['depth_pct']\n",
        "    print(f\"  L{layer:>2} ({depth:>2}%): \"\n",
        "          f\"{lf['n_total_features'].iloc[0]:,} -> \"\n",
        "          f\"{lf['n_has_variance'].mean():.0f} variance -> \"\n",
        "          f\"{lf['n_not_noisy'].mean():.0f} clean ({lf['pct_noisy'].mean():.1f}% noisy) -> \"\n",
        "          f\"{lf['n_fdr_significant'].mean():.0f} FDR sig -> \"\n",
        "          f\"{lf['n_candidates'].mean():.0f} cand -> \"\n",
        "          f\"{lf['n_selected'].mean():.0f} selected\")\n",
        "\n",
        "# Prompt results (EVs are layer-independent; replicate with layer column for compatibility)\n",
        "all_prompt_results = []\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    pr = prompt_results_base.copy()\n",
        "    pr['layer'] = layer\n",
        "    all_prompt_results.append(pr)\n",
        "prompt_results_combined = pd.concat(all_prompt_results, ignore_index=True)\n",
        "prompt_results_combined.to_parquet(OUTPUT_DIR / 'prompt_results.parquet', index=False)\n",
        "print(f\"Saved prompt_results.parquet ({len(prompt_results_combined):,} rows)\")\n",
        "\n",
        "metadata = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'model': 'google/gemma-2-9b-it',\n",
        "    'sae_repos': {\n",
        "        'pt': 'google/gemma-scope-9b-pt-res',\n",
        "        'it': 'google/gemma-scope-9b-it-res',\n",
        "    },\n",
        "    'sae_width': '16k',\n",
        "    'sae_activation': 'JumpReLU (no b_dec centering)',\n",
        "    'layers': ANALYSIS_LAYERS,\n",
        "    'n_features_select': N_FEATURES_SELECT,\n",
        "    'min_effect_size': MIN_EFFECT_SIZE,\n",
        "    'fdr_alpha': FDR_ALPHA,\n",
        "    'n_prompts': len(prompts_df),\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'optimization': 'multi-layer single forward pass',\n",
        "    'seed': SEED,\n",
        "    'noise_filter_thresholds': {\n",
        "        'sign_agreement': SIGN_AGREEMENT_THRESHOLD,\n",
        "        'cv': CV_THRESHOLD,\n",
        "        'outlier_ratio': OUTLIER_RATIO_THRESHOLD,\n",
        "    },\n",
        "    'fdr_scope': 'per_demo_domain_group',\n",
        "    'extraction_time_minutes': round(elapsed_total, 1),\n",
        "    'selection_summary': {\n",
        "        str(layer): {\n",
        "            'significant': sum(1 for v in feats.values() if v['selection_method'] == 'significant'),\n",
        "            'fallback': sum(1 for v in feats.values() if v['selection_method'] != 'significant'),\n",
        "        }\n",
        "        for layer, feats in all_selected_features.items()\n",
        "    },\n",
        "    'layer_sae_types': {\n",
        "        str(layer): LAYER_SAE_CONFIG[layer]['repo'].upper()\n",
        "        for layer in ANALYSIS_LAYERS\n",
        "    },\n",
        "}\n",
        "with open(OUTPUT_DIR / 'extraction_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"Saved extraction_metadata.json\")\n",
        "\n",
        "print(f\"\\nDone. Output: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "HxSvFSqnr9Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction analysis — figures (part 1 of 2)\n",
        "# Figures 1-5: encoding heatmaps, encoding trends, behavioral effects,\n",
        "# feature quality, domain overlap\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from itertools import combinations as combs\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans',\n",
        "    'font.size': 11,\n",
        "    'axes.labelsize': 12,\n",
        "    'axes.titlesize': 13,\n",
        "    'legend.fontsize': 10,\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 300,\n",
        "    'savefig.bbox': 'tight',\n",
        "})\n",
        "\n",
        "INPUT_DIR = Path(\"./outputs_gemma_replication/feature_extraction\")\n",
        "BUNDLE = Path(\"./feature-extraction-results-gemma\")\n",
        "BUNDLE.mkdir(exist_ok=True)\n",
        "FIGS   = BUNDLE / \"figures\";   FIGS.mkdir(exist_ok=True)\n",
        "TABLES = BUNDLE / \"tables\";    TABLES.mkdir(exist_ok=True)\n",
        "DATA   = BUNDLE / \"data\";      DATA.mkdir(exist_ok=True)\n",
        "\n",
        "ALL_DEMOS   = ['income', 'age', 'gender', 'education', 'vote']\n",
        "ALL_DOMAINS = ['climate', 'health', 'digital', 'economy', 'values']\n",
        "DEMO_SHORT  = ['Inco', 'Age', 'Gend', 'Educ', 'Vote']\n",
        "\n",
        "N_MODEL_LAYERS = 41\n",
        "LAYER_DEPTH = {5: 12, 9: 22, 14: 34, 18: 44, 20: 49, 27: 66, 32: 78, 36: 88}\n",
        "IT_LAYERS = {20}\n",
        "\n",
        "COLORS = {\n",
        "    'income': '#1abc9c', 'age': '#9b59b6', 'gender': '#e91e63',\n",
        "    'education': '#3498db', 'vote': '#f44336',\n",
        "}\n",
        "\n",
        "\n",
        "def layer_label(layer):\n",
        "    depth = LAYER_DEPTH.get(layer, int(layer / N_MODEL_LAYERS * 100))\n",
        "    tag = \" (IT)\" if layer in IT_LAYERS else \"\"\n",
        "    return f\"L{layer} ({depth}%{tag})\"\n",
        "\n",
        "\n",
        "def make_2x4_grid(layers):\n",
        "    if len(layers) <= 4:\n",
        "        return 1, len(layers)\n",
        "    return 2, 4\n",
        "\n",
        "\n",
        "# --- Load ---\n",
        "\n",
        "print(\"Loading...\")\n",
        "\n",
        "stats_df = pd.read_csv(INPUT_DIR / 'feature_stats.csv')\n",
        "with open(INPUT_DIR / 'selected_features.json') as f:\n",
        "    selected_features = json.load(f)\n",
        "\n",
        "behav_rows = []\n",
        "for layer_str, layer_data in selected_features.items():\n",
        "    layer = int(layer_str)\n",
        "    for key_str, info in layer_data.items():\n",
        "        parts = key_str.split('_', 1)\n",
        "        demo = parts[0]\n",
        "        domain = parts[1] if len(parts) > 1 else 'unknown'\n",
        "        behav_rows.append({\n",
        "            'layer': layer,\n",
        "            'demographic': demo,\n",
        "            'domain': domain,\n",
        "            'effect': info.get('behavioral_effect', 0),\n",
        "            'p_value': info.get('behavioral_p', None),\n",
        "            't_stat': info.get('behavioral_t', None),\n",
        "            'n_pairs': info.get('n_pairs', 0),\n",
        "            'selection_method': info.get('selection_method', 'unknown'),\n",
        "        })\n",
        "behav_df = pd.DataFrame(behav_rows)\n",
        "behav_df['p_value'] = pd.to_numeric(behav_df['p_value'], errors='coerce')\n",
        "behav_df['t_stat'] = pd.to_numeric(behav_df['t_stat'], errors='coerce')\n",
        "\n",
        "LAYERS = sorted(stats_df['layer'].unique().tolist())\n",
        "\n",
        "print(f\"  feature_stats: {len(stats_df):,} rows, layers={LAYERS}\")\n",
        "print(f\"  behavioral_effects: {len(behav_df)} rows\")\n",
        "\n",
        "funnel_path = INPUT_DIR / 'filtering_funnel.csv'\n",
        "HAS_FUNNEL = funnel_path.exists()\n",
        "funnel_df = pd.read_csv(funnel_path) if HAS_FUNNEL else None\n",
        "\n",
        "for name in ['feature_stats.csv', 'selected_features.json', 'filtering_funnel.csv']:\n",
        "    src = INPUT_DIR / name\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, DATA / name)\n",
        "behav_df.to_csv(DATA / 'behavioral_effects.csv', index=False)\n",
        "\n",
        "\n",
        "# --- Figure 1: Encoding heatmap by layer ---\n",
        "\n",
        "print(\"\\nFigure 1: Encoding heatmap by layer\")\n",
        "\n",
        "N_TOP = 10\n",
        "nrows, ncols = make_2x4_grid(LAYERS)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 0.6 * N_TOP + 2.5 * nrows))\n",
        "axes_flat = np.atleast_1d(axes).flatten()\n",
        "\n",
        "for ax_idx, layer in enumerate(LAYERS):\n",
        "    ax = axes_flat[ax_idx]\n",
        "    ls = stats_df[stats_df['layer'] == layer].copy()\n",
        "    if len(ls) == 0:\n",
        "        ax.set_title(f'{layer_label(layer)}\\n(no data)')\n",
        "        continue\n",
        "\n",
        "    pivot_d = ls.pivot_table(\n",
        "        values='cohens_d', index='feature_idx', columns='demographic', aggfunc='mean'\n",
        "    ).reindex(columns=ALL_DEMOS)\n",
        "    pivot_p = ls.pivot_table(\n",
        "        values='p_corrected', index='feature_idx', columns='demographic', aggfunc='min'\n",
        "    ).reindex(columns=ALL_DEMOS)\n",
        "\n",
        "    mean_abs_d = pivot_d.abs().mean(axis=1)\n",
        "    top_feats = mean_abs_d.nlargest(N_TOP).index\n",
        "    heat_d = pivot_d.loc[top_feats]\n",
        "    heat_p = pivot_p.loc[top_feats]\n",
        "\n",
        "    n_cells = heat_p.size\n",
        "    n_sig = sum(1 for i in range(heat_d.shape[0]) for j in range(heat_d.shape[1])\n",
        "                if pd.notna(heat_p.iloc[i, j]) and heat_p.iloc[i, j] * n_cells < 0.001)\n",
        "\n",
        "    vmax = max(3.0, float(np.nanmax(np.abs(heat_d.values))))\n",
        "    sns.heatmap(heat_d, annot=False, cmap='RdBu_r', center=0,\n",
        "                vmin=-vmax, vmax=vmax, xticklabels=DEMO_SHORT,\n",
        "                yticklabels=heat_d.index.astype(str), ax=ax, linewidths=0.5,\n",
        "                cbar_kws={'label': \"Cohen's d\", 'shrink': 0.7})\n",
        "\n",
        "    for i in range(heat_d.shape[0]):\n",
        "        for j in range(heat_d.shape[1]):\n",
        "            p_val = heat_p.iloc[i, j]\n",
        "            if pd.notna(p_val) and p_val * n_cells < 0.001:\n",
        "                ax.text(j + 0.5, i + 0.5, '***', ha='center', va='center',\n",
        "                        fontsize=7, color='black', fontweight='bold')\n",
        "\n",
        "    ax.set_title(f'{layer_label(layer)}\\n{n_sig}/{n_cells} sig', fontsize=11)\n",
        "    ax.set_ylabel('Feature' if ax_idx % ncols == 0 else '')\n",
        "\n",
        "for ax_idx in range(len(LAYERS), len(axes_flat)):\n",
        "    axes_flat[ax_idx].set_visible(False)\n",
        "\n",
        "plt.suptitle(\"Feature-Demographic Encoding by Layer — Gemma 2 9B\\n\"\n",
        "             \"(*** = p < 0.001 after Bonferroni)\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_encoding_by_layer.png')\n",
        "plt.savefig(FIGS / 'fig_encoding_by_layer.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_encoding_by_layer\")\n",
        "\n",
        "\n",
        "# --- Figure 2: Encoding strength across layers ---\n",
        "\n",
        "print(\"Figure 2: Encoding strength across layers\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "depths = [LAYER_DEPTH.get(l, l) for l in LAYERS]\n",
        "\n",
        "ax = axes[0]\n",
        "for demo in ALL_DEMOS:\n",
        "    means = [stats_df[(stats_df['layer'] == l) & (stats_df['demographic'] == demo)]['cohens_d'].abs().mean()\n",
        "             for l in LAYERS]\n",
        "    ax.plot(depths, means, marker='o', linewidth=2, markersize=7,\n",
        "            label=demo.capitalize(), color=COLORS[demo])\n",
        "for layer in IT_LAYERS:\n",
        "    d = LAYER_DEPTH.get(layer, layer)\n",
        "    ax.axvline(d, color='gray', linestyle=':', alpha=0.4)\n",
        "ax.set_xlabel('Depth (%)')\n",
        "ax.set_ylabel(\"Mean |Cohen's d|\")\n",
        "ax.set_title('A. Encoding Strength', fontweight='bold')\n",
        "ax.set_xticks(depths)\n",
        "ax.set_xticklabels([f'L{l}' for l in LAYERS], rotation=45, fontsize=9)\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "ax = axes[1]\n",
        "sig_col = 'feature_significant' if 'feature_significant' in stats_df.columns else 'significant'\n",
        "for demo in ALL_DEMOS:\n",
        "    counts = [((stats_df['layer'] == l) & (stats_df['demographic'] == demo) &\n",
        "               (stats_df[sig_col] == True)).sum() for l in LAYERS]\n",
        "    ax.plot(depths, counts, marker='s', linewidth=2, markersize=7,\n",
        "            label=demo.capitalize(), color=COLORS[demo])\n",
        "for layer in IT_LAYERS:\n",
        "    ax.axvline(LAYER_DEPTH.get(layer, layer), color='gray', linestyle=':', alpha=0.4)\n",
        "ax.set_xlabel('Depth (%)')\n",
        "ax.set_ylabel(f'# Features ({sig_col})')\n",
        "ax.set_title('B. Significant Features', fontweight='bold')\n",
        "ax.set_xticks(depths)\n",
        "ax.set_xticklabels([f'L{l}' for l in LAYERS], rotation=45, fontsize=9)\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "ax = axes[2]\n",
        "for demo in ALL_DEMOS:\n",
        "    cands = []\n",
        "    for layer in LAYERS:\n",
        "        layer_str = str(layer)\n",
        "        if layer_str in selected_features:\n",
        "            dc = [v.get('n_candidates', 0) for k, v in selected_features[layer_str].items()\n",
        "                  if k.startswith(f'{demo}_')]\n",
        "            cands.append(np.mean(dc) if dc else 0)\n",
        "        else:\n",
        "            cands.append(0)\n",
        "    ax.plot(depths, cands, marker='^', linewidth=2, markersize=7,\n",
        "            label=demo.capitalize(), color=COLORS[demo])\n",
        "for layer in IT_LAYERS:\n",
        "    ax.axvline(LAYER_DEPTH.get(layer, layer), color='gray', linestyle=':', alpha=0.4)\n",
        "ax.set_xlabel('Depth (%)')\n",
        "ax.set_ylabel('Mean # Candidates')\n",
        "ax.set_title('C. Candidate Features (sig + large + clean)', fontweight='bold')\n",
        "ax.set_xticks(depths)\n",
        "ax.set_xticklabels([f'L{l}' for l in LAYERS], rotation=45, fontsize=9)\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "plt.suptitle('Encoding Trends Across Layers — Gemma 2 9B', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_encoding_across_layers.png')\n",
        "plt.savefig(FIGS / 'fig_encoding_across_layers.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_encoding_across_layers\")\n",
        "\n",
        "\n",
        "# --- Figure 3: Behavioral effects ---\n",
        "\n",
        "print(\"Figure 3: Behavioral effects heatmaps\")\n",
        "\n",
        "nrows, ncols = make_2x4_grid(LAYERS)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5 * nrows))\n",
        "axes_flat = np.atleast_1d(axes).flatten()\n",
        "\n",
        "for ax_idx, layer in enumerate(LAYERS):\n",
        "    ax = axes_flat[ax_idx]\n",
        "    lb = behav_df[behav_df['layer'] == layer]\n",
        "    if len(lb) == 0:\n",
        "        ax.set_title(f'{layer_label(layer)}\\n(no data)')\n",
        "        continue\n",
        "\n",
        "    pivot_eff = lb.pivot_table(values='effect', index='demographic', columns='domain',\n",
        "                                aggfunc='mean').reindex(index=ALL_DEMOS, columns=ALL_DOMAINS)\n",
        "    pivot_p = lb.pivot_table(values='p_value', index='demographic', columns='domain',\n",
        "                              aggfunc='min').reindex(index=ALL_DEMOS, columns=ALL_DOMAINS)\n",
        "    pivot_method = lb.pivot_table(values='selection_method', index='demographic', columns='domain',\n",
        "                                   aggfunc='first').reindex(index=ALL_DEMOS, columns=ALL_DOMAINS)\n",
        "\n",
        "    vmax = max(0.5, float(np.nanmax(np.abs(pivot_eff.values))))\n",
        "    sns.heatmap(pivot_eff, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
        "                vmin=-vmax, vmax=vmax, linewidths=0.5, ax=ax,\n",
        "                cbar_kws={'label': 'dEV', 'shrink': 0.7},\n",
        "                xticklabels=[d[:4].capitalize() for d in ALL_DOMAINS],\n",
        "                yticklabels=[d[:4].capitalize() for d in ALL_DEMOS])\n",
        "\n",
        "    for i in range(pivot_eff.shape[0]):\n",
        "        for j in range(pivot_eff.shape[1]):\n",
        "            p = pivot_p.iloc[i, j] if pd.notna(pivot_p.iloc[i, j]) else 1.0\n",
        "            star = '***' if p < 0.001 else ('**' if p < 0.01 else ('*' if p < 0.05 else ''))\n",
        "            if star:\n",
        "                ax.text(j + 0.85, i + 0.15, star, fontsize=6, ha='right', va='top')\n",
        "            method = pivot_method.iloc[i, j] if pd.notna(pivot_method.iloc[i, j]) else ''\n",
        "            if 'fallback' in str(method):\n",
        "                ax.text(j + 0.15, i + 0.85, '+', fontsize=7, ha='left', va='bottom',\n",
        "                        color='gray', fontstyle='italic')\n",
        "    ax.set_title(f'{layer_label(layer)}', fontsize=11)\n",
        "\n",
        "for ax_idx in range(len(LAYERS), len(axes_flat)):\n",
        "    axes_flat[ax_idx].set_visible(False)\n",
        "\n",
        "plt.suptitle('Behavioral Effects — Gemma 2 9B\\n'\n",
        "             '(* p<.05, ** p<.01, *** p<.001; + = fallback)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_behavioral_effects.png')\n",
        "plt.savefig(FIGS / 'fig_behavioral_effects.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_behavioral_effects\")\n",
        "\n",
        "\n",
        "# --- Figure 4: Feature quality ---\n",
        "\n",
        "print(\"Figure 4: Feature quality distributions\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(13, 10))\n",
        "\n",
        "ax = axes[0, 0]\n",
        "violin_data = [stats_df[stats_df['layer'] == l]['cohens_d'].dropna().values for l in LAYERS]\n",
        "violin_labels = [f'L{l}' for l in LAYERS]\n",
        "parts = ax.violinplot(violin_data, positions=range(len(LAYERS)), showmeans=True, showmedians=True)\n",
        "for pc in parts['bodies']:\n",
        "    pc.set_facecolor('#3498db'); pc.set_alpha(0.6)\n",
        "ax.set_xticks(range(len(LAYERS)))\n",
        "ax.set_xticklabels(violin_labels, fontsize=9)\n",
        "ax.set_ylabel(\"Cohen's d\")\n",
        "ax.set_title(\"A. Effect Size Distribution by Layer\", fontweight='bold')\n",
        "ax.axhline(0, color='black', linestyle='--', linewidth=0.5)\n",
        "ax.axhline(0.3, color='red', linestyle=':', linewidth=0.5, alpha=0.5)\n",
        "ax.axhline(-0.3, color='red', linestyle=':', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "ax = axes[0, 1]\n",
        "if 'sign_agreement' in stats_df.columns and stats_df['sign_agreement'].notna().any():\n",
        "    sa_data = [stats_df[stats_df['layer'] == l]['sign_agreement'].dropna().values for l in LAYERS]\n",
        "    parts = ax.violinplot(sa_data, positions=range(len(LAYERS)), showmeans=True, showmedians=True)\n",
        "    for pc in parts['bodies']:\n",
        "        pc.set_facecolor('#2ecc71'); pc.set_alpha(0.6)\n",
        "    ax.set_xticks(range(len(LAYERS)))\n",
        "    ax.set_xticklabels(violin_labels, fontsize=9)\n",
        "    ax.set_ylabel('Sign Agreement')\n",
        "    ax.set_title('B. Sign Consistency by Layer', fontweight='bold')\n",
        "    ax.axhline(0.6, color='red', linestyle='--', linewidth=1, label='Threshold')\n",
        "    ax.legend()\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'sign_agreement unavailable', ha='center', va='center',\n",
        "            transform=ax.transAxes, fontsize=12)\n",
        "    ax.set_title('B. Sign Consistency', fontweight='bold')\n",
        "\n",
        "ax = axes[1, 0]\n",
        "sig_rows = []\n",
        "for layer in LAYERS:\n",
        "    ls = stats_df[stats_df['layer'] == layer]\n",
        "    row = {'Layer': f'L{layer}', 'FDR Sig': int(ls['significant'].sum()),\n",
        "           'Large |d|': int(ls['large_effect'].sum()) if 'large_effect' in ls.columns else 0}\n",
        "    if 'feature_significant' in ls.columns:\n",
        "        row['Both + Clean'] = int(ls['feature_significant'].sum())\n",
        "    else:\n",
        "        row['Both + Clean'] = int((ls['significant'] & ls['large_effect']).sum())\n",
        "    sig_rows.append(row)\n",
        "sig_df = pd.DataFrame(sig_rows)\n",
        "x = np.arange(len(LAYERS)); w = 0.25\n",
        "ax.bar(x - w, sig_df['FDR Sig'], w, label='FDR Significant', color='#3498db')\n",
        "ax.bar(x, sig_df['Large |d|'], w, label='Large |d| (>=0.3)', color='#e74c3c')\n",
        "ax.bar(x + w, sig_df['Both + Clean'], w, label='Sig + Large + Clean', color='#2ecc71')\n",
        "ax.set_xticks(x); ax.set_xticklabels(sig_df['Layer'], fontsize=9)\n",
        "ax.set_ylabel('# Features')\n",
        "ax.set_title('C. Selection Criteria by Layer', fontweight='bold')\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "ax = axes[1, 1]\n",
        "if HAS_FUNNEL:\n",
        "    noisy_pct = [funnel_df[funnel_df['layer'] == l]['pct_noisy'].mean() for l in LAYERS]\n",
        "    colors_bars = ['#e67e22' if l in IT_LAYERS else '#e74c3c' for l in LAYERS]\n",
        "    ax.bar([f'L{l}' for l in LAYERS], noisy_pct, color=colors_bars, edgecolor='black')\n",
        "    for i, p in enumerate(noisy_pct):\n",
        "        ax.annotate(f'{p:.1f}%', (i, p), ha='center', va='bottom', fontsize=9)\n",
        "    ax.set_ylabel('% Noisy (all 16k features)')\n",
        "    ax.set_title('D. Pre-Selection Noise Rate', fontweight='bold')\n",
        "    ax.set_ylim(0, max(noisy_pct) * 1.3 + 1)\n",
        "elif 'is_noisy' in stats_df.columns:\n",
        "    noisy_pct = [stats_df[stats_df['layer'] == l]['is_noisy'].mean() * 100 for l in LAYERS]\n",
        "    colors_bars = ['#e67e22' if l in IT_LAYERS else '#e74c3c' for l in LAYERS]\n",
        "    ax.bar([f'L{l}' for l in LAYERS], noisy_pct, color=colors_bars, edgecolor='black')\n",
        "    for i, p in enumerate(noisy_pct):\n",
        "        ax.annotate(f'{p:.1f}%', (i, p), ha='center', va='bottom', fontsize=9)\n",
        "    ax.set_ylabel('% Noisy (selected features only)')\n",
        "    ax.set_title('D. Noise Rate (selected only)', fontweight='bold')\n",
        "    ax.set_ylim(0, max(max(noisy_pct) * 1.3, 5))\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'Noise data unavailable', ha='center', va='center',\n",
        "            transform=ax.transAxes, fontsize=12)\n",
        "    ax.set_title('D. Noise Rate', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Feature Quality Analysis — Gemma 2 9B', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_feature_quality.png')\n",
        "plt.savefig(FIGS / 'fig_feature_quality.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_feature_quality\")\n",
        "\n",
        "\n",
        "# --- Figure 5: Domain overlap ---\n",
        "\n",
        "print(\"Figure 5: Domain overlap\")\n",
        "\n",
        "nrows, ncols = make_2x4_grid(LAYERS)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(4.5 * ncols, 4 * nrows))\n",
        "axes_flat = np.atleast_1d(axes).flatten()\n",
        "\n",
        "for ax_idx, layer in enumerate(LAYERS):\n",
        "    ax = axes_flat[ax_idx]\n",
        "    layer_str = str(layer)\n",
        "    if layer_str not in selected_features:\n",
        "        ax.set_title(f'{layer_label(layer)}\\n(no data)')\n",
        "        continue\n",
        "\n",
        "    layer_data = selected_features[layer_str]\n",
        "    feat_sets = {}\n",
        "    for key_str, info in layer_data.items():\n",
        "        parts = key_str.split('_', 1)\n",
        "        demo, domain = parts[0], parts[1] if len(parts) > 1 else 'unknown'\n",
        "        feat_sets[(demo, domain)] = set(info.get('features', []))\n",
        "\n",
        "    n_dom = len(ALL_DOMAINS)\n",
        "    jac_sum = np.zeros((n_dom, n_dom))\n",
        "    jac_cnt = np.zeros((n_dom, n_dom))\n",
        "\n",
        "    for demo in ALL_DEMOS:\n",
        "        for (d_i, d1), (d_j, d2) in combs(enumerate(ALL_DOMAINS), 2):\n",
        "            s1 = feat_sets.get((demo, d1), set())\n",
        "            s2 = feat_sets.get((demo, d2), set())\n",
        "            if len(s1 | s2) > 0:\n",
        "                jac_sum[d_i, d_j] += len(s1 & s2) / len(s1 | s2)\n",
        "                jac_sum[d_j, d_i] += len(s1 & s2) / len(s1 | s2)\n",
        "            jac_cnt[d_i, d_j] += 1\n",
        "            jac_cnt[d_j, d_i] += 1\n",
        "\n",
        "    mask = jac_cnt > 0\n",
        "    jac_avg = np.zeros_like(jac_sum)\n",
        "    jac_avg[mask] = jac_sum[mask] / jac_cnt[mask]\n",
        "    np.fill_diagonal(jac_avg, 1.0)\n",
        "\n",
        "    sns.heatmap(jac_avg, annot=True, fmt='.2f', cmap='YlOrRd',\n",
        "                xticklabels=[d[:4].capitalize() for d in ALL_DOMAINS],\n",
        "                yticklabels=[d[:4].capitalize() for d in ALL_DOMAINS],\n",
        "                ax=ax, vmin=0, vmax=0.5, linewidths=0.5, cbar_kws={'shrink': 0.7})\n",
        "    ax.set_title(f'{layer_label(layer)}', fontsize=11)\n",
        "\n",
        "for ax_idx in range(len(LAYERS), len(axes_flat)):\n",
        "    axes_flat[ax_idx].set_visible(False)\n",
        "\n",
        "plt.suptitle('Cross-Domain Feature Overlap (Jaccard) — Gemma 2 9B',\n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_domain_overlap.png')\n",
        "plt.savefig(FIGS / 'fig_domain_overlap.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_domain_overlap\")\n",
        "\n",
        "print(\"\\nFigures 1-5 done.\")"
      ],
      "metadata": {
        "id": "GDhzf0rFueWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction analysis — figures 6-10, tables, summary, bundling\n",
        "\n",
        "# --- Figure 6: Cross-demographic feature profiles ---\n",
        "\n",
        "print(\"Figure 6: Cross-demographic feature profiles\")\n",
        "\n",
        "target_layer = max(LAYERS)\n",
        "ls_target = stats_df[stats_df['layer'] == target_layer]\n",
        "\n",
        "feat_demo_count = ls_target.groupby('feature_idx')['demographic'].nunique()\n",
        "multi_feats = feat_demo_count[feat_demo_count >= 3].index.tolist()\n",
        "\n",
        "if len(multi_feats) > 0:\n",
        "    feat_abs_d = ls_target[ls_target['feature_idx'].isin(multi_feats)].groupby(\n",
        "        'feature_idx')['cohens_d'].apply(lambda x: x.abs().mean())\n",
        "    top_cross = feat_abs_d.nlargest(min(15, len(multi_feats))).index.tolist()\n",
        "\n",
        "    profile_rows = []\n",
        "    for feat in top_cross:\n",
        "        fs = ls_target[ls_target['feature_idx'] == feat]\n",
        "        row = {demo: fs[fs['demographic'] == demo]['cohens_d'].mean()\n",
        "               if len(fs[fs['demographic'] == demo]) > 0 else 0\n",
        "               for demo in ALL_DEMOS}\n",
        "        profile_rows.append(row)\n",
        "\n",
        "    profile_matrix = pd.DataFrame(profile_rows, index=top_cross)[ALL_DEMOS]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, max(4, len(top_cross) * 0.5 + 1)))\n",
        "    vmax = max(3.0, float(np.nanmax(np.abs(profile_matrix.values))))\n",
        "    sns.heatmap(profile_matrix, annot=True, fmt='.1f', cmap='RdBu_r', center=0,\n",
        "                vmin=-vmax, vmax=vmax, linewidths=0.5,\n",
        "                xticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "                ax=ax, cbar_kws={'label': \"Cohen's d\"})\n",
        "    ax.set_title(f'Cross-Demographic Features — Gemma 2 9B (Layer {target_layer})\\n'\n",
        "                 f'Features in >=3 demographics, ranked by mean |d|', fontweight='bold')\n",
        "    ax.set_ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGS / 'fig_feature_profiles.png')\n",
        "    plt.savefig(FIGS / 'fig_feature_profiles.pdf')\n",
        "    plt.close()\n",
        "    print(f\"  Saved: fig_feature_profiles ({len(top_cross)} features)\")\n",
        "else:\n",
        "    print(f\"  Skipped: no features in >=3 demographics at L{target_layer}\")\n",
        "\n",
        "\n",
        "# --- Figure 7: Encoding vs behavioral effect ---\n",
        "\n",
        "print(\"Figure 7: Encoding vs behavioral effect\")\n",
        "\n",
        "corr_rows = []\n",
        "for layer in LAYERS:\n",
        "    ls = stats_df[stats_df['layer'] == layer]\n",
        "    lb = behav_df[behav_df['layer'] == layer]\n",
        "    for demo in ALL_DEMOS:\n",
        "        for domain in ALL_DOMAINS:\n",
        "            feat_d = ls[(ls['demographic'] == demo) & (ls['domain'] == domain)]['cohens_d'].abs()\n",
        "            brow = lb[(lb['demographic'] == demo) & (lb['domain'] == domain)]\n",
        "            if len(feat_d) > 0 and len(brow) > 0:\n",
        "                corr_rows.append({\n",
        "                    'layer': layer, 'demographic': demo, 'domain': domain,\n",
        "                    'mean_abs_d': feat_d.mean(),\n",
        "                    'abs_behavioral': abs(brow['effect'].iloc[0]),\n",
        "                    'selection_method': brow['selection_method'].iloc[0],\n",
        "                    'depth': LAYER_DEPTH.get(layer, layer),\n",
        "                })\n",
        "corr_df = pd.DataFrame(corr_rows)\n",
        "\n",
        "if len(corr_df) >= 5:\n",
        "    early_layers = [l for l in LAYERS if l <= 20]\n",
        "    late_layers = [l for l in LAYERS if l > 20]\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    for ax, layer_group, title in zip(axes,\n",
        "                                       [early_layers, late_layers],\n",
        "                                       ['Early-Mid Layers (L5-L20)', 'Late Layers (L27-L36)']):\n",
        "        lc = corr_df[corr_df['layer'].isin(layer_group)]\n",
        "        if len(lc) < 3:\n",
        "            ax.set_title(f'{title}\\n(insufficient data)')\n",
        "            continue\n",
        "\n",
        "        scatter = ax.scatter(lc['mean_abs_d'], lc['abs_behavioral'],\n",
        "                             c=lc['depth'], cmap='viridis', s=50, alpha=0.7,\n",
        "                             edgecolors='black', linewidth=0.5)\n",
        "        plt.colorbar(scatter, ax=ax, label='Depth %', shrink=0.8)\n",
        "\n",
        "        fb = lc[lc['selection_method'].str.contains('fallback', na=False)]\n",
        "        if len(fb) > 0:\n",
        "            ax.scatter(fb['mean_abs_d'], fb['abs_behavioral'],\n",
        "                       marker='x', color='red', s=80, linewidths=2, label='Fallback')\n",
        "\n",
        "        r, p_r = pearsonr(lc['mean_abs_d'], lc['abs_behavioral'])\n",
        "        rho, _ = spearmanr(lc['mean_abs_d'], lc['abs_behavioral'])\n",
        "        z = np.polyfit(lc['mean_abs_d'], lc['abs_behavioral'], 1)\n",
        "        x_line = np.linspace(lc['mean_abs_d'].min(), lc['mean_abs_d'].max(), 50)\n",
        "        ax.plot(x_line, np.polyval(z, x_line), 'k--', alpha=0.5, linewidth=1)\n",
        "\n",
        "        ax.set_xlabel(\"Mean |Cohen's d|\")\n",
        "        ax.set_ylabel('|Behavioral Effect|')\n",
        "        sig = 'p<.001' if p_r < 0.001 else f'p={p_r:.3f}'\n",
        "        ax.set_title(f'{title}\\nr={r:.2f} ({sig}), rho={rho:.2f}', fontweight='bold')\n",
        "        ax.legend(fontsize=8)\n",
        "\n",
        "    plt.suptitle('Encoding Strength vs Behavioral Effect — Gemma 2 9B',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGS / 'fig_encoding_vs_behavior.png')\n",
        "    plt.savefig(FIGS / 'fig_encoding_vs_behavior.pdf')\n",
        "    plt.close()\n",
        "    print(\"  Saved: fig_encoding_vs_behavior\")\n",
        "else:\n",
        "    print(\"  Skipped: insufficient data\")\n",
        "\n",
        "\n",
        "# --- Figure 8: Encoding by domain ---\n",
        "\n",
        "print(\"Figure 8: Encoding by domain\")\n",
        "\n",
        "nrows, ncols = make_2x4_grid(LAYERS)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(4.5 * ncols, 4 * nrows))\n",
        "axes_flat = np.atleast_1d(axes).flatten()\n",
        "\n",
        "for ax_idx, layer in enumerate(LAYERS):\n",
        "    ax = axes_flat[ax_idx]\n",
        "    ls = stats_df[stats_df['layer'] == layer]\n",
        "    pivot = ls.pivot_table(values='cohens_d', index='domain', columns='demographic',\n",
        "                            aggfunc=lambda x: x.abs().mean()\n",
        "                            ).reindex(index=ALL_DOMAINS, columns=ALL_DEMOS)\n",
        "    vmax = max(1.0, float(np.nanmax(pivot.values)))\n",
        "    sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', xticklabels=DEMO_SHORT,\n",
        "                yticklabels=[d[:4].capitalize() for d in ALL_DOMAINS],\n",
        "                ax=ax, vmin=0, vmax=vmax, linewidths=0.5, cbar_kws={'shrink': 0.7})\n",
        "    ax.set_title(f'{layer_label(layer)}', fontsize=11)\n",
        "    ax.set_ylabel('Domain' if ax_idx % ncols == 0 else '')\n",
        "\n",
        "for ax_idx in range(len(LAYERS), len(axes_flat)):\n",
        "    axes_flat[ax_idx].set_visible(False)\n",
        "\n",
        "plt.suptitle(\"Mean |Cohen's d| by Domain x Demographic — Gemma 2 9B\",\n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_encoding_by_domain.png')\n",
        "plt.savefig(FIGS / 'fig_encoding_by_domain.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_encoding_by_domain\")\n",
        "\n",
        "\n",
        "# --- Figure 9: Selection method diagnostics ---\n",
        "\n",
        "print(\"Figure 9: Selection method diagnostics\")\n",
        "\n",
        "if 'selection_method' in stats_df.columns:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    ax = axes[0, 0]\n",
        "    method_data = []\n",
        "    for layer in LAYERS:\n",
        "        lb = behav_df[behav_df['layer'] == layer]\n",
        "        counts = lb['selection_method'].value_counts()\n",
        "        method_data.append({\n",
        "            'Layer': f'L{layer}',\n",
        "            'Significant': counts.get('significant', 0),\n",
        "            'Fallback (filtered)': counts.get('fallback_noise_filtered', 0),\n",
        "            'Fallback (unfiltered)': counts.get('fallback_unfiltered', 0),\n",
        "        })\n",
        "    method_df = pd.DataFrame(method_data)\n",
        "    x = np.arange(len(LAYERS)); w = 0.25\n",
        "    ax.bar(x - w, method_df['Significant'], w, label='Significant', color='#2ecc71')\n",
        "    ax.bar(x, method_df['Fallback (filtered)'], w, label='Fallback (filtered)', color='#f39c12')\n",
        "    ax.bar(x + w, method_df['Fallback (unfiltered)'], w, label='Fallback (unfiltered)', color='#e74c3c')\n",
        "    ax.set_xticks(x); ax.set_xticklabels(method_df['Layer'], fontsize=9)\n",
        "    ax.set_ylabel('# Demo x Domain Pairs')\n",
        "    ax.set_title('A. Selection Method by Layer', fontweight='bold')\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "    ax = axes[0, 1]\n",
        "    target_layer = max(LAYERS)\n",
        "    lb_target = behav_df[behav_df['layer'] == target_layer]\n",
        "    method_pivot = lb_target.pivot_table(values='selection_method', index='demographic',\n",
        "                                          columns='domain', aggfunc='first'\n",
        "                                          ).reindex(index=ALL_DEMOS, columns=ALL_DOMAINS)\n",
        "    method_numeric = method_pivot.copy()\n",
        "    method_map = {'significant': 0, 'fallback_noise_filtered': 1, 'fallback_unfiltered': 2}\n",
        "    for col in method_numeric.columns:\n",
        "        method_numeric[col] = method_numeric[col].map(method_map).fillna(-1)\n",
        "    from matplotlib.colors import ListedColormap\n",
        "    cmap_method = ListedColormap(['#2ecc71', '#f39c12', '#e74c3c'])\n",
        "    sns.heatmap(method_numeric.astype(float), annot=method_pivot.values, fmt='',\n",
        "                cmap=cmap_method, vmin=0, vmax=2, linewidths=0.5, ax=ax,\n",
        "                xticklabels=[d[:4].capitalize() for d in ALL_DOMAINS],\n",
        "                yticklabels=[d[:4].capitalize() for d in ALL_DEMOS], cbar=False)\n",
        "    ax.set_title(f'B. Selection Method Map (L{target_layer})', fontweight='bold')\n",
        "\n",
        "    ax = axes[1, 0]\n",
        "    if 'feature_significant' in stats_df.columns:\n",
        "        ls_target = stats_df[stats_df['layer'] == target_layer]\n",
        "        fs_rate = ls_target.pivot_table(values='feature_significant', index='domain',\n",
        "                                         columns='demographic', aggfunc='mean'\n",
        "                                         ).reindex(index=ALL_DOMAINS, columns=ALL_DEMOS) * 100\n",
        "        sns.heatmap(fs_rate, annot=True, fmt='.0f', cmap='YlGn', xticklabels=DEMO_SHORT,\n",
        "                    yticklabels=[d[:4].capitalize() for d in ALL_DOMAINS],\n",
        "                    ax=ax, vmin=0, vmax=100, linewidths=0.5,\n",
        "                    cbar_kws={'label': '% feature_significant'})\n",
        "        ax.set_title(f'C. % Individually Significant (L{target_layer})', fontweight='bold')\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'feature_significant\\nnot available',\n",
        "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
        "\n",
        "    ax = axes[1, 1]\n",
        "    if HAS_FUNNEL:\n",
        "        lf = funnel_df[funnel_df['layer'] == target_layer]\n",
        "        n_total = lf['n_total_features'].iloc[0] if len(lf) > 0 else 16384\n",
        "        avg_sign_fail = (n_total - lf['n_sign_consistent'].mean()) if 'n_sign_consistent' in lf.columns else 0\n",
        "        avg_cv_fail = (n_total - lf['n_low_cv'].mean()) if 'n_low_cv' in lf.columns else 0\n",
        "        avg_outlier_fail = (n_total - lf['n_few_outliers'].mean()) if 'n_few_outliers' in lf.columns else 0\n",
        "        avg_any_fail = lf['n_noisy'].mean() if 'n_noisy' in lf.columns else 0\n",
        "        labels = ['Sign < 0.6', 'CV >= 3.0', 'Outlier >= 0.1', 'Any (noisy)']\n",
        "        values = [avg_sign_fail, avg_cv_fail, avg_outlier_fail, avg_any_fail]\n",
        "        pcts = [v / n_total * 100 for v in values]\n",
        "        colors_bar = ['#3498db', '#e74c3c', '#f39c12', '#8e44ad']\n",
        "        bars = ax.barh(labels, pcts, color=colors_bar, edgecolor='black')\n",
        "        for bar, pct in zip(bars, pcts):\n",
        "            ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2,\n",
        "                    f'{pct:.1f}%', va='center', fontsize=9)\n",
        "        ax.set_xlabel(f'% of All {n_total:,} Features')\n",
        "        ax.set_title(f'D. Noise Filter Failures (L{target_layer}, pre-selection)', fontweight='bold')\n",
        "        ax.set_xlim(0, max(pcts) * 1.3 + 5)\n",
        "    elif 'sign_agreement' in stats_df.columns:\n",
        "        ls_t = stats_df[stats_df['layer'] == target_layer]\n",
        "        n_total = len(ls_t)\n",
        "        failed = [\n",
        "            (ls_t['sign_agreement'] < 0.60).sum(),\n",
        "            (ls_t['cv'] >= 3.0).sum() if ls_t['cv'].notna().any() else 0,\n",
        "            (ls_t['outlier_ratio'] >= 0.10).sum(),\n",
        "            ls_t['is_noisy'].sum() if 'is_noisy' in ls_t.columns else 0,\n",
        "        ]\n",
        "        labels = ['Sign < 0.6', 'CV >= 3.0', 'Outlier >= 0.1', 'Any (noisy)']\n",
        "        pcts = [v / n_total * 100 if n_total > 0 else 0 for v in failed]\n",
        "        colors_bar = ['#3498db', '#e74c3c', '#f39c12', '#8e44ad']\n",
        "        bars = ax.barh(labels, pcts, color=colors_bar, edgecolor='black')\n",
        "        for bar, pct in zip(bars, pcts):\n",
        "            ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2,\n",
        "                    f'{pct:.1f}%', va='center', fontsize=9)\n",
        "        ax.set_xlabel('% of Selected Features (post-selection)')\n",
        "        ax.set_title(f'D. Noise Failures (L{target_layer}, selected only)', fontweight='bold')\n",
        "        ax.set_xlim(0, max(pcts) * 1.3 + 5)\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'Noise columns unavailable', ha='center', va='center',\n",
        "                transform=ax.transAxes, fontsize=12)\n",
        "\n",
        "    plt.suptitle('Selection Method & Quality Diagnostics — Gemma 2 9B',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGS / 'fig_selection_diagnostics.png')\n",
        "    plt.savefig(FIGS / 'fig_selection_diagnostics.pdf')\n",
        "    plt.close()\n",
        "    print(\"  Saved: fig_selection_diagnostics\")\n",
        "else:\n",
        "    print(\"  Skipped: selection_method column not available\")\n",
        "\n",
        "\n",
        "# --- Figure 10: Cross-layer encoding gradient ---\n",
        "\n",
        "print(\"Figure 10: Cross-layer encoding gradient\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "depths = [LAYER_DEPTH.get(l, l) for l in LAYERS]\n",
        "\n",
        "ax = axes[0]\n",
        "for demo in ALL_DEMOS:\n",
        "    sensitivity = [stats_df[(stats_df['layer'] == l) & (stats_df['demographic'] == demo)\n",
        "                            ]['mean_diff'].abs().mean() for l in LAYERS]\n",
        "    ax.plot(depths, sensitivity, marker='o', linewidth=2, markersize=7,\n",
        "            label=demo.capitalize(), color=COLORS[demo])\n",
        "for layer in IT_LAYERS:\n",
        "    ax.axvline(LAYER_DEPTH.get(layer, layer), color='gray', linestyle=':', alpha=0.4)\n",
        "ax.set_xlabel('Depth (%)')\n",
        "ax.set_ylabel('Mean |Activation Diff|')\n",
        "ax.set_title('A. SAE Activation Sensitivity', fontweight='bold')\n",
        "ax.set_xticks(depths)\n",
        "ax.set_xticklabels([f'L{l}' for l in LAYERS], rotation=45, fontsize=9)\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "ax = axes[1]\n",
        "unique_per_layer = [stats_df[stats_df['layer'] == l]['feature_idx'].nunique() for l in LAYERS]\n",
        "bar_colors = ['#e67e22' if l in IT_LAYERS else '#3498db' for l in LAYERS]\n",
        "ax.bar([f'L{l}' for l in LAYERS], unique_per_layer, color=bar_colors, edgecolor='black')\n",
        "for i, v in enumerate(unique_per_layer):\n",
        "    ax.annotate(str(v), (i, v), ha='center', va='bottom', fontsize=9)\n",
        "ax.set_ylabel('# Unique Features')\n",
        "ax.set_title('B. Feature Diversity by Layer', fontweight='bold')\n",
        "\n",
        "ax = axes[2]\n",
        "jaccard_by_layer = []\n",
        "for layer in LAYERS:\n",
        "    layer_str = str(layer)\n",
        "    if layer_str not in selected_features:\n",
        "        jaccard_by_layer.append(0)\n",
        "        continue\n",
        "    layer_data = selected_features[layer_str]\n",
        "    feat_sets = {}\n",
        "    for key_str, info in layer_data.items():\n",
        "        parts = key_str.split('_', 1)\n",
        "        demo, domain = parts[0], parts[1] if len(parts) > 1 else 'unknown'\n",
        "        feat_sets[(demo, domain)] = set(info.get('features', []))\n",
        "    jaccards = []\n",
        "    for demo in ALL_DEMOS:\n",
        "        for d1, d2 in combs(ALL_DOMAINS, 2):\n",
        "            s1 = feat_sets.get((demo, d1), set())\n",
        "            s2 = feat_sets.get((demo, d2), set())\n",
        "            if len(s1 | s2) > 0:\n",
        "                jaccards.append(len(s1 & s2) / len(s1 | s2))\n",
        "    jaccard_by_layer.append(np.mean(jaccards) if jaccards else 0)\n",
        "\n",
        "ax.plot(depths, jaccard_by_layer, 'o-', color='#8e44ad', linewidth=2.5, markersize=9)\n",
        "for layer in IT_LAYERS:\n",
        "    ax.axvline(LAYER_DEPTH.get(layer, layer), color='gray', linestyle=':', alpha=0.4)\n",
        "if len(depths) >= 4:\n",
        "    r, p = pearsonr(depths, jaccard_by_layer)\n",
        "    ax.set_title(f'C. Domain Specialisation with Depth\\nr={r:.2f}, p={p:.3f}', fontweight='bold')\n",
        "else:\n",
        "    ax.set_title('C. Domain Specialisation with Depth', fontweight='bold')\n",
        "ax.set_xlabel('Depth (%)')\n",
        "ax.set_ylabel('Mean Cross-Domain Jaccard')\n",
        "ax.set_xticks(depths)\n",
        "ax.set_xticklabels([f'L{l}' for l in LAYERS], rotation=45, fontsize=9)\n",
        "\n",
        "plt.suptitle('Cross-Layer Encoding Gradient — Gemma 2 9B', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_encoding_gradient.png')\n",
        "plt.savefig(FIGS / 'fig_encoding_gradient.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_encoding_gradient\")\n",
        "\n",
        "\n",
        "# --- Tables ---\n",
        "\n",
        "print(\"\\nTable 1: Feature selection summary\")\n",
        "\n",
        "sel_rows = []\n",
        "for layer_str in sorted(selected_features.keys(), key=int):\n",
        "    layer = int(layer_str)\n",
        "    for key_str, info in selected_features[layer_str].items():\n",
        "        parts = key_str.split('_', 1)\n",
        "        demo, domain = parts[0], parts[1] if len(parts) > 1 else 'unknown'\n",
        "        sel_rows.append({\n",
        "            'Layer': layer,\n",
        "            'Depth': f\"{LAYER_DEPTH.get(layer, '?')}%\",\n",
        "            'SAE': 'IT' if layer in IT_LAYERS else 'PT',\n",
        "            'Demographic': demo,\n",
        "            'Domain': domain,\n",
        "            'N Selected': len(info.get('features', [])),\n",
        "            'N Significant': info.get('n_significant', 0),\n",
        "            'N Candidates': info.get('n_candidates', 0),\n",
        "            'N Not Noisy': info.get('n_not_noisy', 0),\n",
        "            'N Pairs': info.get('n_pairs', 0),\n",
        "            'Behav Effect': info.get('behavioral_effect', 0),\n",
        "            'Behav p': info.get('behavioral_p', None),\n",
        "            'Mean |d| top': np.mean(np.abs(info.get('cohens_d', [0]))),\n",
        "            'Selection Method': info.get('selection_method', 'unknown'),\n",
        "        })\n",
        "\n",
        "sel_df = pd.DataFrame(sel_rows)\n",
        "\n",
        "layer_summary = sel_df.groupby(['Layer', 'Depth', 'SAE']).agg({\n",
        "    'N Significant': 'mean', 'N Candidates': 'mean', 'N Not Noisy': 'mean',\n",
        "    'N Pairs': 'mean', 'Mean |d| top': 'mean',\n",
        "}).round(2)\n",
        "\n",
        "print(layer_summary)\n",
        "with open(TABLES / 'table_selection_summary.tex', 'w') as f:\n",
        "    f.write(layer_summary.to_latex(escape=False))\n",
        "sel_df.to_csv(DATA / 'selection_detail.csv', index=False)\n",
        "\n",
        "if 'Selection Method' in sel_df.columns:\n",
        "    method_summary = sel_df.groupby(['Layer', 'Selection Method']).size().unstack(fill_value=0)\n",
        "    print(f\"\\nSelection method summary:\")\n",
        "    print(method_summary)\n",
        "    with open(TABLES / 'table_selection_methods.tex', 'w') as f:\n",
        "        f.write(method_summary.to_latex(escape=False))\n",
        "\n",
        "\n",
        "print(\"\\nTable 2: Behavioral effects\")\n",
        "\n",
        "for layer in LAYERS:\n",
        "    lb = behav_df[behav_df['layer'] == layer]\n",
        "    pivot = lb.pivot_table(values='effect', index='demographic', columns='domain',\n",
        "                            aggfunc='mean').reindex(index=ALL_DEMOS, columns=ALL_DOMAINS).round(3)\n",
        "    print(f\"\\nLayer {layer} ({LAYER_DEPTH.get(layer, '?')}%):\")\n",
        "    print(pivot.to_string())\n",
        "    with open(TABLES / f'table_behavioral_L{layer}.tex', 'w') as f:\n",
        "        f.write(pivot.to_latex(escape=False))\n",
        "\n",
        "\n",
        "# --- Summary statistics ---\n",
        "\n",
        "print(\"\\nSummary statistics\")\n",
        "\n",
        "summary = {\n",
        "    'model': 'Gemma 2 9B IT',\n",
        "    'n_unique_features_total': int(stats_df['feature_idx'].nunique()),\n",
        "    'n_rows': len(stats_df),\n",
        "    'layers': LAYERS,\n",
        "    'layer_depths': LAYER_DEPTH,\n",
        "    'it_sae_layers': list(IT_LAYERS),\n",
        "}\n",
        "\n",
        "for layer in LAYERS:\n",
        "    ls = stats_df[stats_df['layer'] == layer]\n",
        "    lb = behav_df[behav_df['layer'] == layer]\n",
        "    layer_info = {\n",
        "        'depth_pct': LAYER_DEPTH.get(layer, '?'),\n",
        "        'sae_type': 'IT' if layer in IT_LAYERS else 'PT',\n",
        "        'n_features': int(ls['feature_idx'].nunique()),\n",
        "        'n_significant': int(ls['significant'].sum()),\n",
        "        'mean_abs_d': round(float(ls['cohens_d'].abs().mean()), 3),\n",
        "        'mean_abs_behav': round(float(lb['effect'].abs().mean()), 4) if len(lb) > 0 else None,\n",
        "        'n_sig_behav': int((lb['p_value'].dropna() < 0.05).sum()) if len(lb) > 0 else None,\n",
        "    }\n",
        "    if 'large_effect' in ls.columns:\n",
        "        layer_info['n_large_effect'] = int(ls['large_effect'].sum())\n",
        "    if 'feature_significant' in ls.columns:\n",
        "        layer_info['n_feature_significant'] = int(ls['feature_significant'].sum())\n",
        "    if 'is_noisy' in ls.columns:\n",
        "        layer_info['n_noisy'] = int(ls['is_noisy'].sum())\n",
        "        layer_info['pct_noisy'] = round(float(ls['is_noisy'].mean() * 100), 1)\n",
        "\n",
        "    n_sig_groups = sum(1 for v in selected_features.get(str(layer), {}).values()\n",
        "                       if v.get('selection_method') == 'significant')\n",
        "    n_fb_groups = sum(1 for v in selected_features.get(str(layer), {}).values()\n",
        "                      if v.get('selection_method', '') != 'significant')\n",
        "    layer_info['n_groups_significant'] = n_sig_groups\n",
        "    layer_info['n_groups_fallback'] = n_fb_groups\n",
        "    summary[f'layer_{layer}'] = layer_info\n",
        "\n",
        "with open(DATA / 'extraction_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "\n",
        "# --- Bundle ---\n",
        "\n",
        "print(\"\\nBundling...\")\n",
        "\n",
        "n_png = len(list(FIGS.glob('*.png')))\n",
        "n_pdf = len(list(FIGS.glob('*.pdf')))\n",
        "\n",
        "readme = f\"\"\"# Feature Extraction Results — Gemma 2 9B IT (8-Layer)\n",
        "# Generated: {pd.Timestamp.now().isoformat()}\n",
        "\n",
        "## Layers\n",
        "{', '.join(f'L{l} ({LAYER_DEPTH.get(l, \"?\")}%{\"—IT SAE\" if l in IT_LAYERS else \"\"})' for l in LAYERS)}\n",
        "\n",
        "## Figures ({n_png} PNG + {n_pdf} PDF)\n",
        "- fig_encoding_by_layer — Feature x Demographic heatmap (2x4)\n",
        "- fig_encoding_across_layers — Encoding strength, significant features, candidates\n",
        "- fig_behavioral_effects — Behavioral EV diff heatmap (2x4)\n",
        "- fig_feature_quality — Effect size, sign agreement, selection criteria, noise\n",
        "- fig_domain_overlap — Cross-domain Jaccard (2x4)\n",
        "- fig_feature_profiles — Cross-demographic features (deepest layer)\n",
        "- fig_encoding_vs_behavior — Encoding vs behavioral effect\n",
        "- fig_encoding_by_domain — Mean |d| by Domain x Demographic (2x4)\n",
        "- fig_selection_diagnostics — Selection method, fallback, significance, noise\n",
        "- fig_encoding_gradient — Sensitivity, diversity, domain specialisation\n",
        "\n",
        "## Tables\n",
        "- table_selection_summary.tex, table_selection_methods.tex\n",
        "- table_behavioral_L{{...}}.tex\n",
        "\n",
        "## Data\n",
        "- feature_stats.csv, behavioral_effects.csv, selected_features.json\n",
        "- selection_detail.csv, extraction_summary.json\n",
        "\"\"\"\n",
        "\n",
        "with open(BUNDLE / 'README.md', 'w') as f:\n",
        "    f.write(readme)\n",
        "\n",
        "shutil.make_archive(str(BUNDLE), 'zip', root_dir=BUNDLE.parent, base_dir=BUNDLE.name)\n",
        "\n",
        "print(f\"  {n_png} figures, {len(list(TABLES.glob('*')))} tables, {len(list(DATA.glob('*')))} data files\")\n",
        "print(f\"  Download: feature-extraction-results-gemma.zip\")\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "BGx_c5_iufAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Causal validation — configuration, data loading, helper functions\n",
        "#\n",
        "# Requires: model, tokenizer, sae_manager from prior cells\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import gc\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.stats import ttest_ind, ttest_1samp\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "N_PAIRS_PER_CONDITION = 30\n",
        "K_VALUES = [5, 10, 20, 50]\n",
        "STEERING_MULTIPLIER = 2.0\n",
        "BASE_MIN_EFFECT = 0.3\n",
        "REFERENCE_SCALE = 10\n",
        "N_RANDOM_DRAWS = 3\n",
        "P10_SPLITS = [0.0, 0.5, 1.0]\n",
        "\n",
        "ANALYSIS_LAYERS = [5, 9, 14, 18, 20, 27, 32, 36]\n",
        "LAYER_DEPTH = {5: 12, 9: 22, 14: 34, 18: 44, 20: 49, 27: 66, 32: 78, 36: 88}\n",
        "IT_LAYERS = {20}\n",
        "\n",
        "ALL_DEMOGRAPHICS = ['income', 'age', 'gender', 'education', 'vote']\n",
        "ALL_DOMAINS = ['climate', 'health', 'digital', 'economy', 'values']\n",
        "DOMAIN_MAP = {\n",
        "    'other': 'values', 'climate': 'climate', 'health': 'health',\n",
        "    'digital': 'digital', 'economy': 'economy', 'values': 'values',\n",
        "}\n",
        "\n",
        "INPUT_DIR = Path(\"./outputs_gemma_replication/feature_extraction\")\n",
        "OUTPUT_DIR = Path(\"./causal_validation_final\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "AGGREGATE_FEATURES_PATH = Path(\"./outputs_gemma_replication/feature_extraction/selected_features.json\")\n",
        "if not AGGREGATE_FEATURES_PATH.exists():\n",
        "    AGGREGATE_FEATURES_PATH = Path(\"./selected_features.json\")\n",
        "\n",
        "print(f\"Layers: {ANALYSIS_LAYERS}\")\n",
        "print(f\"K values: {K_VALUES}, Batch: {BATCH_SIZE}, Pairs/cond: {N_PAIRS_PER_CONDITION}\")\n",
        "print(f\"Est. runtime: ~{len(ANALYSIS_LAYERS) * 33}min\")\n",
        "\n",
        "# --- Load data ---\n",
        "\n",
        "prompts_path = INPUT_DIR / 'prompts_validation.parquet'\n",
        "if not prompts_path.exists():\n",
        "    prompts_path = Path(\"./outputs_gemma_replication\") / 'prompts_selection.parquet'\n",
        "    print(f\"Using selection set (validation not found)\")\n",
        "\n",
        "prompts_df = pd.read_parquet(prompts_path)\n",
        "\n",
        "if 'vocab_idx' in prompts_df.columns:\n",
        "    prompts_df = prompts_df[prompts_df['vocab_idx'] == 0].reset_index(drop=True)\n",
        "\n",
        "prompts_df['domain'] = prompts_df['domain'].map(DOMAIN_MAP).fillna(prompts_df['domain'])\n",
        "\n",
        "if prompts_df['prompt'].iloc[0].startswith('<|begin_of_text|>'):\n",
        "    import re\n",
        "    print(\"Converting Llama prompts to Gemma format...\")\n",
        "    def convert_llama_to_gemma(llama_prompt):\n",
        "        match = re.search(\n",
        "            r'<\\|start_header_id\\|>user<\\|end_header_id\\|>\\n\\n(.*?)<\\|eot_id\\|>',\n",
        "            llama_prompt, re.DOTALL\n",
        "        )\n",
        "        if match:\n",
        "            content = match.group(1)\n",
        "            return f\"<start_of_turn>user\\n{content}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "        return llama_prompt\n",
        "    prompts_df['prompt'] = prompts_df['prompt'].apply(convert_llama_to_gemma)\n",
        "\n",
        "print(f\"Prompts: {len(prompts_df):,}\")\n",
        "\n",
        "aggregate_features = {}\n",
        "if AGGREGATE_FEATURES_PATH.exists():\n",
        "    with open(AGGREGATE_FEATURES_PATH, 'r') as f:\n",
        "        agg_raw = json.load(f)\n",
        "    for layer_str, layer_data in agg_raw.items():\n",
        "        layer_int = int(layer_str)\n",
        "        aggregate_features[layer_int] = {}\n",
        "        for key_str, feat_data in layer_data.items():\n",
        "            aggregate_features[layer_int][key_str] = feat_data['features']\n",
        "    print(f\"Loaded aggregate features: {list(aggregate_features.keys())}\")\n",
        "\n",
        "if hasattr(model, 'tokenizer'):\n",
        "    tokenizer = model.tokenizer\n",
        "\n",
        "USE_TRANSFORMER_LENS = hasattr(model, 'run_with_cache')\n",
        "print(f\"TransformerLens: {USE_TRANSFORMER_LENS}, Device: {DEVICE}\")\n",
        "\n",
        "# --- Token info ---\n",
        "\n",
        "def _build_token_info():\n",
        "    info = {}\n",
        "    for i in range(0, 11):\n",
        "        encoded = tokenizer.encode(str(i), add_special_tokens=False)\n",
        "        if len(encoded) == 1:\n",
        "            info[i] = {'type': 'single', 'token_id': encoded[0]}\n",
        "        else:\n",
        "            info[i] = {\n",
        "                'type': 'multi', 'token_ids': encoded,\n",
        "                'first_token_id': encoded[0], 'second_token_id': encoded[1],\n",
        "            }\n",
        "    return info\n",
        "\n",
        "TOKEN_INFO = _build_token_info()\n",
        "multi_tokens = [v for v, info in TOKEN_INFO.items() if info['type'] == 'multi']\n",
        "if multi_tokens:\n",
        "    for v in multi_tokens:\n",
        "        ids = TOKEN_INFO[v]['token_ids']\n",
        "        print(f\"  Token '{v}' is multi-token: {ids}\")\n",
        "\n",
        "# --- Helper functions ---\n",
        "\n",
        "def compute_response_metrics(logits, scale_min, scale_max, p10_split=0.5):\n",
        "    has_multi = scale_max >= 10 and TOKEN_INFO.get(10, {}).get('type') == 'multi'\n",
        "\n",
        "    if not has_multi:\n",
        "        tokens = [TOKEN_INFO[i]['token_id'] for i in range(scale_min, scale_max + 1)]\n",
        "        logits_subset = logits[tokens].float()\n",
        "        probs = F.softmax(logits_subset, dim=0).cpu().numpy()\n",
        "        values = np.arange(scale_min, scale_max + 1)\n",
        "        ev = float(np.dot(values, probs))\n",
        "        return ev, float(probs.max()), int(values[probs.argmax()])\n",
        "\n",
        "    single_max = min(scale_max, 9)\n",
        "    single_tokens = [TOKEN_INFO[i]['token_id'] for i in range(scale_min, single_max + 1)]\n",
        "    all_probs = F.softmax(logits[single_tokens].float(), dim=0).cpu().numpy()\n",
        "    values = list(range(scale_min, single_max + 1))\n",
        "    probs_list = list(all_probs)\n",
        "\n",
        "    if 1 in values:\n",
        "        idx_of_1 = values.index(1)\n",
        "        p1_raw = probs_list[idx_of_1]\n",
        "        probs_list[idx_of_1] = p1_raw * (1.0 - p10_split)\n",
        "        values.append(10)\n",
        "        probs_list.append(p1_raw * p10_split)\n",
        "    else:\n",
        "        values.append(10)\n",
        "        probs_list.append(0.0)\n",
        "\n",
        "    values = np.array(values)\n",
        "    probs = np.array(probs_list)\n",
        "    probs = probs / probs.sum()\n",
        "    ev = float(np.dot(values, probs))\n",
        "    return ev, float(probs.max()), int(values[probs.argmax()])\n",
        "\n",
        "\n",
        "def compute_ev(logits, scale_min, scale_max, p10_split=0.5):\n",
        "    ev, _, _ = compute_response_metrics(logits, scale_min, scale_max, p10_split=p10_split)\n",
        "    return ev\n",
        "\n",
        "\n",
        "def get_base_pair_key(pk):\n",
        "    parts = pk.rsplit('_v', 1)\n",
        "    return parts[0] if len(parts) == 2 else pk\n",
        "\n",
        "\n",
        "def get_scale_normalized_threshold(scale_min, scale_max):\n",
        "    scale_range = scale_max - scale_min\n",
        "    return BASE_MIN_EFFECT * scale_range / REFERENCE_SCALE if scale_range > 0 else BASE_MIN_EFFECT\n",
        "\n",
        "\n",
        "def build_pairs(prompts_df, demographic, domain, n_pairs):\n",
        "    df = prompts_df[\n",
        "        (prompts_df['demographic'] == demographic) &\n",
        "        (prompts_df['domain'] == domain)\n",
        "    ].copy()\n",
        "    if len(df) == 0:\n",
        "        return []\n",
        "    df['base_pk'] = df['pair_key'].apply(get_base_pair_key)\n",
        "    pairs = []\n",
        "    for base_pk in df['base_pk'].unique():\n",
        "        if len(pairs) >= n_pairs:\n",
        "            break\n",
        "        group = df[df['base_pk'] == base_pk]\n",
        "        row_a = group[group['value_type'] == 'value_a']\n",
        "        row_b = group[group['value_type'] == 'value_b']\n",
        "        if len(row_a) == 0 or len(row_b) == 0:\n",
        "            continue\n",
        "        row_a, row_b = row_a.iloc[0], row_b.iloc[0]\n",
        "        pairs.append({\n",
        "            'pair_key': base_pk,\n",
        "            'prompt_a': row_a['prompt'], 'prompt_b': row_b['prompt'],\n",
        "            'scale_min': int(row_a['scale_min']), 'scale_max': int(row_a['scale_max']),\n",
        "            'question_id': row_a['question_id'],\n",
        "            'question_type': row_a.get('question_type', 'unknown'),\n",
        "            'demographic': demographic, 'domain': domain,\n",
        "        })\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# --- Batched tokenization ---\n",
        "\n",
        "def tokenize_batch(prompts):\n",
        "    encodings = [tokenizer.encode(p, add_special_tokens=True) for p in prompts]\n",
        "    max_len = max(len(e) for e in encodings)\n",
        "    pad_id = getattr(tokenizer, 'pad_token_id', None) or tokenizer.eos_token_id\n",
        "\n",
        "    padded, attention_masks, last_positions = [], [], []\n",
        "    for enc in encodings:\n",
        "        n_pad = max_len - len(enc)\n",
        "        padded.append([pad_id] * n_pad + enc)\n",
        "        attention_masks.append([0] * n_pad + [1] * len(enc))\n",
        "        last_positions.append(max_len - 1)\n",
        "\n",
        "    return (torch.tensor(padded, device=DEVICE, dtype=torch.long),\n",
        "            torch.tensor(attention_masks, device=DEVICE, dtype=torch.long),\n",
        "            torch.tensor(last_positions, device=DEVICE, dtype=torch.long))\n",
        "\n",
        "\n",
        "# --- Batched baseline ---\n",
        "\n",
        "def get_baselines_batched(prompts, layer, sae, scale_mins, scale_maxs):\n",
        "    all_sae_acts, all_evs, all_n_active = [], [], []\n",
        "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
        "\n",
        "    for batch_start in range(0, len(prompts), BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, len(prompts))\n",
        "        batch_prompts = prompts[batch_start:batch_end]\n",
        "        token_ids, attn_mask, last_pos = tokenize_batch(batch_prompts)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            logits, cache = model.run_with_cache(\n",
        "                token_ids, names_filter=lambda n: hook_name in n,\n",
        "            )\n",
        "            resid = cache[hook_name]\n",
        "            for i in range(len(batch_prompts)):\n",
        "                h = resid[i, last_pos[i], :].float()\n",
        "                sae_act = sae.encode(h.unsqueeze(0))[0]\n",
        "                all_sae_acts.append(sae_act)\n",
        "                all_n_active.append((sae_act > 0).sum().item())\n",
        "                all_evs.append(compute_ev(\n",
        "                    logits[i, last_pos[i], :].cpu(),\n",
        "                    scale_mins[batch_start + i], scale_maxs[batch_start + i]\n",
        "                ))\n",
        "            del cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return all_sae_acts, all_evs, all_n_active\n",
        "\n",
        "\n",
        "# --- Intervention spec + batched execution ---\n",
        "\n",
        "class InterventionSpec:\n",
        "    __slots__ = ['prompt', 'features', 'delta', 'mode', 'scale_min', 'scale_max', 'tag']\n",
        "    def __init__(self, prompt, features, delta, mode, scale_min, scale_max, tag=None):\n",
        "        self.prompt = prompt; self.features = features; self.delta = delta\n",
        "        self.mode = mode; self.scale_min = scale_min; self.scale_max = scale_max\n",
        "        self.tag = tag\n",
        "\n",
        "\n",
        "def run_interventions_batched(specs, layer, sae):\n",
        "    results = [None] * len(specs)\n",
        "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
        "\n",
        "    for batch_start in range(0, len(specs), BATCH_SIZE):\n",
        "        batch_specs = specs[batch_start:batch_start + BATCH_SIZE]\n",
        "        batch_prompts = [s.prompt for s in batch_specs]\n",
        "        token_ids, attn_mask, last_pos = tokenize_batch(batch_prompts)\n",
        "\n",
        "        contributions, signs = [], []\n",
        "        for s in batch_specs:\n",
        "            feat_idx = s.features.to(DEVICE)\n",
        "            decoder_weights = sae.W_dec[feat_idx, :].float()\n",
        "            contrib = (s.delta.float().to(DEVICE).unsqueeze(1) * decoder_weights).sum(dim=0)\n",
        "            contributions.append(contrib)\n",
        "            signs.append(1.0 if s.mode == 'add' else -1.0)\n",
        "\n",
        "        contrib_stack = torch.stack(contributions)\n",
        "        sign_tensor = torch.tensor(signs, device=DEVICE, dtype=torch.float32)\n",
        "        batch_last_pos = last_pos[:len(batch_specs)]\n",
        "\n",
        "        def hook_fn(activation, hook):\n",
        "            for i in range(activation.shape[0]):\n",
        "                pos = batch_last_pos[i]\n",
        "                h = activation[i, pos, :].float()\n",
        "                activation[i, pos, :] = (h + sign_tensor[i] * contrib_stack[i]).to(activation.dtype)\n",
        "            return activation\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            with model.hooks(fwd_hooks=[(hook_name, hook_fn)]):\n",
        "                logits = model(token_ids)\n",
        "\n",
        "        for i, s in enumerate(batch_specs):\n",
        "            results[batch_start + i] = compute_ev(\n",
        "                logits[i, last_pos[i], :].cpu(), s.scale_min, s.scale_max\n",
        "            )\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# --- Random feature sampling ---\n",
        "\n",
        "def sample_active_random_features(sae_a, sae_b, k, diff_all, rng=None):\n",
        "    if rng is None: rng = np.random.default_rng()\n",
        "    active_mask = ((sae_a > 0) | (sae_b > 0)).cpu().numpy()\n",
        "    active_indices = np.where(active_mask)[0]\n",
        "    if len(active_indices) < k:\n",
        "        active_indices = np.arange(len(active_mask))\n",
        "    chosen = rng.choice(active_indices, size=min(k, len(active_indices)), replace=False)\n",
        "    features_random = torch.tensor(chosen, device=DEVICE, dtype=torch.long)\n",
        "    return features_random, diff_all[features_random]\n",
        "\n",
        "\n",
        "# --- Cross-pair reservoir ---\n",
        "\n",
        "class CrossPairReservoir:\n",
        "    def __init__(self):\n",
        "        self.reservoir = {}\n",
        "\n",
        "    def add(self, demo, domain, features, delta):\n",
        "        key = (demo, domain)\n",
        "        if key not in self.reservoir:\n",
        "            self.reservoir[key] = []\n",
        "        self.reservoir[key].append((features.clone(), delta.clone()))\n",
        "\n",
        "    def get_cross(self, demo, domain, k, rng=None):\n",
        "        if rng is None: rng = np.random.default_rng()\n",
        "        candidates = []\n",
        "        for (d, dom), pairs in self.reservoir.items():\n",
        "            if d != demo or dom != domain:\n",
        "                candidates.extend(pairs)\n",
        "        if len(candidates) == 0:\n",
        "            return None, None\n",
        "        feat, delta = candidates[rng.integers(len(candidates))]\n",
        "        return feat[:k].to(DEVICE), delta[:k].to(DEVICE)\n",
        "\n",
        "\n",
        "def pre_populate_cross_reservoir(prompts_df, layer, sae):\n",
        "    print(f\"  Pre-populating cross-pair reservoir...\")\n",
        "    reservoir = CrossPairReservoir()\n",
        "    n_added = 0\n",
        "\n",
        "    for demo in ALL_DEMOGRAPHICS:\n",
        "        for domain in ALL_DOMAINS:\n",
        "            pairs = build_pairs(prompts_df, demo, domain, N_PAIRS_PER_CONDITION)\n",
        "            if not pairs: continue\n",
        "\n",
        "            sae_acts_a, evs_a, _ = get_baselines_batched(\n",
        "                [p['prompt_a'] for p in pairs], layer, sae,\n",
        "                [p['scale_min'] for p in pairs], [p['scale_max'] for p in pairs]\n",
        "            )\n",
        "            sae_acts_b, evs_b, _ = get_baselines_batched(\n",
        "                [p['prompt_b'] for p in pairs], layer, sae,\n",
        "                [p['scale_min'] for p in pairs], [p['scale_max'] for p in pairs]\n",
        "            )\n",
        "\n",
        "            for i, pair in enumerate(pairs):\n",
        "                effect_ev = evs_a[i] - evs_b[i]\n",
        "                threshold = get_scale_normalized_threshold(pair['scale_min'], pair['scale_max'])\n",
        "                if abs(effect_ev) < threshold: continue\n",
        "\n",
        "                diff = (sae_acts_a[i] - sae_acts_b[i]).float()\n",
        "                sorted_idx = torch.argsort(diff.abs(), descending=True)\n",
        "                reservoir.add(demo, domain, sorted_idx[:max(K_VALUES)], diff[sorted_idx[:max(K_VALUES)]])\n",
        "                n_added += 1\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"  Cross-pair reservoir: {n_added} entries, {len(reservoir.reservoir)} conditions\")\n",
        "    return reservoir\n",
        "\n",
        "\n",
        "print(\"\\nSetup complete.\")"
      ],
      "metadata": {
        "id": "avygTBmru4TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Causal validation — main validation loop\n",
        "#\n",
        "# Requires: all variables from cell 1\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RUNNING VALIDATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_results = []\n",
        "top_features_log = []\n",
        "exclusion_log = []\n",
        "\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    sae_type = 'IT' if layer in IT_LAYERS else 'PT'\n",
        "    depth = LAYER_DEPTH.get(layer, '?')\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LAYER {layer} ({depth}% depth, {sae_type} SAE)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    t0 = datetime.now()\n",
        "    sae = sae_manager.load_sae(layer)\n",
        "\n",
        "    if hasattr(sae, 'threshold') and sae.threshold is not None:\n",
        "        print(f\"  SAE: JumpReLU (threshold mean={float(sae.threshold.mean()):.3f})\")\n",
        "\n",
        "    layer_agg_features = aggregate_features.get(layer, {})\n",
        "    cross_reservoir = pre_populate_cross_reservoir(prompts_df, layer, sae)\n",
        "    rng = np.random.default_rng(SEED + layer)\n",
        "\n",
        "    for demo in ALL_DEMOGRAPHICS:\n",
        "        for domain in ALL_DOMAINS:\n",
        "            pairs = build_pairs(prompts_df, demo, domain, N_PAIRS_PER_CONDITION)\n",
        "            if not pairs: continue\n",
        "\n",
        "            agg_key = f\"{demo}_{domain}\"\n",
        "            agg_feature_list = layer_agg_features.get(agg_key, None)\n",
        "\n",
        "            all_prompts_a = [p['prompt_a'] for p in pairs]\n",
        "            all_prompts_b = [p['prompt_b'] for p in pairs]\n",
        "            scale_mins = [p['scale_min'] for p in pairs]\n",
        "            scale_maxs = [p['scale_max'] for p in pairs]\n",
        "\n",
        "            sae_acts_a, evs_a, n_active_a = get_baselines_batched(\n",
        "                all_prompts_a, layer, sae, scale_mins, scale_maxs\n",
        "            )\n",
        "            sae_acts_b, evs_b, n_active_b = get_baselines_batched(\n",
        "                all_prompts_b, layer, sae, scale_mins, scale_maxs\n",
        "            )\n",
        "\n",
        "            valid_pairs, skipped_pairs = 0, 0\n",
        "\n",
        "            for pair_idx, pair in enumerate(pairs):\n",
        "                scale_min, scale_max = pair['scale_min'], pair['scale_max']\n",
        "                ev_a, ev_b = evs_a[pair_idx], evs_b[pair_idx]\n",
        "                sa, sb = sae_acts_a[pair_idx], sae_acts_b[pair_idx]\n",
        "                effect_ev = ev_a - ev_b\n",
        "\n",
        "                threshold = get_scale_normalized_threshold(scale_min, scale_max)\n",
        "                if abs(effect_ev) < threshold:\n",
        "                    skipped_pairs += 1\n",
        "                    exclusion_log.append({\n",
        "                        'layer': layer, 'demographic': demo, 'domain': domain,\n",
        "                        'question_id': pair['question_id'],\n",
        "                        'question_type': pair.get('question_type', 'unknown'),\n",
        "                        'scale_min': scale_min, 'scale_max': scale_max,\n",
        "                        'scale_range': scale_max - scale_min,\n",
        "                        'threshold': threshold, 'effect_ev': effect_ev, 'excluded': True,\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "                valid_pairs += 1\n",
        "                diff = (sa - sb).float()\n",
        "                sorted_idx = torch.argsort(diff.abs(), descending=True)\n",
        "\n",
        "                top_features_log.append({\n",
        "                    'layer': layer, 'demographic': demo, 'domain': domain,\n",
        "                    'pair_key': pair['pair_key'], 'effect_ev': effect_ev,\n",
        "                    'top_5_features': sorted_idx[:5].cpu().numpy().tolist(),\n",
        "                    'top_5_deltas': diff[sorted_idx[:5]].cpu().numpy().tolist(),\n",
        "                })\n",
        "\n",
        "                result_base = {\n",
        "                    'pair_key': pair['pair_key'], 'demographic': demo, 'domain': domain,\n",
        "                    'layer': layer, 'question_id': pair['question_id'],\n",
        "                    'question_type': pair.get('question_type', 'unknown'),\n",
        "                    'scale_min': scale_min, 'scale_max': scale_max,\n",
        "                    'ev_a_base': ev_a, 'ev_b_base': ev_b, 'effect_ev_base': effect_ev,\n",
        "                    'n_active_a': n_active_a[pair_idx], 'n_active_b': n_active_b[pair_idx],\n",
        "                }\n",
        "\n",
        "                for K in K_VALUES:\n",
        "                    top_k = sorted_idx[:K]\n",
        "                    delta_k = diff[top_k]\n",
        "\n",
        "                    specs, labels = [], []\n",
        "\n",
        "                    # Per-pair: patch, steer forward, steer reverse, ablate\n",
        "                    specs.append(InterventionSpec(pair['prompt_b'], top_k, delta_k, 'add', scale_min, scale_max))\n",
        "                    labels.append('patch_same_perpair')\n",
        "                    specs.append(InterventionSpec(pair['prompt_a'], top_k, delta_k * STEERING_MULTIPLIER, 'add', scale_min, scale_max))\n",
        "                    labels.append('steer_same_perpair')\n",
        "                    specs.append(InterventionSpec(pair['prompt_a'], top_k, delta_k * (-STEERING_MULTIPLIER), 'add', scale_min, scale_max))\n",
        "                    labels.append('steer_reverse_perpair')\n",
        "                    specs.append(InterventionSpec(pair['prompt_a'], top_k, delta_k, 'sub', scale_min, scale_max))\n",
        "                    labels.append('ablate_same_perpair')\n",
        "\n",
        "                    # Cross-pair\n",
        "                    cross_feat, cross_delta = cross_reservoir.get_cross(demo, domain, K, rng=rng)\n",
        "                    has_cross = cross_feat is not None\n",
        "                    if has_cross:\n",
        "                        specs.append(InterventionSpec(pair['prompt_b'], cross_feat, cross_delta, 'add', scale_min, scale_max))\n",
        "                        labels.append('patch_cross')\n",
        "\n",
        "                    # Aggregate\n",
        "                    has_agg = (agg_feature_list is not None and len(agg_feature_list) >= K)\n",
        "                    if has_agg:\n",
        "                        agg_feats = torch.tensor(agg_feature_list[:K], device=DEVICE, dtype=torch.long)\n",
        "                        agg_delta = diff[agg_feats]\n",
        "                        specs.append(InterventionSpec(pair['prompt_b'], agg_feats, agg_delta, 'add', scale_min, scale_max))\n",
        "                        labels.append('patch_agg')\n",
        "                        specs.append(InterventionSpec(pair['prompt_a'], agg_feats, agg_delta * STEERING_MULTIPLIER, 'add', scale_min, scale_max))\n",
        "                        labels.append('steer_agg')\n",
        "                        specs.append(InterventionSpec(pair['prompt_a'], agg_feats, agg_delta, 'sub', scale_min, scale_max))\n",
        "                        labels.append('ablate_agg')\n",
        "\n",
        "                    # Random controls\n",
        "                    for draw in range(N_RANDOM_DRAWS):\n",
        "                        rng_a = np.random.default_rng(SEED + layer * 100003 + pair_idx * 1009 + K * 101 + draw * 7)\n",
        "                        rng_b = np.random.default_rng(SEED + layer * 200003 + pair_idx * 2003 + K * 211 + draw * 13)\n",
        "\n",
        "                        feat_ra, delta_ra = sample_active_random_features(sa, sb, K, diff, rng=rng_a)\n",
        "                        feat_rb, _ = sample_active_random_features(sa, sb, K, diff, rng=rng_b)\n",
        "                        perm_seed = SEED + layer * 300007 + pair_idx * 3001 + draw * 17\n",
        "                        delta_shuffled = delta_k[torch.randperm(K, generator=torch.Generator().manual_seed(perm_seed))]\n",
        "\n",
        "                        specs.append(InterventionSpec(pair['prompt_b'], feat_ra, delta_ra, 'add', scale_min, scale_max))\n",
        "                        labels.append(f'patch_rand_a_{draw}')\n",
        "                        specs.append(InterventionSpec(pair['prompt_b'], feat_rb, delta_shuffled, 'add', scale_min, scale_max))\n",
        "                        labels.append(f'patch_rand_b_{draw}')\n",
        "                        specs.append(InterventionSpec(pair['prompt_a'], feat_ra, delta_ra * STEERING_MULTIPLIER, 'add', scale_min, scale_max))\n",
        "                        labels.append(f'steer_rand_{draw}')\n",
        "                        specs.append(InterventionSpec(pair['prompt_a'], feat_ra, delta_ra, 'sub', scale_min, scale_max))\n",
        "                        labels.append(f'ablate_rand_{draw}')\n",
        "\n",
        "                    # Dispatch\n",
        "                    ev_results = run_interventions_batched(specs, layer, sae)\n",
        "                    R = dict(zip(labels, ev_results))\n",
        "\n",
        "                    # --- Parse results into rows ---\n",
        "\n",
        "                    def _add(condition, method, recovery=np.nan, reduction=np.nan, increase=np.nan, dir_pres=np.nan):\n",
        "                        all_results.append({\n",
        "                            **result_base, 'K': K,\n",
        "                            'condition': condition, 'feature_method': method,\n",
        "                            'recovery_ev': recovery, 'reduction_ev': reduction,\n",
        "                            'increase_ev': increase, 'direction_preserved': dir_pres,\n",
        "                        })\n",
        "\n",
        "                    # Patching\n",
        "                    rec = (R['patch_same_perpair'] - ev_b) / effect_ev if effect_ev != 0 else 0\n",
        "                    _add('patch_same', 'per_pair', recovery=rec)\n",
        "\n",
        "                    rec_ra = (np.mean([R[f'patch_rand_a_{d}'] for d in range(N_RANDOM_DRAWS)]) - ev_b) / effect_ev if effect_ev != 0 else 0\n",
        "                    _add('patch_random_a', 'active_random', recovery=rec_ra)\n",
        "\n",
        "                    rec_rb = (np.mean([R[f'patch_rand_b_{d}'] for d in range(N_RANDOM_DRAWS)]) - ev_b) / effect_ev if effect_ev != 0 else 0\n",
        "                    _add('patch_random_b', 'active_random', recovery=rec_rb)\n",
        "\n",
        "                    if has_cross:\n",
        "                        rec_c = (R['patch_cross'] - ev_b) / effect_ev if effect_ev != 0 else 0\n",
        "                        _add('patch_cross', 'cross_demo_domain', recovery=rec_c)\n",
        "\n",
        "                    # Steering\n",
        "                    eff_s = R['steer_same_perpair'] - ev_b\n",
        "                    _add('steer_same', 'per_pair',\n",
        "                         increase=(eff_s / effect_ev - 1.0) if effect_ev != 0 else 0,\n",
        "                         dir_pres=bool(np.sign(eff_s) == np.sign(effect_ev)))\n",
        "\n",
        "                    eff_r = R['steer_reverse_perpair'] - ev_b\n",
        "                    _add('steer_reverse', 'per_pair',\n",
        "                         increase=(eff_r / effect_ev - 1.0) if effect_ev != 0 else 0,\n",
        "                         dir_pres=bool(np.sign(eff_r) == np.sign(effect_ev)))\n",
        "\n",
        "                    avg_rs = np.mean([R[f'steer_rand_{d}'] for d in range(N_RANDOM_DRAWS)])\n",
        "                    eff_rs = avg_rs - ev_b\n",
        "                    _add('steer_random', 'active_random',\n",
        "                         increase=(eff_rs / effect_ev - 1.0) if effect_ev != 0 else 0,\n",
        "                         dir_pres=bool(np.sign(eff_rs) == np.sign(effect_ev)))\n",
        "\n",
        "                    # Ablation\n",
        "                    eff_abl = R['ablate_same_perpair'] - ev_b\n",
        "                    _add('ablate_same', 'per_pair',\n",
        "                         reduction=1.0 - (eff_abl / effect_ev) if effect_ev != 0 else 0)\n",
        "\n",
        "                    avg_ra = np.mean([R[f'ablate_rand_{d}'] for d in range(N_RANDOM_DRAWS)])\n",
        "                    eff_ra = avg_ra - ev_b\n",
        "                    _add('ablate_random', 'active_random',\n",
        "                         reduction=1.0 - (eff_ra / effect_ev) if effect_ev != 0 else 0)\n",
        "\n",
        "                    # Aggregate\n",
        "                    if has_agg:\n",
        "                        rec_ag = (R['patch_agg'] - ev_b) / effect_ev if effect_ev != 0 else 0\n",
        "                        _add('patch_same', 'aggregate', recovery=rec_ag)\n",
        "\n",
        "                        eff_ag_s = R['steer_agg'] - ev_b\n",
        "                        _add('steer_same', 'aggregate',\n",
        "                             increase=(eff_ag_s / effect_ev - 1.0) if effect_ev != 0 else 0,\n",
        "                             dir_pres=bool(np.sign(eff_ag_s) == np.sign(effect_ev)))\n",
        "\n",
        "                        eff_ag_a = R['ablate_agg'] - ev_b\n",
        "                        _add('ablate_same', 'aggregate',\n",
        "                             reduction=1.0 - (eff_ag_a / effect_ev) if effect_ev != 0 else 0)\n",
        "\n",
        "                if valid_pairs % 10 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            if valid_pairs > 0 or skipped_pairs > 0:\n",
        "                print(f\"  {demo}_{domain}: {valid_pairs} valid, {skipped_pairs} skipped\")\n",
        "\n",
        "    del sae\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"\\nLayer {layer} ({depth}%) done: {(datetime.now() - t0).total_seconds() / 60:.1f} min\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "top_features_df = pd.DataFrame(top_features_log)\n",
        "exclusion_df = pd.DataFrame(exclusion_log)\n",
        "print(f\"\\nValidation complete: {len(results_df)} result rows\")"
      ],
      "metadata": {
        "id": "XsBXHcN0vmbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Causal validation — analysis, summary, saving\n",
        "#\n",
        "# Requires: results_df, top_features_df, exclusion_df from cell 2\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ANALYZING RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# --- Exclusion rates ---\n",
        "\n",
        "if len(exclusion_df) > 0:\n",
        "    print(\"\\nExclusion rates by question type:\")\n",
        "    for qtype in exclusion_df['question_type'].unique():\n",
        "        n_excl = len(exclusion_df[exclusion_df['question_type'] == qtype])\n",
        "        n_incl = results_df[results_df['question_type'] == qtype]['pair_key'].nunique() if 'question_type' in results_df.columns else 0\n",
        "        total = n_excl + n_incl\n",
        "        print(f\"  {qtype}: {n_excl}/{total} ({n_excl/total*100:.1f}%)\" if total > 0 else f\"  {qtype}: 0\")\n",
        "\n",
        "# --- Stats helpers ---\n",
        "\n",
        "def compute_stats(values):\n",
        "    values = values.dropna()\n",
        "    if len(values) == 0: return None\n",
        "    t_stat, p_val = ttest_1samp(values, 0) if len(values) >= 3 else (np.nan, np.nan)\n",
        "    return {\n",
        "        'mean': float(values.mean()), 'std': float(values.std()),\n",
        "        'median': float(values.median()),\n",
        "        'pct_positive': float((values > 0).mean() * 100),\n",
        "        'n': int(len(values)),\n",
        "        't_stat': float(t_stat) if not np.isnan(t_stat) else None,\n",
        "        'p_value': float(p_val) if not np.isnan(p_val) else None,\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_comparison(df, cond1, cond2, metric, method='per_pair'):\n",
        "    vals1 = df[(df['condition'] == cond1) & (df['feature_method'] == method)][metric].dropna()\n",
        "    vals2 = df[(df['condition'] == cond2) & (df['feature_method'] == 'active_random')][metric].dropna()\n",
        "    if len(vals1) < 5 or len(vals2) < 5: return None\n",
        "    t_stat, p_val = ttest_ind(vals1, vals2, equal_var=False)\n",
        "    denom = np.sqrt((vals1.var(ddof=1) + vals2.var(ddof=1)) / 2)\n",
        "    cohens_d = (vals1.mean() - vals2.mean()) / denom if denom > 0 else 0\n",
        "    return {\n",
        "        'diff': float(vals1.mean() - vals2.mean()),\n",
        "        't_stat': float(t_stat), 'p_value': float(p_val),\n",
        "        'cohens_d': float(cohens_d), 'n1': int(len(vals1)), 'n2': int(len(vals2)),\n",
        "    }\n",
        "\n",
        "\n",
        "def apply_fdr_to_comparisons(summary_dict):\n",
        "    p_values, p_locations = [], []\n",
        "\n",
        "    for layer_str, layer_data in summary_dict['layers'].items():\n",
        "        for k_str, k_data in layer_data.get('by_K', {}).items():\n",
        "            for intervention in ['patching', 'steering', 'ablation']:\n",
        "                comp = k_data.get(intervention, {}).get('same_vs_random')\n",
        "                if comp and comp.get('p_value') is not None:\n",
        "                    p_values.append(comp['p_value'])\n",
        "                    p_locations.append(('by_K', layer_str, k_str, intervention))\n",
        "\n",
        "        for demo_name, demo_data in layer_data.get('by_demographic', {}).items():\n",
        "            for intervention in ['patching', 'steering', 'ablation']:\n",
        "                stats = demo_data.get(intervention)\n",
        "                if stats and stats.get('p_value') is not None:\n",
        "                    p_values.append(stats['p_value'])\n",
        "                    p_locations.append(('by_demo', layer_str, demo_name, intervention))\n",
        "\n",
        "    if not p_values: return summary_dict\n",
        "    rejected, p_corrected, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05)\n",
        "\n",
        "    for i, loc in enumerate(p_locations):\n",
        "        if loc[0] == 'by_K':\n",
        "            comp = summary_dict['layers'][loc[1]]['by_K'][loc[2]][loc[3]]['same_vs_random']\n",
        "            comp['p_value_fdr'] = float(p_corrected[i])\n",
        "            comp['significant_fdr'] = bool(rejected[i])\n",
        "        elif loc[0] == 'by_demo':\n",
        "            stats = summary_dict['layers'][loc[1]]['by_demographic'][loc[2]][loc[3]]\n",
        "            stats['p_value_fdr'] = float(p_corrected[i])\n",
        "            stats['significant_fdr'] = bool(rejected[i])\n",
        "\n",
        "    summary_dict['fdr_correction'] = {\n",
        "        'n_tests': len(p_values), 'n_significant': int(sum(rejected)),\n",
        "        'n_by_K': sum(1 for l in p_locations if l[0] == 'by_K'),\n",
        "        'n_by_demographic': sum(1 for l in p_locations if l[0] == 'by_demo'),\n",
        "        'method': 'fdr_bh',\n",
        "    }\n",
        "    return summary_dict\n",
        "\n",
        "\n",
        "# --- Build summary ---\n",
        "\n",
        "summary = {\n",
        "    'metadata': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'model': 'gemma-2-9b-it',\n",
        "        'sae_activation': 'JumpReLU',\n",
        "        'layers': ANALYSIS_LAYERS, 'layer_depths': LAYER_DEPTH,\n",
        "        'it_sae_layers': list(IT_LAYERS), 'k_values': K_VALUES,\n",
        "        'base_min_effect': BASE_MIN_EFFECT, 'steering_multiplier': STEERING_MULTIPLIER,\n",
        "        'n_random_draws': N_RANDOM_DRAWS, 'n_pairs_per_condition': N_PAIRS_PER_CONDITION,\n",
        "        'batch_size': BATCH_SIZE, 'has_aggregate_features': bool(aggregate_features),\n",
        "    },\n",
        "    'layers': {},\n",
        "}\n",
        "\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    layer_df = results_df[results_df['layer'] == layer]\n",
        "    summary['layers'][str(layer)] = {'by_K': {}, 'by_demographic': {}}\n",
        "\n",
        "    for K in K_VALUES:\n",
        "        k_df = layer_df[layer_df['K'] == K]\n",
        "        k_pp = k_df[k_df['feature_method'] == 'per_pair']\n",
        "\n",
        "        k_entry = {\n",
        "            'patching': {\n",
        "                'same': compute_stats(k_pp[k_pp['condition'] == 'patch_same']['recovery_ev']),\n",
        "                'random_a': compute_stats(k_df[k_df['condition'] == 'patch_random_a']['recovery_ev']),\n",
        "                'random_b': compute_stats(k_df[k_df['condition'] == 'patch_random_b']['recovery_ev']),\n",
        "                'cross': compute_stats(k_df[k_df['condition'] == 'patch_cross']['recovery_ev']),\n",
        "                'same_vs_random': compute_comparison(k_df, 'patch_same', 'patch_random_b', 'recovery_ev'),\n",
        "            },\n",
        "            'steering': {\n",
        "                'same': compute_stats(k_pp[k_pp['condition'] == 'steer_same']['increase_ev']),\n",
        "                'reverse': compute_stats(k_pp[k_pp['condition'] == 'steer_reverse']['increase_ev']),\n",
        "                'random': compute_stats(k_df[k_df['condition'] == 'steer_random']['increase_ev']),\n",
        "                'same_vs_random': compute_comparison(k_df, 'steer_same', 'steer_random', 'increase_ev'),\n",
        "                'direction_preserved_pct': float(\n",
        "                    k_pp[k_pp['condition'] == 'steer_same']['direction_preserved'].mean() * 100\n",
        "                ) if len(k_pp[k_pp['condition'] == 'steer_same']) > 0 else None,\n",
        "            },\n",
        "            'ablation': {\n",
        "                'same': compute_stats(k_pp[k_pp['condition'] == 'ablate_same']['reduction_ev']),\n",
        "                'random': compute_stats(k_df[k_df['condition'] == 'ablate_random']['reduction_ev']),\n",
        "                'same_vs_random': compute_comparison(k_df, 'ablate_same', 'ablate_random', 'reduction_ev'),\n",
        "            },\n",
        "        }\n",
        "\n",
        "        k_agg = k_df[k_df['feature_method'] == 'aggregate']\n",
        "        if len(k_agg) > 0:\n",
        "            k_entry['aggregate'] = {\n",
        "                'patching': compute_stats(k_agg[k_agg['condition'] == 'patch_same']['recovery_ev']),\n",
        "                'steering': compute_stats(k_agg[k_agg['condition'] == 'steer_same']['increase_ev']),\n",
        "                'ablation': compute_stats(k_agg[k_agg['condition'] == 'ablate_same']['reduction_ev']),\n",
        "            }\n",
        "\n",
        "        summary['layers'][str(layer)]['by_K'][str(K)] = k_entry\n",
        "\n",
        "    for demo_name in ALL_DEMOGRAPHICS:\n",
        "        demo_df = layer_df[\n",
        "            (layer_df['demographic'] == demo_name) &\n",
        "            (layer_df['K'] == 50) &\n",
        "            (layer_df['feature_method'] == 'per_pair')\n",
        "        ]\n",
        "        if len(demo_df) > 0:\n",
        "            summary['layers'][str(layer)]['by_demographic'][demo_name] = {\n",
        "                'patching': compute_stats(demo_df[demo_df['condition'] == 'patch_same']['recovery_ev']),\n",
        "                'steering': compute_stats(demo_df[demo_df['condition'] == 'steer_same']['increase_ev']),\n",
        "                'ablation': compute_stats(demo_df[demo_df['condition'] == 'ablate_same']['reduction_ev']),\n",
        "                'n_pairs': int(len(demo_df[demo_df['condition'] == 'patch_same'])),\n",
        "            }\n",
        "\n",
        "summary = apply_fdr_to_comparisons(summary)\n",
        "\n",
        "# --- P(10) sensitivity ---\n",
        "\n",
        "has_scale11 = (results_df['scale_max'] >= 10).any() if len(results_df) > 0 else False\n",
        "sensitivity_results = {'has_scale11': bool(has_scale11)}\n",
        "\n",
        "if has_scale11:\n",
        "    affected = results_df[results_df['scale_max'] >= 10][['pair_key', 'layer']].drop_duplicates()\n",
        "    frac = len(affected) / len(results_df['pair_key'].unique()) if len(results_df) > 0 else 0\n",
        "    sensitivity_results.update({\n",
        "        'n_affected_pairs': int(len(affected)),\n",
        "        'splits_tested': P10_SPLITS, 'default_split': 0.5,\n",
        "        'affected_fraction': float(frac),\n",
        "    })\n",
        "    print(f\"\\nP(10) sensitivity: {frac:.1%} of pairs affected\")\n",
        "else:\n",
        "    print(\"\\nNo scale-11 questions — P(10) sensitivity N/A\")\n",
        "\n",
        "\n",
        "# --- Print summary ---\n",
        "\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    depth = LAYER_DEPTH.get(layer, '?')\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LAYER {layer} ({depth}% depth)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(f\"\\n{'K':<6} {'Patching':<20} {'Steering':<20} {'Ablation':<20}\")\n",
        "    print(\"-\" * 70)\n",
        "    for K in K_VALUES:\n",
        "        k_data = summary['layers'][str(layer)]['by_K'][str(K)]\n",
        "        p = k_data['patching']['same']\n",
        "        s = k_data['steering']['same']\n",
        "        a = k_data['ablation']['same']\n",
        "        print(f\"{K:<6} {p['mean']:+.1%  if p else 'N/A':<20} \"\n",
        "              f\"{s['mean']:+.1% if s else 'N/A':<20} \"\n",
        "              f\"{a['mean']:+.1% if a else 'N/A':<20}\")\n",
        "\n",
        "    k50 = summary['layers'][str(layer)]['by_K']['50']\n",
        "    if 'aggregate' in k50:\n",
        "        print(f\"\\nAggregate vs Per-Pair (K=50):\")\n",
        "        for metric, label in [('patching', 'Patch'), ('steering', 'Steer'), ('ablation', 'Ablate')]:\n",
        "            pp = k50[metric]['same']\n",
        "            ag = k50['aggregate'].get(metric)\n",
        "            print(f\"  {label}: per-pair={pp['mean']:+.1%}, agg={ag['mean']:+.1%}\" if pp and ag else f\"  {label}: N/A\")\n",
        "\n",
        "    if k50['patching']['same_vs_random']:\n",
        "        comp = k50['patching']['same_vs_random']\n",
        "        fdr_p = comp.get('p_value_fdr', comp['p_value'])\n",
        "        print(f\"\\n  Same vs Random: d={comp['cohens_d']:.2f}, p_fdr={fdr_p:.4f}\")\n",
        "\n",
        "    if k50['steering'].get('direction_preserved_pct') is not None:\n",
        "        print(f\"  Direction preserved: {k50['steering']['direction_preserved_pct']:.1f}%\")\n",
        "\n",
        "    print(f\"\\n  {'Demo':<12} {'N':<6} {'Patch':<12} {'Steer':<12} {'Ablate':<12}\")\n",
        "    print(\"  \" + \"-\" * 55)\n",
        "    for d in ALL_DEMOGRAPHICS:\n",
        "        if d in summary['layers'][str(layer)]['by_demographic']:\n",
        "            dd = summary['layers'][str(layer)]['by_demographic'][d]\n",
        "            n = dd['n_pairs']\n",
        "            pv = f\"{dd['patching']['mean']:+.1%}\" if dd['patching'] else \"N/A\"\n",
        "            sv = f\"{dd['steering']['mean']:+.1%}\" if dd['steering'] else \"N/A\"\n",
        "            av = f\"{dd['ablation']['mean']:+.1%}\" if dd['ablation'] else \"N/A\"\n",
        "            print(f\"  {d:<12} {n:<6} {pv:<12} {sv:<12} {av:<12}\")\n",
        "\n",
        "\n",
        "# --- Save ---\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAVING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "results_df.to_csv(OUTPUT_DIR / 'validation_results.csv', index=False)\n",
        "top_features_df.to_csv(OUTPUT_DIR / 'top_features.csv', index=False)\n",
        "exclusion_df.to_csv(OUTPUT_DIR / 'exclusion_log.csv', index=False)\n",
        "\n",
        "summary['sensitivity_p10'] = sensitivity_results\n",
        "with open(OUTPUT_DIR / 'validation_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2, default=str)\n",
        "with open(OUTPUT_DIR / 'sensitivity_p10.json', 'w') as f:\n",
        "    json.dump(sensitivity_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"validation_results.csv ({len(results_df)} rows)\")\n",
        "print(f\"top_features.csv ({len(top_features_df)} rows)\")\n",
        "print(f\"exclusion_log.csv ({len(exclusion_df)} rows)\")\n",
        "print(f\"validation_summary.json\")\n",
        "print(f\"sensitivity_p10.json\")\n",
        "\n",
        "# --- Top features for interpretation ---\n",
        "\n",
        "feature_counts = defaultdict(lambda: defaultdict(int))\n",
        "for _, row in top_features_df.iterrows():\n",
        "    key = (row['layer'], row['demographic'])\n",
        "    for feat in row['top_5_features']:\n",
        "        feature_counts[key][feat] += 1\n",
        "\n",
        "for rep_layer in [5, 20, 36]:\n",
        "    if rep_layer in ANALYSIS_LAYERS:\n",
        "        print(f\"\\nFrequent top-5 features (L{rep_layer}, {LAYER_DEPTH.get(rep_layer, '?')}%):\")\n",
        "        for d in ALL_DEMOGRAPHICS:\n",
        "            key = (rep_layer, d)\n",
        "            if key in feature_counts:\n",
        "                top = sorted(feature_counts[key].items(), key=lambda x: -x[1])[:5]\n",
        "                print(f\"  {d}: {', '.join(f'F{f}({c}x)' for f, c in top)}\")\n",
        "\n",
        "# --- Google Drive / zip ---\n",
        "\n",
        "drive_base = Path(\"/content/drive/MyDrive\")\n",
        "drive_dest = drive_base / \"TACL_Gemma_Results\" / \"causal_validation\"\n",
        "\n",
        "if drive_base.exists():\n",
        "    drive_dest.mkdir(parents=True, exist_ok=True)\n",
        "    for fname in ['validation_results.csv', 'top_features.csv', 'exclusion_log.csv',\n",
        "                   'validation_summary.json', 'sensitivity_p10.json']:\n",
        "        src = OUTPUT_DIR / fname\n",
        "        if src.exists(): shutil.copy2(src, drive_dest / fname)\n",
        "    print(f\"\\nSaved to Drive: {drive_dest}\")\n",
        "else:\n",
        "    zip_file = shutil.make_archive(str(Path('.') / \"gemma_causal_validation\"), 'zip', root_dir=str(OUTPUT_DIR))\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(zip_file)\n",
        "    except ImportError:\n",
        "        print(f\"Zip: {zip_file}\")\n",
        "\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "ChvC69zov8Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Causal validation analysis — configuration and data loading\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ast\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from scipy.stats import ttest_rel, pearsonr\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def safe_parse_list(x):\n",
        "    if isinstance(x, str):\n",
        "        return ast.literal_eval(x)\n",
        "    return x\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans', 'font.size': 11,\n",
        "    'axes.labelsize': 12, 'axes.titlesize': 13, 'legend.fontsize': 10,\n",
        "    'figure.dpi': 150, 'savefig.dpi': 300, 'savefig.bbox': 'tight',\n",
        "})\n",
        "\n",
        "MODEL_NAME = \"Gemma 2 9B IT\"\n",
        "ANALYSIS_LAYERS = [5, 9, 14, 18, 20, 27, 32, 36]\n",
        "BEST_LAYER = 36\n",
        "N_LAYERS_TOTAL = 42\n",
        "\n",
        "LAYER_DEPTH = {5: 12, 9: 22, 14: 34, 18: 44, 20: 49, 27: 66, 32: 78, 36: 88}\n",
        "IT_LAYERS = {20}\n",
        "\n",
        "EARLY_LAYERS = [5, 9, 14]\n",
        "MID_LAYERS = [18, 20]\n",
        "LATE_LAYERS = [27, 32, 36]\n",
        "\n",
        "def layer_label(l, short=False):\n",
        "    depth = LAYER_DEPTH.get(l, l / N_LAYERS_TOTAL * 100)\n",
        "    it_tag = \" (IT)\" if l in IT_LAYERS else \"\"\n",
        "    return f\"L{l}{it_tag}\" if short else f\"Layer {l} ({depth:.0f}%{it_tag})\"\n",
        "\n",
        "def layer_label_compact(l):\n",
        "    depth = LAYER_DEPTH.get(l, '?')\n",
        "    it_tag = \"*\" if l in IT_LAYERS else \"\"\n",
        "    return f\"L{l} ({depth}%){it_tag}\"\n",
        "\n",
        "INPUT_DIR = Path(\"/content/\")\n",
        "VALIDATION_DIR = INPUT_DIR / \"causal_validation_final\"\n",
        "REANALYSIS_DIR = VALIDATION_DIR / \"reanalysis\"\n",
        "EXTRACTION_DIR = INPUT_DIR / \"outputs_gemma_replication\" / \"feature_extraction\"\n",
        "\n",
        "BUNDLE_DIR = Path(\"./causal-validation-gemma-results\")\n",
        "BUNDLE_DIR.mkdir(exist_ok=True)\n",
        "FIG_DIR = BUNDLE_DIR / \"figures\"\n",
        "TABLE_DIR = BUNDLE_DIR / \"tables\"\n",
        "DATA_DIR = BUNDLE_DIR / \"data\"\n",
        "SUMMARY_DIR = BUNDLE_DIR / \"summary\"\n",
        "for d in [FIG_DIR, TABLE_DIR, DATA_DIR, SUMMARY_DIR]:\n",
        "    d.mkdir(exist_ok=True)\n",
        "\n",
        "STEER_CLIP_LO, STEER_CLIP_HI = -2.0, 2.0\n",
        "\n",
        "COLORS = {\n",
        "    'patching': '#2ecc71', 'steering': '#3498db', 'ablation': '#e74c3c',\n",
        "    'random_a': '#7f8c8d', 'random_b': '#bdc3c7', 'random': '#95a5a6',\n",
        "    'cross': '#f39c12', 'aggregate': '#8e44ad',\n",
        "    'income': '#1abc9c', 'age': '#9b59b6', 'gender': '#e91e63',\n",
        "    'education': '#3498db', 'vote': '#f44336',\n",
        "    'early': '#3498db', 'mid': '#f39c12', 'late': '#e74c3c',\n",
        "}\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "results_df = pd.read_csv(VALIDATION_DIR / 'validation_results.csv')\n",
        "top_features_df = pd.read_csv(VALIDATION_DIR / 'top_features.csv')\n",
        "\n",
        "patched_summary_path = REANALYSIS_DIR / 'validation_summary_patched.json'\n",
        "old_summary_path = VALIDATION_DIR / 'validation_summary.json'\n",
        "\n",
        "if patched_summary_path.exists():\n",
        "    with open(patched_summary_path, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "    print(f\"  Loaded PATCHED summary (paired tests)\")\n",
        "    using_patched = True\n",
        "else:\n",
        "    with open(old_summary_path, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "    print(f\"  Using OLD summary (patched not found)\")\n",
        "    using_patched = False\n",
        "\n",
        "paired_comp_path = REANALYSIS_DIR / 'paired_comparisons.csv'\n",
        "paired_comp_df = pd.read_csv(paired_comp_path) if paired_comp_path.exists() else None\n",
        "\n",
        "behav_path = EXTRACTION_DIR / 'behavioral_effects.csv'\n",
        "behavioral_df = pd.read_csv(behav_path) if behav_path.exists() else None\n",
        "\n",
        "exclusion_path = VALIDATION_DIR / 'exclusion_log.csv'\n",
        "exclusion_df = pd.read_csv(exclusion_path) if exclusion_path.exists() else pd.DataFrame()\n",
        "\n",
        "feature_stats_path = EXTRACTION_DIR / 'feature_stats.csv'\n",
        "feature_stats_df = pd.read_csv(feature_stats_path) if feature_stats_path.exists() else None\n",
        "\n",
        "has_method = 'feature_method' in results_df.columns\n",
        "has_direction = 'direction_preserved' in results_df.columns\n",
        "\n",
        "print(f\"  Results: {len(results_df)} rows, layers: {sorted(results_df['layer'].unique())}\")\n",
        "print(f\"  Feature stats: {'yes' if feature_stats_df is not None else 'no'}\")\n",
        "\n",
        "def per_pair(df):\n",
        "    return df[df['feature_method'] == 'per_pair'] if has_method else df\n",
        "\n",
        "def agg_only(df):\n",
        "    return df[df['feature_method'] == 'aggregate'] if has_method else pd.DataFrame()\n",
        "\n",
        "def winsorize_steering(series):\n",
        "    return series.clip(lower=STEER_CLIP_LO, upper=STEER_CLIP_HI)\n",
        "\n",
        "ALL_DEMOS = ['income', 'age', 'gender', 'education', 'vote']\n",
        "best_str = str(BEST_LAYER)\n",
        "\n",
        "methods = [\n",
        "    ('Patching', 'patch_same', 'patch_random_a', 'recovery_ev'),\n",
        "    ('Steering', 'steer_same', 'steer_random', 'increase_ev'),\n",
        "    ('Ablation', 'ablate_same', 'ablate_random', 'reduction_ev'),\n",
        "]\n",
        "\n",
        "method_conds_list = ['patch_same', 'steer_same', 'ablate_same']\n",
        "method_metrics_list = ['recovery_ev', 'increase_ev', 'reduction_ev']\n",
        "\n",
        "print(\"Setup complete.\")"
      ],
      "metadata": {
        "id": "jWMEqA2bw8Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Causal validation analysis — figures 1-4c\n",
        "\n",
        "# --- 1. Behavioral effects heatmap ---\n",
        "\n",
        "print(\"1. Behavioral effects\")\n",
        "\n",
        "pairs_df = results_df[['pair_key', 'demographic', 'domain', 'layer',\n",
        "                        'effect_ev_base', 'ev_a_base', 'ev_b_base']].drop_duplicates()\n",
        "pairs_best = pairs_df[pairs_df['layer'] == BEST_LAYER]\n",
        "\n",
        "pivot_effects = pairs_best.pivot_table(\n",
        "    values='effect_ev_base', index='demographic', columns='domain', aggfunc='mean'\n",
        ")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.heatmap(pivot_effects, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
        "            linewidths=0.5, ax=ax, cbar_kws={'label': 'Effect (EV_a - EV_b)'})\n",
        "ax.set_title(f'{MODEL_NAME}: Baseline Demographic Effects by Domain')\n",
        "ax.set_xlabel('Domain'); ax.set_ylabel('Demographic')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig1_behavioral_effects_heatmap.png')\n",
        "plt.savefig(FIG_DIR / 'fig1_behavioral_effects_heatmap.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig1_behavioral_effects_heatmap\")\n",
        "\n",
        "# 1b. Exclusion rates\n",
        "if len(exclusion_df) > 0 and 'scale_range' in exclusion_df.columns:\n",
        "    scale_groups = exclusion_df.groupby('scale_range').agg(\n",
        "        n_excluded=('question_id', 'count'), threshold=('threshold', 'first')\n",
        "    ).reset_index()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.bar(scale_groups['scale_range'].astype(str), scale_groups['n_excluded'],\n",
        "           color='#e74c3c', edgecolor='black')\n",
        "    for i, row in scale_groups.iterrows():\n",
        "        ax.annotate(f\"thr={row['threshold']:.3f}\", (i, row['n_excluded']),\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "    ax.set_xlabel('Scale Range'); ax.set_ylabel('Pairs Excluded')\n",
        "    ax.set_title(f'{MODEL_NAME}: Exclusion Rates by Scale Range')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / 'fig1b_exclusion_rates.png')\n",
        "    plt.savefig(FIG_DIR / 'fig1b_exclusion_rates.pdf')\n",
        "    plt.close()\n",
        "    print(\"  Saved: fig1b_exclusion_rates\")\n",
        "\n",
        "\n",
        "# --- 2. Main causal validation results ---\n",
        "\n",
        "print(\"\\n2. Main results\")\n",
        "\n",
        "main_all = results_df[(results_df['layer'] == BEST_LAYER) & (results_df['K'] == 50)]\n",
        "main_df = per_pair(main_all)\n",
        "\n",
        "main_results = []\n",
        "computed_cohens_d = {}\n",
        "\n",
        "if using_patched:\n",
        "    k50_best = summary['layers'][best_str]['by_K']['50']\n",
        "    for name, same_cond, rand_cond, metric in methods:\n",
        "        same_stats = k50_best[name.lower()]['same']\n",
        "        if name == 'Patching':\n",
        "            comp = k50_best['patching']['same_vs_random_a']\n",
        "            rand_stats = k50_best['patching']['random_a']\n",
        "        elif name == 'Steering':\n",
        "            comp = k50_best['steering']['same_vs_random']\n",
        "            rand_stats = k50_best['steering']['random']\n",
        "        else:\n",
        "            comp = k50_best['ablation']['same_vs_random']\n",
        "            rand_stats = k50_best['ablation']['random']\n",
        "\n",
        "        d_z = comp['cohens_d_z'] if comp else 0\n",
        "        p_val = comp['p_value'] if comp else 1\n",
        "        pct_better = comp['pct_same_better'] if comp else 0\n",
        "        computed_cohens_d[name] = d_z\n",
        "\n",
        "        main_results.append({\n",
        "            'Method': name,\n",
        "            'Same-Pair': f\"{same_stats['mean']:.1%}\",\n",
        "            'Median': f\"{same_stats['median']:.1%}\",\n",
        "            'Random': f\"{rand_stats['mean']:.1%}\" if rand_stats else \"N/A\",\n",
        "            \"d_z (paired)\": f\"{d_z:.2f}\",\n",
        "            'p-value': f\"<1e-10\" if p_val < 1e-10 else f\"{p_val:.2e}\",\n",
        "            'Same>Rand': f\"{pct_better:.0f}%\",\n",
        "            'N': same_stats['n'],\n",
        "        })\n",
        "else:\n",
        "    for name, same_cond, rand_cond, metric in methods:\n",
        "        same = main_df[main_df['condition'] == same_cond][metric].dropna()\n",
        "        rand = main_all[main_all['condition'] == rand_cond][metric].dropna()\n",
        "        if 'steer' in same_cond:\n",
        "            same = winsorize_steering(same); rand = winsorize_steering(rand)\n",
        "        pooled_std = np.sqrt((same.var() + rand.var()) / 2)\n",
        "        d = (same.mean() - rand.mean()) / pooled_std if pooled_std > 0 else 0\n",
        "        computed_cohens_d[name] = d\n",
        "        main_results.append({\n",
        "            'Method': name, 'Same-Pair': f\"{same.mean():.1%}\",\n",
        "            'Median': f\"{same.median():.1%}\", 'Random': f\"{rand.mean():.1%}\",\n",
        "            \"Cohen's d\": f\"{d:.2f}\", 'p-value': \"<0.001\", 'N': len(same),\n",
        "        })\n",
        "\n",
        "main_results_df = pd.DataFrame(main_results)\n",
        "print(main_results_df.to_string(index=False))\n",
        "\n",
        "with open(TABLE_DIR / 'table1_main_results.tex', 'w') as f:\n",
        "    f.write(main_results_df.to_latex(index=False, escape=False))\n",
        "\n",
        "# 2a. Bar chart\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for idx, (name, same_cond, rand_cond, metric) in enumerate(methods):\n",
        "    ax = axes[idx]\n",
        "    same = main_df[main_df['condition'] == same_cond][metric].dropna()\n",
        "    if 'steer' in same_cond: same = winsorize_steering(same)\n",
        "\n",
        "    if 'patch' in same_cond:\n",
        "        rand_a = main_all[main_all['condition'] == 'patch_random_a'][metric].dropna()\n",
        "        rand_b = main_all[main_all['condition'] == 'patch_random_b'][metric].dropna()\n",
        "        cross = main_all[main_all['condition'] == 'patch_cross'][metric].dropna()\n",
        "        data = [same.mean(), rand_a.mean(), cross.mean(), rand_b.mean()]\n",
        "        errors = [same.sem(), rand_a.sem(), cross.sem(), rand_b.sem()]\n",
        "        labels = ['Same-Pair', 'Random-A\\n(active+delta)', 'Cross\\n(diff d×d)', 'Random-B\\n(shuffled Δ)']\n",
        "        colors = [COLORS['patching'], COLORS['random_a'], COLORS['cross'], COLORS['random_b']]\n",
        "    elif 'steer' in same_cond:\n",
        "        rand = winsorize_steering(main_all[main_all['condition'] == 'steer_random']['increase_ev'].dropna())\n",
        "        data = [same.mean(), rand.mean()]\n",
        "        errors = [same.sem(), rand.sem()]\n",
        "        labels = ['Same-Pair\\n(Winsor. ±200%)', 'Random\\n(Winsor. ±200%)']\n",
        "        colors = [COLORS['steering'], COLORS['random']]\n",
        "    else:\n",
        "        rand = main_all[main_all['condition'] == rand_cond][metric].dropna()\n",
        "        data = [same.mean(), rand.mean()]\n",
        "        errors = [same.sem(), rand.sem()]\n",
        "        labels = ['Same-Pair', 'Random\\n(active+delta)']\n",
        "        colors = [COLORS['ablation'], COLORS['random']]\n",
        "\n",
        "    bars = ax.bar(range(len(data)), [d * 100 for d in data],\n",
        "                  yerr=[e * 100 for e in errors], capsize=5,\n",
        "                  color=colors, edgecolor='black', linewidth=1)\n",
        "    ax.set_xticks(range(len(data)))\n",
        "    ax.set_xticklabels(labels, rotation=15, ha='right', fontsize=9)\n",
        "    ax.set_ylabel('Effect (%)')\n",
        "    ax.set_title(f'{name} (d_z={computed_cohens_d.get(name, 0):.2f})')\n",
        "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "    for bar, val in zip(bars, data):\n",
        "        h = bar.get_height()\n",
        "        ax.annotate(f'{val:.1%}', xy=(bar.get_x() + bar.get_width() / 2, h),\n",
        "                    xytext=(0, 3 if h >= 0 else -15), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom' if h >= 0 else 'top', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Causal Validation (Layer {BEST_LAYER}, K=50, Paired Tests)',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig2_main_results.png')\n",
        "plt.savefig(FIG_DIR / 'fig2_main_results.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig2_main_results\")\n",
        "\n",
        "# 2b. Aggregate vs per-pair\n",
        "agg_main = agg_only(main_all)\n",
        "if len(agg_main) > 0:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "    agg_results = []\n",
        "    for idx, (name, same_cond, _, metric) in enumerate(methods):\n",
        "        ax = axes[idx]\n",
        "        pp_vals = main_df[main_df['condition'] == same_cond][metric].dropna()\n",
        "        ag_vals = agg_main[agg_main['condition'] == same_cond][metric].dropna()\n",
        "        if 'steer' in same_cond:\n",
        "            pp_vals = winsorize_steering(pp_vals); ag_vals = winsorize_steering(ag_vals)\n",
        "\n",
        "        data = [pp_vals.mean() if len(pp_vals) > 0 else 0,\n",
        "                ag_vals.mean() if len(ag_vals) > 0 else 0]\n",
        "        errors = [pp_vals.sem() if len(pp_vals) > 0 else 0,\n",
        "                  ag_vals.sem() if len(ag_vals) > 0 else 0]\n",
        "\n",
        "        bars = ax.bar([0, 1], [d * 100 for d in data], yerr=[e * 100 for e in errors],\n",
        "                      capsize=5, color=[COLORS[name.lower()], COLORS['aggregate']],\n",
        "                      edgecolor='black', linewidth=1)\n",
        "        ax.set_xticks([0, 1]); ax.set_xticklabels(['Per-Pair', 'Aggregate'])\n",
        "        ax.set_ylabel('Effect (%)'); ax.set_title(name)\n",
        "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "        for bar, val in zip(bars, data):\n",
        "            h = bar.get_height()\n",
        "            ax.annotate(f'{val:.1%}', xy=(bar.get_x() + bar.get_width() / 2, h),\n",
        "                        xytext=(0, 3 if h >= 0 else -15), textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom' if h >= 0 else 'top', fontsize=10, fontweight='bold')\n",
        "        agg_results.append({'Method': name, 'Per-Pair': f\"{data[0]:.1%}\", 'Aggregate': f\"{data[1]:.1%}\",\n",
        "                            'Ratio': f\"{data[1]/data[0]:.2f}\" if abs(data[0]) > 1e-6 else \"N/A\"})\n",
        "\n",
        "    plt.suptitle(f'{MODEL_NAME}: Per-Pair vs Aggregate (Layer {BEST_LAYER}, K=50)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / 'fig2b_aggregate_vs_perpair.png')\n",
        "    plt.savefig(FIG_DIR / 'fig2b_aggregate_vs_perpair.pdf')\n",
        "    plt.close()\n",
        "    with open(TABLE_DIR / 'table1b_aggregate_vs_perpair.tex', 'w') as f:\n",
        "        f.write(pd.DataFrame(agg_results).to_latex(index=False, escape=False))\n",
        "    print(\"  Saved: fig2b_aggregate_vs_perpair\")\n",
        "\n",
        "\n",
        "# --- 3. Dose-response ---\n",
        "\n",
        "print(\"\\n3. Dose-response\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "layer_pp = per_pair(results_df[results_df['layer'] == BEST_LAYER])\n",
        "layer_all = results_df[results_df['layer'] == BEST_LAYER]\n",
        "\n",
        "for idx, (name, same_cond, metric, color) in enumerate([\n",
        "    ('Patching Recovery', 'patch_same', 'recovery_ev', COLORS['patching']),\n",
        "    ('Steering Increase (Winsor.)', 'steer_same', 'increase_ev', COLORS['steering']),\n",
        "    ('Ablation Reduction', 'ablate_same', 'reduction_ev', COLORS['ablation']),\n",
        "]):\n",
        "    ax = axes[idx]\n",
        "    k_values = [5, 10, 20, 50]\n",
        "    means, sems = [], []\n",
        "    for K in k_values:\n",
        "        vals = layer_pp[(layer_pp['K'] == K) & (layer_pp['condition'] == same_cond)][metric].dropna()\n",
        "        if 'steer' in same_cond: vals = winsorize_steering(vals)\n",
        "        means.append(vals.mean()); sems.append(vals.sem())\n",
        "\n",
        "    ax.errorbar(k_values, [m * 100 for m in means], yerr=[s * 100 for s in sems],\n",
        "                marker='o', markersize=8, linewidth=2, capsize=5, color=color, label='Same-Pair')\n",
        "\n",
        "    rand_cond = 'patch_random_a' if 'patch' in same_cond else (\n",
        "        'steer_random' if 'steer' in same_cond else 'ablate_random')\n",
        "    rand_means = []\n",
        "    for K in k_values:\n",
        "        vals = layer_all[(layer_all['K'] == K) & (layer_all['condition'] == rand_cond)][metric].dropna()\n",
        "        if 'steer' in same_cond: vals = winsorize_steering(vals)\n",
        "        rand_means.append(vals.mean() if len(vals) > 0 else 0)\n",
        "    ax.plot(k_values, [m * 100 for m in rand_means], 's--', markersize=6, linewidth=1.5,\n",
        "            color=COLORS['random'], label='Random-A')\n",
        "\n",
        "    ax.set_xlabel('Number of Features (K)'); ax.set_ylabel('Effect (%)')\n",
        "    ax.set_title(name); ax.legend(loc='lower right', fontsize=9)\n",
        "    ax.set_xticks(k_values); ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Dose-Response (Layer {BEST_LAYER})', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig3_dose_response.png')\n",
        "plt.savefig(FIG_DIR / 'fig3_dose_response.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig3_dose_response\")\n",
        "\n",
        "\n",
        "# --- 4. Layer comparison ---\n",
        "\n",
        "print(\"\\n4. Layer comparison\")\n",
        "\n",
        "layer_matrix = np.zeros((len(ANALYSIS_LAYERS), 3))\n",
        "for i, layer_val in enumerate(ANALYSIS_LAYERS):\n",
        "    for j, (cond, metric) in enumerate(zip(method_conds_list, method_metrics_list)):\n",
        "        vals = per_pair(results_df[(results_df['layer'] == layer_val) & (results_df['K'] == 50)])\n",
        "        vals = vals[vals['condition'] == cond][metric].dropna()\n",
        "        if 'steer' in cond: vals = winsorize_steering(vals)\n",
        "        layer_matrix[i, j] = vals.mean() * 100\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 7))\n",
        "sns.heatmap(layer_matrix, annot=True, fmt='.1f', cmap='YlGnBu',\n",
        "            xticklabels=['Patching', 'Steering (Winsor.)', 'Ablation'],\n",
        "            yticklabels=[layer_label_compact(l) for l in ANALYSIS_LAYERS],\n",
        "            ax=ax, linewidths=0.5, cbar_kws={'label': 'Effect (%)'})\n",
        "\n",
        "for i, l in enumerate(ANALYSIS_LAYERS):\n",
        "    if l in IT_LAYERS:\n",
        "        ax.add_patch(plt.Rectangle((0, i), 3, 1, fill=False,\n",
        "                                    edgecolor='orange', linewidth=2.5, linestyle='--'))\n",
        "\n",
        "ax.set_title(f'{MODEL_NAME}: Causal Effect Across 8 Layers (K=50)\\n(* = IT SAE, dashed = IT layer)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig4_layer_comparison.png')\n",
        "plt.savefig(FIG_DIR / 'fig4_layer_comparison.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig4_layer_comparison\")\n",
        "\n",
        "print(f\"\\n{'Layer':<18} {'Patching':>10} {'Steering':>10} {'Ablation':>10}\")\n",
        "print(\"-\" * 52)\n",
        "for i, l in enumerate(ANALYSIS_LAYERS):\n",
        "    tag = \" (IT)\" if l in IT_LAYERS else \"\"\n",
        "    print(f\"L{l} ({LAYER_DEPTH[l]}%){tag:<6} {layer_matrix[i,0]:>9.1f}% {layer_matrix[i,1]:>9.1f}% {layer_matrix[i,2]:>9.1f}%\")\n",
        "\n",
        "\n",
        "# --- 4b. Encoding-causal dissociation ---\n",
        "\n",
        "print(\"\\n4b. Encoding-causal dissociation\")\n",
        "\n",
        "causal_by_layer = {}\n",
        "for l in ANALYSIS_LAYERS:\n",
        "    vals = per_pair(results_df[(results_df['layer'] == l) & (results_df['K'] == 50)])\n",
        "    vals = vals[vals['condition'] == 'patch_same']['recovery_ev'].dropna()\n",
        "    causal_by_layer[l] = vals.mean()\n",
        "\n",
        "encoding_by_layer = {}\n",
        "if feature_stats_df is not None and 'layer' in feature_stats_df.columns:\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        layer_feats = feature_stats_df[feature_stats_df['layer'] == l]\n",
        "        if len(layer_feats) > 0 and 'cohens_d' in layer_feats.columns:\n",
        "            encoding_by_layer[l] = layer_feats['cohens_d'].abs().mean()\n",
        "        elif len(layer_feats) > 0 and 'mean_abs_delta' in layer_feats.columns:\n",
        "            encoding_by_layer[l] = layer_feats['mean_abs_delta'].mean()\n",
        "        else:\n",
        "            encoding_by_layer[l] = len(layer_feats)\n",
        "else:\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        vals = per_pair(results_df[(results_df['layer'] == l) & (results_df['K'] == 50)])\n",
        "        encoding_by_layer[l] = len(vals[vals['condition'] == 'patch_same'])\n",
        "\n",
        "if len(encoding_by_layer) > 0:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "    depths = [LAYER_DEPTH[l] for l in ANALYSIS_LAYERS]\n",
        "    causal_vals = [causal_by_layer[l] * 100 for l in ANALYSIS_LAYERS]\n",
        "    encoding_vals = [encoding_by_layer[l] for l in ANALYSIS_LAYERS]\n",
        "    enc_max = max(encoding_vals) if max(encoding_vals) > 0 else 1\n",
        "    encoding_norm = [v / enc_max * 100 for v in encoding_vals]\n",
        "\n",
        "    # Panel A: dual-axis depth gradient\n",
        "    ax = axes[0]\n",
        "    ax.plot(depths, encoding_norm, 'o-', color='#3498db', linewidth=2.5, markersize=8, label='Encoding Strength', zorder=3)\n",
        "    ax.plot(depths, causal_vals, 's-', color='#e74c3c', linewidth=2.5, markersize=8, label='Causal Influence', zorder=3)\n",
        "    for l in IT_LAYERS:\n",
        "        if l in ANALYSIS_LAYERS:\n",
        "            d = LAYER_DEPTH[l]\n",
        "            ax.axvline(x=d, color='orange', linestyle='--', alpha=0.5, linewidth=1.5)\n",
        "            ax.annotate('IT', (d, 105), ha='center', fontsize=9, color='orange', fontweight='bold')\n",
        "    ax.set_xlabel('Layer Depth (%)'); ax.set_ylabel('Normalized Strength (%)')\n",
        "    ax.set_title('A. Encoding vs Causal Influence\\nAcross Depth', fontweight='bold')\n",
        "    ax.legend(loc='center left', fontsize=9); ax.set_xlim(5, 95)\n",
        "\n",
        "    # Panel B: patching recovery bars\n",
        "    ax = axes[1]\n",
        "    bar_colors = ['orange' if l in IT_LAYERS else '#2ecc71' for l in ANALYSIS_LAYERS]\n",
        "    bars = ax.bar(range(len(ANALYSIS_LAYERS)), causal_vals, color=bar_colors, edgecolor='black', linewidth=0.8)\n",
        "    ax.set_xticks(range(len(ANALYSIS_LAYERS)))\n",
        "    ax.set_xticklabels([f\"L{l}\\n{LAYER_DEPTH[l]}%\" for l in ANALYSIS_LAYERS], fontsize=9)\n",
        "    ax.set_ylabel('Patching Recovery (%)')\n",
        "    ax.set_title('B. Causal Influence by Layer\\n(K=50, Per-Pair)', fontweight='bold')\n",
        "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    for bar, val in zip(bars, causal_vals):\n",
        "        ax.annotate(f'{val:.1f}%', (bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
        "                   ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "    # Panel C: scatter\n",
        "    ax = axes[2]\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        c = 'orange' if l in IT_LAYERS else '#2c3e50'\n",
        "        marker = 'D' if l in IT_LAYERS else 'o'\n",
        "        ax.scatter(encoding_by_layer[l], causal_by_layer[l] * 100,\n",
        "                  s=120, c=c, marker=marker, edgecolors='black', linewidth=1, zorder=3)\n",
        "        ax.annotate(f'L{l}', (encoding_by_layer[l], causal_by_layer[l] * 100),\n",
        "                   textcoords=\"offset points\", xytext=(6, 4), fontsize=9)\n",
        "    if len(ANALYSIS_LAYERS) >= 4:\n",
        "        r, p = pearsonr(list(encoding_by_layer.values()),\n",
        "                        [causal_by_layer[l] * 100 for l in ANALYSIS_LAYERS])\n",
        "        ax.set_title(f'C. Encoding vs Causal (r={r:.2f})', fontweight='bold')\n",
        "    else:\n",
        "        ax.set_title('C. Encoding vs Causal', fontweight='bold')\n",
        "    ax.set_xlabel('Encoding Strength'); ax.set_ylabel('Patching Recovery (%)')\n",
        "\n",
        "    plt.suptitle(f'{MODEL_NAME}: Encoding-Causal Dissociation Across 8 Layers', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / 'fig4b_encoding_causal_dissociation.png')\n",
        "    plt.savefig(FIG_DIR / 'fig4b_encoding_causal_dissociation.pdf')\n",
        "    plt.close()\n",
        "    print(\"  Saved: fig4b_encoding_causal_dissociation\")\n",
        "else:\n",
        "    print(\"  Skipped: insufficient encoding data\")\n",
        "\n",
        "\n",
        "# --- 4c. Depth gradient line plots ---\n",
        "\n",
        "print(\"\\n4c. Depth gradient\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "for idx, (name, cond, metric, color) in enumerate([\n",
        "    ('Patching Recovery', 'patch_same', 'recovery_ev', COLORS['patching']),\n",
        "    ('Steering Increase (Winsor.)', 'steer_same', 'increase_ev', COLORS['steering']),\n",
        "    ('Ablation Reduction', 'ablate_same', 'reduction_ev', COLORS['ablation']),\n",
        "]):\n",
        "    ax = axes[idx]\n",
        "    depths = [LAYER_DEPTH[l] for l in ANALYSIS_LAYERS]\n",
        "\n",
        "    means, sems = [], []\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        vals = per_pair(results_df[(results_df['layer'] == l) & (results_df['K'] == 50)])\n",
        "        vals = vals[vals['condition'] == cond][metric].dropna()\n",
        "        if 'steer' in cond: vals = winsorize_steering(vals)\n",
        "        means.append(vals.mean() * 100); sems.append(vals.sem() * 100)\n",
        "\n",
        "    ax.errorbar(depths, means, yerr=sems, marker='o', markersize=8,\n",
        "                linewidth=2, capsize=4, color=color, label='Same-Pair', zorder=3)\n",
        "\n",
        "    rand_cond = 'patch_random_a' if 'patch' in cond else (\n",
        "        'steer_random' if 'steer' in cond else 'ablate_random')\n",
        "    rand_means = []\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        vals = results_df[(results_df['layer'] == l) & (results_df['K'] == 50) &\n",
        "                          (results_df['condition'] == rand_cond)][metric].dropna()\n",
        "        if 'steer' in cond: vals = winsorize_steering(vals)\n",
        "        rand_means.append(vals.mean() * 100 if len(vals) > 0 else 0)\n",
        "\n",
        "    ax.plot(depths, rand_means, 's--', markersize=6, linewidth=1.5,\n",
        "            color=COLORS['random'], label='Random-A', alpha=0.7)\n",
        "\n",
        "    for l in IT_LAYERS:\n",
        "        if l in ANALYSIS_LAYERS:\n",
        "            ax.axvline(x=LAYER_DEPTH[l], color='orange', linestyle=':', alpha=0.5, linewidth=1.5)\n",
        "\n",
        "    ax.set_xlabel('Layer Depth (%)'); ax.set_ylabel('Effect (%)')\n",
        "    ax.set_title(name); ax.legend(fontsize=9, loc='upper left' if 'Patch' in name else 'best')\n",
        "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5); ax.set_xlim(5, 95)\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Causal Effects Across Depth (K=50)\\nOrange dotted = IT SAE layer',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig4c_depth_gradient.png')\n",
        "plt.savefig(FIG_DIR / 'fig4c_depth_gradient.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig4c_depth_gradient\")\n",
        "\n",
        "print(\"\\nSections 1-4c done.\")"
      ],
      "metadata": {
        "id": "nO3tFLiUxL1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Causal validation analysis — sections 5-10\n",
        "\n",
        "# --- 5. Demographic breakdown ---\n",
        "\n",
        "print(\"5. Demographic breakdown\")\n",
        "\n",
        "main_df_best = per_pair(results_df[(results_df['layer'] == BEST_LAYER) & (results_df['K'] == 50)])\n",
        "\n",
        "demo_results = []\n",
        "for demo in ALL_DEMOS:\n",
        "    demo_pp = main_df_best[main_df_best['demographic'] == demo]\n",
        "    patch = demo_pp[demo_pp['condition'] == 'patch_same']['recovery_ev'].dropna()\n",
        "    steer = winsorize_steering(demo_pp[demo_pp['condition'] == 'steer_same']['increase_ev'].dropna())\n",
        "    ablate = demo_pp[demo_pp['condition'] == 'ablate_same']['reduction_ev'].dropna()\n",
        "    demo_results.append({\n",
        "        'Demographic': demo.capitalize(), 'N Pairs': len(patch),\n",
        "        'Patching (mean/med)': f\"{patch.mean():.1%} / {patch.median():.1%}\" if len(patch) > 0 else \"N/A\",\n",
        "        'Steering (mean/med)': f\"{steer.mean():.1%} / {steer.median():.1%}\" if len(steer) > 0 else \"N/A\",\n",
        "        'Ablation (mean/med)': f\"{ablate.mean():.1%} / {ablate.median():.1%}\" if len(ablate) > 0 else \"N/A\",\n",
        "    })\n",
        "\n",
        "demo_results_df = pd.DataFrame(demo_results)\n",
        "print(demo_results_df.to_string(index=False))\n",
        "with open(TABLE_DIR / 'table2_demographic_breakdown.tex', 'w') as f:\n",
        "    f.write(demo_results_df.to_latex(index=False, escape=False))\n",
        "\n",
        "# Demographics heatmap\n",
        "demo_matrix = np.zeros((len(ALL_DEMOS), 3))\n",
        "for i, demo in enumerate(ALL_DEMOS):\n",
        "    for j, (cond, metric) in enumerate(zip(method_conds_list, method_metrics_list)):\n",
        "        vals = main_df_best[(main_df_best['demographic'] == demo) &\n",
        "                            (main_df_best['condition'] == cond)][metric].dropna()\n",
        "        if 'steer' in cond: vals = winsorize_steering(vals)\n",
        "        demo_matrix[i, j] = vals.mean() * 100 if len(vals) > 0 else 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "sns.heatmap(demo_matrix, annot=True, fmt='.1f', cmap='YlGnBu',\n",
        "            xticklabels=['Patching', 'Steering (Winsor.)', 'Ablation'],\n",
        "            yticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "            ax=ax, linewidths=0.5, cbar_kws={'label': 'Effect (%)'})\n",
        "ax.set_title(f'{MODEL_NAME}: Causal Effects by Demographic (Layer {BEST_LAYER}, K=50)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig5_demographic_breakdown.png')\n",
        "plt.savefig(FIG_DIR / 'fig5_demographic_breakdown.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig5_demographic_breakdown\")\n",
        "\n",
        "# 5b. Steering direction\n",
        "if has_direction:\n",
        "    print(\"\\n5b. Steering direction\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    ax = axes[0]\n",
        "    k_values = [5, 10, 20, 50]\n",
        "    fwd_pcts = []\n",
        "    for K_val in k_values:\n",
        "        dir_vals = per_pair(results_df[\n",
        "            (results_df['layer'] == BEST_LAYER) & (results_df['K'] == K_val) &\n",
        "            (results_df['condition'] == 'steer_same')\n",
        "        ])['direction_preserved'].dropna()\n",
        "        fwd_pcts.append(dir_vals.mean() * 100 if len(dir_vals) > 0 else 0)\n",
        "    ax.bar([str(k) for k in k_values], fwd_pcts, color=COLORS['steering'], edgecolor='black')\n",
        "    ax.set_xlabel('Number of Features (K)'); ax.set_ylabel('Direction Preserved (%)')\n",
        "    ax.set_title(f'Forward Steering (×2) — Layer {BEST_LAYER}'); ax.set_ylim(0, 105)\n",
        "    for i, p in enumerate(fwd_pcts):\n",
        "        ax.annotate(f'{p:.1f}%', (i, p), ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    ax = axes[1]\n",
        "    fwd_by_layer, rev_by_layer = [], []\n",
        "    for layer_val in ANALYSIS_LAYERS:\n",
        "        fwd = per_pair(results_df[\n",
        "            (results_df['layer'] == layer_val) & (results_df['K'] == 50) &\n",
        "            (results_df['condition'] == 'steer_same')\n",
        "        ])['direction_preserved'].dropna()\n",
        "        rev = per_pair(results_df[\n",
        "            (results_df['layer'] == layer_val) & (results_df['K'] == 50) &\n",
        "            (results_df['condition'] == 'steer_reverse')\n",
        "        ])['direction_preserved'].dropna()\n",
        "        fwd_by_layer.append(fwd.mean() * 100 if len(fwd) > 0 else 0)\n",
        "        rev_by_layer.append(rev.mean() * 100 if len(rev) > 0 else 0)\n",
        "\n",
        "    x = np.arange(len(ANALYSIS_LAYERS)); w = 0.35\n",
        "    ax.bar(x - w/2, fwd_by_layer, w, label='Forward (×2)', color=COLORS['steering'], edgecolor='black')\n",
        "    ax.bar(x + w/2, rev_by_layer, w, label='Reverse (×-2)', color='#c0392b', edgecolor='black')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels([f'L{l}\\n{LAYER_DEPTH[l]}%' for l in ANALYSIS_LAYERS], fontsize=8)\n",
        "    ax.set_ylabel('Direction Preserved (%)'); ax.set_title('Forward vs Reverse by Layer')\n",
        "    ax.set_ylim(0, 105)\n",
        "    ax.axhline(y=50, color='gray', linestyle=':', linewidth=1, alpha=0.7, label='Chance')\n",
        "    ax.legend(fontsize=8, loc='lower right')\n",
        "    for i, (f, r) in enumerate(zip(fwd_by_layer, rev_by_layer)):\n",
        "        ax.annotate(f'{f:.0f}', (i - w/2, f), ha='center', va='bottom', fontsize=7)\n",
        "        ax.annotate(f'{r:.0f}', (i + w/2, r), ha='center', va='bottom', fontsize=7)\n",
        "\n",
        "    plt.suptitle(f'{MODEL_NAME}: Steering Direction Control (8 Layers)', fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / 'fig5b_steering_direction.png')\n",
        "    plt.savefig(FIG_DIR / 'fig5b_steering_direction.pdf')\n",
        "    plt.close()\n",
        "    print(\"  Saved: fig5b_steering_direction\")\n",
        "\n",
        "# 5c. Behavioral effect vs causal recovery scatter\n",
        "print(\"\\n5c. Effect vs recovery scatter\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "for ax_idx, (name, cond, metric, color) in enumerate([\n",
        "    ('Patching Recovery', 'patch_same', 'recovery_ev', COLORS['patching']),\n",
        "    ('Steering Increase (Winsor.)', 'steer_same', 'increase_ev', COLORS['steering']),\n",
        "    ('Ablation Reduction', 'ablate_same', 'reduction_ev', COLORS['ablation']),\n",
        "]):\n",
        "    ax = axes[ax_idx]\n",
        "    cond_df = main_df_best[main_df_best['condition'] == cond][\n",
        "        ['pair_key', 'demographic', 'effect_ev_base', metric]].dropna().copy()\n",
        "    if 'steer' in cond: cond_df[metric] = winsorize_steering(cond_df[metric])\n",
        "\n",
        "    if len(cond_df) < 5:\n",
        "        ax.set_title(f'{name}\\n(insufficient data)'); continue\n",
        "\n",
        "    for demo in ALL_DEMOS:\n",
        "        demo_sub = cond_df[cond_df['demographic'] == demo]\n",
        "        ax.scatter(demo_sub['effect_ev_base'].abs(), demo_sub[metric],\n",
        "                  color=COLORS[demo], alpha=0.3, s=15, label=demo.capitalize())\n",
        "\n",
        "    r, p = pearsonr(cond_df['effect_ev_base'].abs(), cond_df[metric])\n",
        "    z = np.polyfit(cond_df['effect_ev_base'].abs(), cond_df[metric], 1)\n",
        "    x_line = np.linspace(0, cond_df['effect_ev_base'].abs().max(), 100)\n",
        "    ax.plot(x_line, np.polyval(z, x_line), 'k--', alpha=0.7, linewidth=1.5)\n",
        "    ax.set_xlabel('|Baseline Effect|'); ax.set_ylabel(name)\n",
        "    ax.set_title(f'{name}\\nr={r:.2f} ({\"p<0.001\" if p < 0.001 else f\"p={p:.3f}\"})', fontweight='bold')\n",
        "    ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
        "    if ax_idx == 0: ax.legend(fontsize=7, loc='upper left')\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Larger Baseline Effect → Stronger Recovery?', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig5c_effect_vs_recovery_scatter.png')\n",
        "plt.savefig(FIG_DIR / 'fig5c_effect_vs_recovery_scatter.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig5c_effect_vs_recovery_scatter\")\n",
        "\n",
        "\n",
        "# --- 6. Feature analysis ---\n",
        "\n",
        "print(\"\\n6. Feature analysis\")\n",
        "\n",
        "all_features_by_demo = defaultdict(list)\n",
        "for _, row in top_features_df.iterrows():\n",
        "    feats = safe_parse_list(row['top_5_features'])\n",
        "    deltas = safe_parse_list(row['top_5_deltas'])\n",
        "    for f, d in zip(feats, deltas):\n",
        "        all_features_by_demo[(row['layer'], row['demographic'])].append((f, d))\n",
        "\n",
        "top_10_per_demo = {}\n",
        "for demo in ALL_DEMOS:\n",
        "    key = (BEST_LAYER, demo)\n",
        "    if key in all_features_by_demo:\n",
        "        features = [f for f, d in all_features_by_demo[key]]\n",
        "        counts = Counter(features)\n",
        "        top_10_per_demo[demo] = set([f for f, c in counts.most_common(10)])\n",
        "        print(f\"  {demo}: top 5 = {[f'{f}({c}x)' for f, c in counts.most_common(5)]}\")\n",
        "\n",
        "# Overlap heatmap\n",
        "overlap_matrix = np.zeros((len(ALL_DEMOS), len(ALL_DEMOS)))\n",
        "for i, d1 in enumerate(ALL_DEMOS):\n",
        "    for j, d2 in enumerate(ALL_DEMOS):\n",
        "        f1 = top_10_per_demo.get(d1, set())\n",
        "        f2 = top_10_per_demo.get(d2, set())\n",
        "        if len(f1 | f2) > 0:\n",
        "            overlap_matrix[i, j] = len(f1 & f2) / len(f1 | f2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(overlap_matrix, annot=True, fmt='.2f', cmap='YlOrRd',\n",
        "            xticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "            yticklabels=[d.capitalize() for d in ALL_DEMOS], ax=ax, vmin=0, vmax=1)\n",
        "ax.set_title(f'{MODEL_NAME}: Feature Overlap (Jaccard, Top-10, Layer {BEST_LAYER})')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'fig6_feature_overlap.png')\n",
        "plt.savefig(FIG_DIR / 'fig6_feature_overlap.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig6_feature_overlap\")\n",
        "\n",
        "\n",
        "# --- 7. Key features table ---\n",
        "\n",
        "print(\"\\n7. Key features for interpretation\")\n",
        "\n",
        "all_best_features = []\n",
        "for _, row in top_features_df[top_features_df['layer'] == BEST_LAYER].iterrows():\n",
        "    feats = safe_parse_list(row['top_5_features'])\n",
        "    deltas = safe_parse_list(row['top_5_deltas'])\n",
        "    for f, d in zip(feats, deltas):\n",
        "        all_best_features.append({\n",
        "            'feature': f, 'delta': d, 'demographic': row['demographic'],\n",
        "            'domain': row['domain'], 'effect': row['effect_ev']\n",
        "        })\n",
        "\n",
        "features_agg_df = pd.DataFrame(all_best_features)\n",
        "feature_summary_table = features_agg_df.groupby('feature').agg({\n",
        "    'delta': ['mean', 'std', 'count'],\n",
        "    'demographic': lambda x: ', '.join(sorted(set(x))),\n",
        "}).reset_index()\n",
        "feature_summary_table.columns = ['Feature', 'Mean Delta', 'Std Delta', 'Count', 'Demographics']\n",
        "feature_summary_table = feature_summary_table.sort_values('Count', ascending=False)\n",
        "\n",
        "print(feature_summary_table.head(15).to_string(index=False))\n",
        "feature_summary_table.head(50).to_csv(DATA_DIR / 'key_features_for_neuronpedia.csv', index=False)\n",
        "\n",
        "\n",
        "# --- 8. Summary statistics ---\n",
        "\n",
        "print(\"\\n8. Summary statistics\")\n",
        "\n",
        "unique_pairs = len(pairs_df[pairs_df['layer'] == BEST_LAYER])\n",
        "fdr_info = summary.get('fdr', summary.get('fdr_correction', {}))\n",
        "\n",
        "steer_same_winsor = winsorize_steering(\n",
        "    main_df_best[main_df_best['condition'] == 'steer_same']['increase_ev'].dropna()\n",
        ")\n",
        "\n",
        "summary_stats = {\n",
        "    'Model': MODEL_NAME,\n",
        "    'Best Layer': f\"{BEST_LAYER} ({LAYER_DEPTH[BEST_LAYER]}% depth)\",\n",
        "    'Dataset': {\n",
        "        'Unique pairs (best layer)': unique_pairs,\n",
        "        'Demographics': 5, 'Domains': 5,\n",
        "        'Layers analyzed': len(ANALYSIS_LAYERS),\n",
        "        'IT SAE layers': str(list(IT_LAYERS)),\n",
        "    },\n",
        "    f'Main Results (Layer {BEST_LAYER}, K=50, paired)': {\n",
        "        'Patching recovery': f\"{main_df_best[main_df_best['condition'] == 'patch_same']['recovery_ev'].mean():.1%}\",\n",
        "        'Steering increase (Winsorized)': f\"{steer_same_winsor.mean():.1%}\",\n",
        "        'Ablation reduction': f\"{main_df_best[main_df_best['condition'] == 'ablate_same']['reduction_ev'].mean():.1%}\",\n",
        "    },\n",
        "    'Effect Sizes (paired d_z)': {\n",
        "        'Patching': f\"{computed_cohens_d.get('Patching', 0):.2f}\",\n",
        "        'Steering': f\"{computed_cohens_d.get('Steering', 0):.2f}\",\n",
        "        'Ablation': f\"{computed_cohens_d.get('Ablation', 0):.2f}\",\n",
        "    },\n",
        "    'Controls': {\n",
        "        'Random-A': f\"{main_all[main_all['condition'] == 'patch_random_a']['recovery_ev'].mean():.1%}\",\n",
        "        'Random-B': f\"{main_all[main_all['condition'] == 'patch_random_b']['recovery_ev'].mean():.1%}\",\n",
        "        'Cross-pair': f\"{main_all[main_all['condition'] == 'patch_cross']['recovery_ev'].mean():.1%}\",\n",
        "    },\n",
        "    'Layer Gradient (patching, K=50)': {\n",
        "        layer_label(l): f\"{causal_by_layer[l]:.1%}\" for l in ANALYSIS_LAYERS\n",
        "    },\n",
        "    'FDR': {\n",
        "        'Tests': fdr_info.get('n_tests', 'N/A'),\n",
        "        'Significant': f\"{fdr_info.get('n_significant', 'N/A')}/{fdr_info.get('n_tests', 'N/A')}\",\n",
        "    },\n",
        "}\n",
        "\n",
        "if has_direction:\n",
        "    dir_fwd = main_df_best[main_df_best['condition'] == 'steer_same']['direction_preserved'].dropna().mean() * 100\n",
        "    dir_rev = main_df_best[main_df_best['condition'] == 'steer_reverse']['direction_preserved'].dropna().mean() * 100\n",
        "    summary_stats['Steering Direction'] = {\n",
        "        'Forward preserved': f\"{dir_fwd:.1f}%\", 'Reverse preserved': f\"{dir_rev:.1f}%\",\n",
        "    }\n",
        "\n",
        "for section, stats in summary_stats.items():\n",
        "    if isinstance(stats, dict):\n",
        "        print(f\"\\n{section}:\")\n",
        "        for k, v in stats.items(): print(f\"  {k}: {v}\")\n",
        "    else:\n",
        "        print(f\"\\n{section}: {stats}\")\n",
        "\n",
        "with open(SUMMARY_DIR / 'summary_statistics.json', 'w') as f:\n",
        "    json.dump(summary_stats, f, indent=2, default=str)\n",
        "\n",
        "\n",
        "# --- 9. Combined paper figure ---\n",
        "\n",
        "print(\"\\n9. Combined figure\")\n",
        "\n",
        "fig = plt.figure(figsize=(18, 14))\n",
        "gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.35)\n",
        "\n",
        "# A: Main results\n",
        "ax_a = fig.add_subplot(gs[0, :])\n",
        "methods_data = []\n",
        "for name, same_cond, rand_cond, metric in methods:\n",
        "    same = main_df_best[main_df_best['condition'] == same_cond][metric].dropna()\n",
        "    rand = main_all[main_all['condition'] == rand_cond][metric].dropna()\n",
        "    if 'steer' in same_cond:\n",
        "        same = winsorize_steering(same); rand = winsorize_steering(rand)\n",
        "    methods_data.append((name, same.mean(), same.sem(), rand.mean(), computed_cohens_d.get(name, 0)))\n",
        "\n",
        "x_pos = np.arange(3); w = 0.35\n",
        "bars1 = ax_a.bar(x_pos - w/2, [d[1]*100 for d in methods_data], w,\n",
        "                  yerr=[d[2]*100 for d in methods_data], capsize=5,\n",
        "                  label='Same-Pair',\n",
        "                  color=[COLORS['patching'], COLORS['steering'], COLORS['ablation']],\n",
        "                  edgecolor='black', linewidth=1)\n",
        "bars2 = ax_a.bar(x_pos + w/2, [d[3]*100 for d in methods_data], w,\n",
        "                  label='Random-A', color=COLORS['random'], edgecolor='black', linewidth=1)\n",
        "ax_a.set_xticks(x_pos)\n",
        "ax_a.set_xticklabels([f\"{d[0]}\\nd_z={d[4]:.2f}\" for d in methods_data])\n",
        "ax_a.set_ylabel('Effect (%)'); ax_a.legend()\n",
        "ax_a.set_title(f'A. Same-Pair vs Random (Layer {BEST_LAYER}, K=50, Paired)', fontweight='bold')\n",
        "ax_a.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "for i in range(3):\n",
        "    ax_a.annotate('***', xy=(i - w/2, methods_data[i][1]*100 + methods_data[i][2]*100 + 3),\n",
        "                  ha='center', fontsize=12)\n",
        "\n",
        "# B: Dose-response\n",
        "ax_b = fig.add_subplot(gs[1, 0])\n",
        "for name, same_cond, metric, color in [\n",
        "    ('Patching', 'patch_same', 'recovery_ev', COLORS['patching']),\n",
        "    ('Steering (W)', 'steer_same', 'increase_ev', COLORS['steering']),\n",
        "    ('Ablation', 'ablate_same', 'reduction_ev', COLORS['ablation']),\n",
        "]:\n",
        "    means = []\n",
        "    for K in [5, 10, 20, 50]:\n",
        "        vals = per_pair(results_df[(results_df['layer'] == BEST_LAYER) & (results_df['K'] == K)])\n",
        "        vals = vals[vals['condition'] == same_cond][metric].dropna()\n",
        "        if 'steer' in same_cond: vals = winsorize_steering(vals)\n",
        "        means.append(vals.mean())\n",
        "    ax_b.plot([5, 10, 20, 50], [m*100 for m in means], marker='o', label=name, color=color, linewidth=2)\n",
        "ax_b.set_xlabel('K'); ax_b.set_ylabel('Effect (%)')\n",
        "ax_b.set_title('B. Dose-Response', fontweight='bold')\n",
        "ax_b.legend(fontsize=7); ax_b.set_xticks([5, 10, 20, 50])\n",
        "\n",
        "# C: Layer heatmap\n",
        "ax_c = fig.add_subplot(gs[1, 1])\n",
        "sns.heatmap(layer_matrix, annot=True, fmt='.1f', cmap='YlGnBu',\n",
        "            xticklabels=['Patch', 'Steer(W)', 'Ablate'],\n",
        "            yticklabels=[layer_label_compact(l) for l in ANALYSIS_LAYERS],\n",
        "            ax=ax_c, linewidths=0.5, cbar=False, annot_kws={'fontsize': 8})\n",
        "ax_c.set_title('C. Layer Comparison (%)', fontweight='bold')\n",
        "ax_c.tick_params(axis='y', labelsize=8)\n",
        "for i, l in enumerate(ANALYSIS_LAYERS):\n",
        "    if l in IT_LAYERS:\n",
        "        ax_c.add_patch(plt.Rectangle((0, i), 3, 1, fill=False,\n",
        "                                      edgecolor='orange', linewidth=2, linestyle='--'))\n",
        "\n",
        "# D: Depth gradient\n",
        "ax_d = fig.add_subplot(gs[1, 2])\n",
        "depths = [LAYER_DEPTH[l] for l in ANALYSIS_LAYERS]\n",
        "patch_by_depth = [causal_by_layer[l] * 100 for l in ANALYSIS_LAYERS]\n",
        "ax_d.plot(depths, patch_by_depth, 'o-', color=COLORS['patching'], linewidth=2.5, markersize=8)\n",
        "for i, l in enumerate(ANALYSIS_LAYERS):\n",
        "    c = 'orange' if l in IT_LAYERS else 'black'\n",
        "    ax_d.annotate(f'L{l}', (depths[i], patch_by_depth[i]),\n",
        "                 textcoords=\"offset points\", xytext=(4, 5), fontsize=7, color=c)\n",
        "ax_d.set_xlabel('Depth (%)'); ax_d.set_ylabel('Patching Recovery (%)')\n",
        "ax_d.set_title('D. Causal Influence\\nAcross Depth', fontweight='bold')\n",
        "ax_d.set_xlim(5, 95); ax_d.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# E: Control hierarchy\n",
        "ax_e = fig.add_subplot(gs[1, 3])\n",
        "same_patch = main_df_best[main_df_best['condition'] == 'patch_same']['recovery_ev'].dropna().mean()\n",
        "rand_a_patch = main_all[main_all['condition'] == 'patch_random_a']['recovery_ev'].dropna().mean()\n",
        "rand_b_patch = main_all[main_all['condition'] == 'patch_random_b']['recovery_ev'].dropna().mean()\n",
        "cross_patch = main_all[main_all['condition'] == 'patch_cross']['recovery_ev'].dropna().mean()\n",
        "control_data = [('Same', same_patch, COLORS['patching']), ('Rand-A', rand_a_patch, COLORS['random_a']),\n",
        "                ('Cross', cross_patch, COLORS['cross']), ('Rand-B', rand_b_patch, COLORS['random_b'])]\n",
        "ax_e.bar(range(4), [d[1]*100 for d in control_data],\n",
        "         color=[d[2] for d in control_data], edgecolor='black')\n",
        "ax_e.set_xticks(range(4)); ax_e.set_xticklabels([d[0] for d in control_data], fontsize=9)\n",
        "ax_e.set_ylabel('Recovery (%)'); ax_e.set_title('E. Control Hierarchy', fontweight='bold')\n",
        "ax_e.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "for i, (lbl, val, _) in enumerate(control_data):\n",
        "    ax_e.annotate(f'{val:.1%}', (i, val*100), ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "# F: Demographics heatmap\n",
        "ax_f = fig.add_subplot(gs[2, :2])\n",
        "sns.heatmap(demo_matrix, annot=True, fmt='.1f', cmap='YlGnBu',\n",
        "            xticklabels=['Patching', 'Steering (W)', 'Ablation'],\n",
        "            yticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "            ax=ax_f, linewidths=0.5, cbar_kws={'label': 'Effect (%)', 'shrink': 0.6})\n",
        "ax_f.set_title('F. Results by Demographic', fontweight='bold')\n",
        "\n",
        "# G: Feature overlap\n",
        "ax_g = fig.add_subplot(gs[2, 2])\n",
        "sns.heatmap(overlap_matrix, annot=True, fmt='.2f', cmap='YlOrRd',\n",
        "            xticklabels=[d[:3].capitalize() for d in ALL_DEMOS],\n",
        "            yticklabels=[d[:3].capitalize() for d in ALL_DEMOS],\n",
        "            ax=ax_g, vmin=0, vmax=0.5, cbar_kws={'shrink': 0.8})\n",
        "ax_g.set_title('G. Feature Overlap\\n(Jaccard)', fontweight='bold')\n",
        "\n",
        "# H: Direction by layer\n",
        "if has_direction:\n",
        "    ax_h = fig.add_subplot(gs[2, 3])\n",
        "    fwd_vals = []\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        fwd = per_pair(results_df[\n",
        "            (results_df['layer'] == l) & (results_df['K'] == 50) &\n",
        "            (results_df['condition'] == 'steer_same')\n",
        "        ])['direction_preserved'].dropna()\n",
        "        fwd_vals.append(fwd.mean() * 100 if len(fwd) > 0 else 0)\n",
        "    bar_colors_h = ['orange' if l in IT_LAYERS else COLORS['steering'] for l in ANALYSIS_LAYERS]\n",
        "    ax_h.bar(range(len(ANALYSIS_LAYERS)), fwd_vals, color=bar_colors_h, edgecolor='black', linewidth=0.8)\n",
        "    ax_h.set_xticks(range(len(ANALYSIS_LAYERS)))\n",
        "    ax_h.set_xticklabels([f'L{l}' for l in ANALYSIS_LAYERS], fontsize=7)\n",
        "    ax_h.set_ylabel('Direction (%)'); ax_h.set_title('H. Steering Direction\\nby Layer', fontweight='bold')\n",
        "    ax_h.axhline(y=50, color='gray', linestyle=':', linewidth=1, alpha=0.7); ax_h.set_ylim(0, 105)\n",
        "\n",
        "plt.savefig(FIG_DIR / 'fig_combined_main.png')\n",
        "plt.savefig(FIG_DIR / 'fig_combined_main.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_combined_main\")\n",
        "\n",
        "\n",
        "# --- 10. Bundle ---\n",
        "\n",
        "print(\"\\n10. Bundling\")\n",
        "\n",
        "raw_files = [\n",
        "    (VALIDATION_DIR / 'validation_results.csv', DATA_DIR / 'validation_results.csv'),\n",
        "    (VALIDATION_DIR / 'top_features.csv', DATA_DIR / 'top_features.csv'),\n",
        "    (VALIDATION_DIR / 'validation_summary.json', DATA_DIR / 'validation_summary_old.json'),\n",
        "    (EXTRACTION_DIR / 'behavioral_effects.csv', DATA_DIR / 'behavioral_effects.csv'),\n",
        "    (EXTRACTION_DIR / 'selected_features.json', DATA_DIR / 'selected_features.json'),\n",
        "    (EXTRACTION_DIR / 'feature_stats.csv', DATA_DIR / 'feature_stats.csv'),\n",
        "    (EXTRACTION_DIR / 'feature_overlap.json', DATA_DIR / 'feature_overlap.json'),\n",
        "]\n",
        "if patched_summary_path.exists():\n",
        "    raw_files.append((patched_summary_path, DATA_DIR / 'validation_summary_patched.json'))\n",
        "if exclusion_path.exists():\n",
        "    raw_files.append((exclusion_path, DATA_DIR / 'exclusion_log.csv'))\n",
        "if paired_comp_path.exists():\n",
        "    raw_files.append((paired_comp_path, DATA_DIR / 'paired_comparisons.csv'))\n",
        "\n",
        "for src, dst in raw_files:\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "readme = f\"\"\"# Causal Validation Results — {MODEL_NAME} (8-Layer)\n",
        "# Generated: {pd.Timestamp.now().isoformat()}\n",
        "\n",
        "## Model\n",
        "- {MODEL_NAME}, {N_LAYERS_TOTAL} layers total\n",
        "- Analysis: {ANALYSIS_LAYERS}\n",
        "- Depths: {LAYER_DEPTH}\n",
        "- IT SAE: {list(IT_LAYERS)}, all others PT\n",
        "- Best: L{BEST_LAYER} ({LAYER_DEPTH[BEST_LAYER]}%)\n",
        "\n",
        "## Methodology\n",
        "- Paired tests (ttest_rel, d_z)\n",
        "- Steering Winsorized ±200%\n",
        "- Random-A (active+delta) vs Random-B (shuffled)\n",
        "- Cross-pair from different demo×domain\n",
        "- FDR correction (BH)\n",
        "- Scale-normalized threshold (0.3 × range/10)\n",
        "\"\"\"\n",
        "\n",
        "with open(BUNDLE_DIR / 'README.md', 'w') as f:\n",
        "    f.write(readme)\n",
        "\n",
        "shutil.make_archive(str(BUNDLE_DIR), 'zip', root_dir=BUNDLE_DIR.parent, base_dir=BUNDLE_DIR.name)\n",
        "\n",
        "n_figs = len(list(FIG_DIR.glob('*.png')))\n",
        "n_tables = len(list(TABLE_DIR.glob('*.tex')))\n",
        "n_data = len(list(DATA_DIR.glob('*')))\n",
        "print(f\"  {n_figs} figures, {n_tables} tables, {n_data} data files\")\n",
        "print(f\"  Download: {BUNDLE_DIR.name}.zip\")\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "3BPoHbAqxMhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature interpretation — setup, top features, encoding matrix\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "import re\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "from scipy import stats\n",
        "from scipy.stats import t as t_dist\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'DejaVu Sans', 'font.size': 11,\n",
        "    'axes.labelsize': 12, 'axes.titlesize': 13,\n",
        "    'figure.dpi': 150, 'savefig.dpi': 300, 'savefig.bbox': 'tight',\n",
        "})\n",
        "\n",
        "MODEL_NAME = \"Gemma 2 9B IT\"\n",
        "LAYER = 36\n",
        "N_LAYERS_TOTAL = 42\n",
        "\n",
        "ALL_DEMOS = ['income', 'age', 'gender', 'education', 'vote']\n",
        "N_TOP_FEATURES = 10\n",
        "N_PAIRS_ENCODING = 30\n",
        "N_PAIRS_LOCATION = 20\n",
        "ALPHA = 0.05\n",
        "\n",
        "DEMO_COLORS = {\n",
        "    'income': '#1abc9c', 'age': '#9b59b6', 'gender': '#e91e63',\n",
        "    'education': '#3498db', 'vote': '#f44336',\n",
        "}\n",
        "\n",
        "# --- Path detection ---\n",
        "\n",
        "BASE_DIR = Path(\"/content\")\n",
        "\n",
        "def find_file(filename, search_dirs=None):\n",
        "    if search_dirs is None: search_dirs = [BASE_DIR]\n",
        "    for d in search_dirs:\n",
        "        if d is None: continue\n",
        "        direct = d / filename\n",
        "        if direct.exists(): return direct\n",
        "    for match in BASE_DIR.rglob(filename):\n",
        "        return match\n",
        "    return None\n",
        "\n",
        "def find_dir(dirname):\n",
        "    direct = BASE_DIR / dirname\n",
        "    if direct.exists() and direct.is_dir(): return direct\n",
        "    for d in BASE_DIR.iterdir():\n",
        "        if d.is_dir() and d.name.startswith(dirname[:20]): return d\n",
        "    return None\n",
        "\n",
        "VALIDATION_DIR = find_dir(\"causal_validation_final\")\n",
        "if VALIDATION_DIR is None:\n",
        "    nested = find_dir(\"outputs_gemma_replication\")\n",
        "    if nested and (nested / \"causal_validation_final\").exists():\n",
        "        VALIDATION_DIR = nested / \"causal_validation_final\"\n",
        "\n",
        "EXTRACTION_DIR = None\n",
        "for d in BASE_DIR.iterdir():\n",
        "    if d.is_dir() and d.name.startswith(\"feature-extraction-results-ge\"):\n",
        "        EXTRACTION_DIR = d; break\n",
        "if EXTRACTION_DIR is None:\n",
        "    EXTRACTION_DIR = find_dir(\"feature_extraction\")\n",
        "\n",
        "BUNDLE = Path(\"/content/feature-interpretation-results-gemma\")\n",
        "BUNDLE.mkdir(exist_ok=True)\n",
        "FIGS   = BUNDLE / \"figures\";   FIGS.mkdir(exist_ok=True)\n",
        "TABLES = BUNDLE / \"tables\";    TABLES.mkdir(exist_ok=True)\n",
        "DATA   = BUNDLE / \"data\";      DATA.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"VALIDATION_DIR: {VALIDATION_DIR}\")\n",
        "print(f\"EXTRACTION_DIR: {EXTRACTION_DIR}\")\n",
        "\n",
        "if hasattr(model, 'tokenizer'):\n",
        "    tokenizer = model.tokenizer\n",
        "\n",
        "# --- Load data ---\n",
        "\n",
        "prompts_path = find_file(\"prompts_validation.parquet\",\n",
        "                          [EXTRACTION_DIR, BASE_DIR] if EXTRACTION_DIR else [BASE_DIR])\n",
        "if prompts_path is None:\n",
        "    raise FileNotFoundError(\"Cannot find prompts_validation.parquet\")\n",
        "prompts_df = pd.read_parquet(prompts_path)\n",
        "if 'vocab_idx' in prompts_df.columns:\n",
        "    prompts_df = prompts_df[prompts_df['vocab_idx'] == 0].reset_index(drop=True)\n",
        "print(f\"Prompts: {len(prompts_df)}\")\n",
        "\n",
        "tf_path = find_file(\"top_features.csv\", [VALIDATION_DIR] if VALIDATION_DIR else [])\n",
        "if tf_path is None:\n",
        "    raise FileNotFoundError(\"Cannot find top_features.csv\")\n",
        "top_features_df = pd.read_csv(tf_path)\n",
        "print(f\"Top features: {len(top_features_df)} rows\")\n",
        "\n",
        "sae = sae_manager.load_sae(LAYER)\n",
        "hook_name = f\"blocks.{LAYER}.hook_resid_post\"\n",
        "\n",
        "# --- Identify top features ---\n",
        "\n",
        "def safe_parse_list(x):\n",
        "    if isinstance(x, list): return x\n",
        "    if isinstance(x, str):\n",
        "        try: return ast.literal_eval(x)\n",
        "        except: return []\n",
        "    return []\n",
        "\n",
        "feature_stats = defaultdict(lambda: {'count': 0, 'deltas': [], 'demographics': []})\n",
        "layer_df = top_features_df[top_features_df['layer'] == LAYER]\n",
        "if 'feature_method' in layer_df.columns:\n",
        "    layer_df = layer_df[layer_df['feature_method'] == 'per_pair']\n",
        "\n",
        "for _, row in layer_df.iterrows():\n",
        "    feats = safe_parse_list(row['top_5_features'])\n",
        "    deltas = safe_parse_list(row['top_5_deltas'])\n",
        "    for f, d in zip(feats, deltas):\n",
        "        feature_stats[f]['count'] += 1\n",
        "        feature_stats[f]['deltas'].append(d)\n",
        "        feature_stats[f]['demographics'].append(row['demographic'])\n",
        "\n",
        "top_features = [f for f, _ in sorted(\n",
        "    feature_stats.items(), key=lambda x: -x[1]['count']\n",
        ")[:N_TOP_FEATURES]]\n",
        "\n",
        "print(f\"\\nTop {N_TOP_FEATURES} features:\")\n",
        "for i, feat in enumerate(top_features):\n",
        "    info = feature_stats[feat]\n",
        "    demo_counts = Counter(info['demographics'])\n",
        "    top_demos = ', '.join(f\"{d}({c})\" for d, c in demo_counts.most_common(3))\n",
        "    print(f\"  {i+1}. Feature {feat}: {info['count']} pairs, {top_demos}\")\n",
        "\n",
        "# --- Encoding matrix ---\n",
        "\n",
        "print(f\"\\nRunning {N_TOP_FEATURES} x {len(ALL_DEMOS)} encoding tests...\")\n",
        "\n",
        "def get_last_token_activation(prompt, feature_idx):\n",
        "    tokens = model.to_tokens(prompt)\n",
        "    with torch.inference_mode():\n",
        "        _, cache = model.run_with_cache(tokens, names_filter=lambda n: hook_name in n)\n",
        "        h = cache[hook_name][0, -1, :].float()\n",
        "        sae_acts = sae.encode(h.unsqueeze(0))[0]\n",
        "        act = sae_acts[feature_idx].float().cpu().item()\n",
        "        del cache\n",
        "    return act\n",
        "\n",
        "\n",
        "def test_encoding(feature_idx, demographic, n_pairs):\n",
        "    demo_df = prompts_df[prompts_df['demographic'] == demographic]\n",
        "    pair_keys = demo_df['pair_key'].unique()[:n_pairs]\n",
        "    acts_a, acts_b = [], []\n",
        "\n",
        "    for pk in pair_keys:\n",
        "        pair = demo_df[demo_df['pair_key'] == pk]\n",
        "        if len(pair) < 2: continue\n",
        "        row_a = pair[pair['value_type'] == 'value_a'].iloc[0]\n",
        "        row_b = pair[pair['value_type'] == 'value_b'].iloc[0]\n",
        "        acts_a.append(get_last_token_activation(row_a['prompt'], feature_idx))\n",
        "        acts_b.append(get_last_token_activation(row_b['prompt'], feature_idx))\n",
        "\n",
        "    n = min(len(acts_a), len(acts_b))\n",
        "    if n < 5: return None\n",
        "\n",
        "    acts_a, acts_b = acts_a[:n], acts_b[:n]\n",
        "    diffs = np.array(acts_a) - np.array(acts_b)\n",
        "\n",
        "    t_stat, p_val = stats.ttest_rel(acts_a, acts_b)\n",
        "    std_diff = np.std(diffs, ddof=1)\n",
        "    cohens_d = np.mean(diffs) / std_diff if std_diff > 1e-10 else 0.0\n",
        "\n",
        "    mean_diff = np.mean(diffs)\n",
        "    se = stats.sem(diffs)\n",
        "    df = n - 1\n",
        "    t_crit = t_dist.ppf(0.975, df)\n",
        "\n",
        "    n_pos = sum(1 for d in diffs if d > 0)\n",
        "    n_neg = sum(1 for d in diffs if d < 0)\n",
        "\n",
        "    return {\n",
        "        'n': n, 'mean_a': np.mean(acts_a), 'mean_b': np.mean(acts_b),\n",
        "        'mean_diff': mean_diff,\n",
        "        'ci_low': mean_diff - t_crit * se, 'ci_high': mean_diff + t_crit * se,\n",
        "        't_stat': t_stat, 'p_val': p_val, 'cohens_d': cohens_d,\n",
        "        'consistency': max(n_pos, n_neg) / n * 100,\n",
        "        'direction': 'value_a > value_b' if mean_diff > 0 else 'value_b > value_a',\n",
        "    }\n",
        "\n",
        "\n",
        "encoding_results = {}\n",
        "for feat in tqdm(top_features, desc=\"Encoding\"):\n",
        "    encoding_results[feat] = {}\n",
        "    for demo in ALL_DEMOS:\n",
        "        encoding_results[feat][demo] = test_encoding(feat, demo, N_PAIRS_ENCODING)\n",
        "\n",
        "# Collect and FDR correct\n",
        "all_rows = []\n",
        "for feat in top_features:\n",
        "    for demo in ALL_DEMOS:\n",
        "        r = encoding_results[feat][demo]\n",
        "        if r is not None:\n",
        "            all_rows.append({\n",
        "                'feature': feat, 'demographic': demo,\n",
        "                'n': r['n'], 'mean_diff': r['mean_diff'],\n",
        "                'ci_low': r['ci_low'], 'ci_high': r['ci_high'],\n",
        "                't_stat': r['t_stat'], 'p_raw': r['p_val'],\n",
        "                'cohens_d': r['cohens_d'], 'consistency': r['consistency'],\n",
        "                'direction': r['direction'],\n",
        "            })\n",
        "\n",
        "enc_df = pd.DataFrame(all_rows)\n",
        "reject, p_corrected, _, _ = multipletests(enc_df['p_raw'], alpha=ALPHA, method='fdr_bh')\n",
        "enc_df['p_corrected'] = p_corrected\n",
        "enc_df['significant'] = reject\n",
        "enc_df['sig_label'] = enc_df['p_corrected'].apply(\n",
        "    lambda p: '***' if p < 0.001 else ('**' if p < 0.01 else ('*' if p < 0.05 else 'ns')))\n",
        "\n",
        "n_sig = enc_df['significant'].sum()\n",
        "n_total = len(enc_df)\n",
        "print(f\"\\nSignificant (FDR α={ALPHA}): {n_sig}/{n_total} ({n_sig/n_total*100:.1f}%)\")\n",
        "\n",
        "enc_df.to_csv(DATA / 'encoding_matrix_full.csv', index=False)\n",
        "\n",
        "# Compact pivot\n",
        "pivot_data = []\n",
        "for feat in top_features:\n",
        "    row = {'Feature': feat, 'N_Pairs': feature_stats[feat]['count']}\n",
        "    for demo in ALL_DEMOS:\n",
        "        r_df = enc_df[(enc_df['feature'] == feat) & (enc_df['demographic'] == demo)]\n",
        "        if len(r_df) == 0: row[demo] = '-'\n",
        "        else:\n",
        "            r = r_df.iloc[0]\n",
        "            row[demo] = f\"{r['sig_label']}(d={r['cohens_d']:+.1f})\" if r['significant'] else 'ns'\n",
        "    pivot_data.append(row)\n",
        "\n",
        "pivot_df = pd.DataFrame(pivot_data)\n",
        "print(pivot_df.to_string(index=False))\n",
        "pivot_df.to_csv(DATA / 'encoding_matrix_compact.csv', index=False)\n",
        "with open(TABLES / 'table_encoding_matrix.tex', 'w') as f:\n",
        "    f.write(pivot_df.to_latex(index=False, escape=False))\n",
        "\n",
        "# --- Fig 1: Encoding heatmap ---\n",
        "\n",
        "heat_matrix = np.zeros((N_TOP_FEATURES, len(ALL_DEMOS)))\n",
        "for i, feat in enumerate(top_features):\n",
        "    for j, demo in enumerate(ALL_DEMOS):\n",
        "        r_df = enc_df[(enc_df['feature'] == feat) & (enc_df['demographic'] == demo)]\n",
        "        heat_matrix[i, j] = r_df['cohens_d'].iloc[0] if len(r_df) > 0 else 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, max(5, N_TOP_FEATURES * 0.5 + 1)))\n",
        "vmax = max(3.0, np.abs(heat_matrix).max())\n",
        "sns.heatmap(heat_matrix, annot=True, fmt='+.1f', cmap='RdBu_r', center=0,\n",
        "            vmin=-vmax, vmax=vmax,\n",
        "            xticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "            yticklabels=[str(f) for f in top_features],\n",
        "            ax=ax, linewidths=0.5, cbar_kws={'label': \"Cohen's d\"})\n",
        "for i, feat in enumerate(top_features):\n",
        "    for j, demo in enumerate(ALL_DEMOS):\n",
        "        r_df = enc_df[(enc_df['feature'] == feat) & (enc_df['demographic'] == demo)]\n",
        "        if len(r_df) > 0 and r_df.iloc[0]['significant']:\n",
        "            ax.text(j + 0.5, i + 0.82, r_df.iloc[0]['sig_label'],\n",
        "                    ha='center', va='center', fontsize=7, color='black')\n",
        "ax.set_title(f\"{MODEL_NAME}: Feature × Demographic Encoding (Layer {LAYER})\\n\"\n",
        "             f\"Cohen's d, FDR-corrected\", fontweight='bold')\n",
        "ax.set_ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_encoding_matrix.png')\n",
        "plt.savefig(FIGS / 'fig_encoding_matrix.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_encoding_matrix\")\n",
        "\n",
        "# --- Fig 2: Encoding strength summary ---\n",
        "\n",
        "sig_counts = enc_df[enc_df['significant']].groupby('feature').size().reindex(top_features, fill_value=0)\n",
        "mean_abs_d = enc_df.groupby('feature')['cohens_d'].apply(\n",
        "    lambda x: x.abs().mean()).reindex(top_features, fill_value=0)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "colors_a = ['#e74c3c' if c >= 3 else '#3498db' if c >= 1 else '#bdc3c7' for c in sig_counts.values]\n",
        "ax1.barh(range(len(top_features)), sig_counts.values, color=colors_a)\n",
        "ax1.set_yticks(range(len(top_features)))\n",
        "ax1.set_yticklabels([str(f) for f in top_features])\n",
        "ax1.set_xlabel('N Significant Demographics (FDR < 0.05)')\n",
        "ax1.set_title('A. Encoding Breadth', fontweight='bold')\n",
        "ax1.set_xlim(0, len(ALL_DEMOS) + 0.5)\n",
        "for i, v in enumerate(sig_counts.values):\n",
        "    ax1.text(v + 0.1, i, str(v), va='center', fontsize=9)\n",
        "ax1.invert_yaxis()\n",
        "\n",
        "colors_b = ['#e74c3c' if d >= 1.0 else '#f39c12' if d >= 0.5 else '#3498db' for d in mean_abs_d.values]\n",
        "ax2.barh(range(len(top_features)), mean_abs_d.values, color=colors_b)\n",
        "ax2.set_yticks(range(len(top_features)))\n",
        "ax2.set_yticklabels([str(f) for f in top_features])\n",
        "ax2.set_xlabel(\"Mean |Cohen's d|\")\n",
        "ax2.set_title('B. Encoding Strength', fontweight='bold')\n",
        "ax2.axvline(x=0.8, color='gray', linestyle='--', alpha=0.3, label='d=0.8')\n",
        "ax2.legend(fontsize=8); ax2.invert_yaxis()\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Feature Encoding Summary (Layer {LAYER})', fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_encoding_strength.png')\n",
        "plt.savefig(FIGS / 'fig_encoding_strength.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_encoding_strength\")\n",
        "\n",
        "# --- Fig 3: Effect size distribution ---\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sig_d = enc_df[enc_df['significant']]['cohens_d'].values\n",
        "ns_d = enc_df[~enc_df['significant']]['cohens_d'].values\n",
        "\n",
        "if len(sig_d) > 0 and len(ns_d) > 0:\n",
        "    ax1.violinplot([sig_d, ns_d], positions=[1, 2], showmeans=True, showmedians=True)\n",
        "    ax1.set_xticks([1, 2]); ax1.set_xticklabels(['Significant', 'Non-significant'])\n",
        "elif len(sig_d) > 0:\n",
        "    ax1.violinplot([sig_d], positions=[1], showmeans=True, showmedians=True)\n",
        "    ax1.set_xticks([1]); ax1.set_xticklabels(['Significant'])\n",
        "else:\n",
        "    ax1.violinplot([enc_df['cohens_d'].values], positions=[1], showmeans=True, showmedians=True)\n",
        "    ax1.set_xticks([1]); ax1.set_xticklabels(['All'])\n",
        "ax1.set_ylabel(\"Cohen's d\"); ax1.set_title('A. Effect Size Distribution', fontweight='bold')\n",
        "ax1.axhline(0, color='gray', linewidth=0.5)\n",
        "ax1.axhline(0.8, color='red', linewidth=0.5, linestyle='--', alpha=0.5)\n",
        "ax1.axhline(-0.8, color='red', linewidth=0.5, linestyle='--', alpha=0.5)\n",
        "\n",
        "bins = np.linspace(0, enc_df['cohens_d'].abs().max() + 0.2, 20)\n",
        "ax2.hist(enc_df[enc_df['significant']]['cohens_d'].abs(), bins=bins,\n",
        "         alpha=0.7, label=f'Significant (n={len(sig_d)})', color='#e74c3c')\n",
        "ax2.hist(enc_df[~enc_df['significant']]['cohens_d'].abs(), bins=bins,\n",
        "         alpha=0.7, label=f'Non-significant (n={len(ns_d)})', color='#bdc3c7')\n",
        "ax2.axvline(0.8, color='black', linestyle='--', linewidth=1, label='d=0.8')\n",
        "ax2.set_xlabel(\"|Cohen's d|\"); ax2.set_ylabel('Count')\n",
        "ax2.set_title('B. Effect Size Magnitude', fontweight='bold'); ax2.legend(fontsize=9)\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Effect Size Analysis (Layer {LAYER})', fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_effect_size_dist.png')\n",
        "plt.savefig(FIGS / 'fig_effect_size_dist.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_effect_size_dist\")\n",
        "\n",
        "# --- Fig 4: Consistency vs effect size ---\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "for demo in ALL_DEMOS:\n",
        "    sub = enc_df[enc_df['demographic'] == demo]\n",
        "    sig_sub = sub[sub['significant']]\n",
        "    ns_sub = sub[~sub['significant']]\n",
        "    if len(sig_sub) > 0:\n",
        "        ax.scatter(sig_sub['cohens_d'].abs(), sig_sub['consistency'],\n",
        "                   c=DEMO_COLORS[demo], label=demo.capitalize(), s=70, alpha=0.8, marker='*')\n",
        "    if len(ns_sub) > 0:\n",
        "        ax.scatter(ns_sub['cohens_d'].abs(), ns_sub['consistency'],\n",
        "                   c=DEMO_COLORS[demo], s=40, alpha=0.4, marker='x',\n",
        "                   label=f'{demo.capitalize()} (ns)' if len(sig_sub) == 0 else '')\n",
        "\n",
        "ax.axhline(60, color='gray', linestyle='--', alpha=0.3)\n",
        "ax.axvline(0.8, color='gray', linestyle='--', alpha=0.3)\n",
        "ax.set_xlabel(\"|Cohen's d|\"); ax.set_ylabel(\"Consistency (%)\")\n",
        "ax.set_title(f\"{MODEL_NAME}: Consistency vs Effect Size (Layer {LAYER})\\n\"\n",
        "             f\"★ = significant, × = non-significant\", fontweight='bold')\n",
        "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9); ax.set_ylim(45, 105)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_consistency_vs_effect.png')\n",
        "plt.savefig(FIGS / 'fig_consistency_vs_effect.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_consistency_vs_effect\")\n",
        "\n",
        "print(\"\\nEncoding analysis done.\")"
      ],
      "metadata": {
        "id": "cKrFXXl_xmJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature interpretation — location analysis, multi-demographic direction, summary\n",
        "\n",
        "# --- Location analysis ---\n",
        "\n",
        "print(\"Location analysis: span vs response position\")\n",
        "\n",
        "DEMO_SPAN_PATTERNS = {\n",
        "    'income': [\n",
        "        r'You are financially wealthy\\..*?money\\.',\n",
        "        r'You are financially poor\\..*?choices\\.',\n",
        "    ],\n",
        "    'age': [\n",
        "        r'You are 75 years old\\..*?phones\\.',\n",
        "        r'You are elderly.*?differently\\.',\n",
        "        r'You are 22 years old\\..*?ahead\\.',\n",
        "        r'You are a young adult.*?identity\\.',\n",
        "    ],\n",
        "    'gender': [\n",
        "        r'You are a man\\..*?expectations\\.',\n",
        "        r'You are male\\..*?norms\\.',\n",
        "        r'You are a woman\\..*?expectations\\.',\n",
        "        r'You are female\\..*?norms\\.',\n",
        "    ],\n",
        "    'education': [\n",
        "        r'You have a PhD.*?nature\\.',\n",
        "        r'You are highly educated.*?pursuits\\.',\n",
        "        r'You did not complete high school\\..*?solving\\.',\n",
        "        r'You have limited formal education\\..*?credentials\\.',\n",
        "    ],\n",
        "    'vote': [\n",
        "        r'You are a regular voter.*?ballot\\.',\n",
        "        r'You are a politically engaged.*?identity\\.',\n",
        "        r'You are a non-voter.*?matters\\.',\n",
        "        r'You are politically disengaged.*?life\\.',\n",
        "    ],\n",
        "}\n",
        "\n",
        "\n",
        "def find_demographic_span(prompt, token_strs, demographic):\n",
        "    char_pos = 0\n",
        "    token_ranges = []\n",
        "    for i, tok in enumerate(token_strs):\n",
        "        token_ranges.append((char_pos, char_pos + len(tok), i))\n",
        "        char_pos += len(tok)\n",
        "\n",
        "    patterns = DEMO_SPAN_PATTERNS.get(demographic, [])\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, prompt, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            return {tok_idx for char_start, char_end, tok_idx in token_ranges\n",
        "                    if char_start < match.end() and char_end > match.start()}\n",
        "    return set()\n",
        "\n",
        "\n",
        "def analyze_location(feature_idx, demographic, n_pairs):\n",
        "    demo_df = prompts_df[prompts_df['demographic'] == demographic]\n",
        "    pair_keys = demo_df['pair_key'].unique()[:n_pairs]\n",
        "\n",
        "    span_diffs, last_diffs = [], []\n",
        "    span_found_count = 0\n",
        "\n",
        "    for pk in pair_keys:\n",
        "        pair = demo_df[demo_df['pair_key'] == pk]\n",
        "        if len(pair) < 2: continue\n",
        "        row_a = pair[pair['value_type'] == 'value_a'].iloc[0]\n",
        "        row_b = pair[pair['value_type'] == 'value_b'].iloc[0]\n",
        "\n",
        "        span_acts, last_acts = {}, {}\n",
        "\n",
        "        for prompt, label in [(row_a['prompt'], 'a'), (row_b['prompt'], 'b')]:\n",
        "            tokens = model.to_tokens(prompt)\n",
        "            token_strs = [tokenizer.decode([t]) for t in tokens[0]]\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                _, cache = model.run_with_cache(tokens, names_filter=lambda n: hook_name in n)\n",
        "                h = cache[hook_name][0, :, :].float()\n",
        "                sae_acts = sae.encode(h)\n",
        "                acts = sae_acts[:, feature_idx].float().cpu().numpy()\n",
        "                del cache\n",
        "\n",
        "            demo_indices = find_demographic_span(prompt, token_strs, demographic)\n",
        "            if demo_indices:\n",
        "                valid_indices = [i for i in demo_indices if i < len(acts)]\n",
        "                if valid_indices:\n",
        "                    span_acts[label] = np.mean([acts[i] for i in valid_indices])\n",
        "                    span_found_count += 1\n",
        "                else:\n",
        "                    span_acts[label] = None\n",
        "            else:\n",
        "                span_acts[label] = None\n",
        "\n",
        "            last_acts[label] = float(acts[-1])\n",
        "\n",
        "        if span_acts.get('a') is not None and span_acts.get('b') is not None:\n",
        "            span_diffs.append(span_acts['a'] - span_acts['b'])\n",
        "        last_diffs.append(last_acts['a'] - last_acts['b'])\n",
        "\n",
        "    n_total_prompts = 2 * len(last_diffs) if last_diffs else 1\n",
        "    result = {'n_pairs': len(last_diffs), 'span_found_pct': span_found_count / n_total_prompts * 100}\n",
        "\n",
        "    if len(span_diffs) > 1:\n",
        "        result['span_mean'] = np.mean(span_diffs)\n",
        "        result['span_t'], result['span_p'] = stats.ttest_1samp(span_diffs, 0)\n",
        "    else:\n",
        "        result['span_mean'] = 0; result['span_t'] = result['span_p'] = np.nan\n",
        "\n",
        "    if len(last_diffs) > 1:\n",
        "        result['last_mean'] = np.mean(last_diffs)\n",
        "        result['last_t'], result['last_p'] = stats.ttest_1samp(last_diffs, 0)\n",
        "    else:\n",
        "        result['last_mean'] = 0; result['last_t'] = result['last_p'] = np.nan\n",
        "\n",
        "    result['ratio'] = abs(result['last_mean'] / result['span_mean']) if abs(result['span_mean']) > 0.001 else float('inf')\n",
        "    return result\n",
        "\n",
        "\n",
        "location_rows = []\n",
        "for feat in tqdm(top_features, desc=\"Location\"):\n",
        "    for demo in ALL_DEMOS:\n",
        "        loc = analyze_location(feat, demo, N_PAIRS_LOCATION)\n",
        "        loc['feature'] = feat; loc['demographic'] = demo\n",
        "        location_rows.append(loc)\n",
        "\n",
        "loc_df = pd.DataFrame(location_rows)\n",
        "loc_df.to_csv(DATA / 'activation_location.csv', index=False)\n",
        "\n",
        "valid_ratios = loc_df[(loc_df['ratio'] < 1000) & (loc_df['ratio'] > 0)]['ratio']\n",
        "mean_ratio = valid_ratios.mean() if len(valid_ratios) > 0 else float('inf')\n",
        "median_ratio = valid_ratios.median() if len(valid_ratios) > 0 else float('inf')\n",
        "n_span_sig = ((loc_df['span_p'] < 0.05) & (~loc_df['span_p'].isna())).sum()\n",
        "n_last_sig = ((loc_df['last_p'] < 0.05) & (~loc_df['last_p'].isna())).sum()\n",
        "\n",
        "print(f\"\\nSpan significant: {n_span_sig}/{len(loc_df)}\")\n",
        "print(f\"Last significant: {n_last_sig}/{len(loc_df)}\")\n",
        "print(f\"Mean |last/span| ratio: {mean_ratio:.1f}x, median: {median_ratio:.1f}x\")\n",
        "\n",
        "# --- Fig 5: Location heatmap ---\n",
        "\n",
        "loc_heat = np.zeros((len(top_features), len(ALL_DEMOS)))\n",
        "for i, feat in enumerate(top_features):\n",
        "    for j, demo in enumerate(ALL_DEMOS):\n",
        "        r = loc_df[(loc_df['feature'] == feat) & (loc_df['demographic'] == demo)]\n",
        "        loc_heat[i, j] = r['last_mean'].iloc[0] if len(r) > 0 else 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, max(5, len(top_features) * 0.5 + 1)))\n",
        "vmax = max(0.5, np.abs(loc_heat).max())\n",
        "sns.heatmap(loc_heat, annot=True, fmt='+.3f', cmap='RdBu_r', center=0,\n",
        "            vmin=-vmax, vmax=vmax,\n",
        "            xticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "            yticklabels=[str(f) for f in top_features],\n",
        "            ax=ax, linewidths=0.5, cbar_kws={'label': 'Activation Diff (a-b)'})\n",
        "ax.set_title(f'{MODEL_NAME}: Last-Token Activation Difference (Layer {LAYER})', fontweight='bold')\n",
        "ax.set_ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_location_heatmap.png')\n",
        "plt.savefig(FIGS / 'fig_location_heatmap.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_location_heatmap\")\n",
        "\n",
        "# --- Fig 6: Span vs last scatter ---\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "valid_loc = loc_df.dropna(subset=['span_mean', 'last_mean'])\n",
        "valid_loc = valid_loc[valid_loc['span_found_pct'] > 50]\n",
        "\n",
        "for demo in ALL_DEMOS:\n",
        "    sub = valid_loc[valid_loc['demographic'] == demo]\n",
        "    ax.scatter(sub['span_mean'], sub['last_mean'],\n",
        "               color=DEMO_COLORS[demo], label=demo.capitalize(), s=60, alpha=0.8)\n",
        "\n",
        "if len(valid_loc) > 0:\n",
        "    lim = max(abs(valid_loc['span_mean']).max(), abs(valid_loc['last_mean']).max()) * 1.1\n",
        "    ax.plot([-lim, lim], [-lim, lim], 'k--', alpha=0.3, linewidth=1, label='Equal')\n",
        "\n",
        "if len(valid_loc) >= 3:\n",
        "    r, p = stats.pearsonr(valid_loc['span_mean'], valid_loc['last_mean'])\n",
        "    ax.set_title(f'{MODEL_NAME}: Span vs Response-Position\\nr={r:.2f} ({\"p<.001\" if p < 0.001 else f\"p={p:.3f}\"})',\n",
        "                 fontweight='bold')\n",
        "else:\n",
        "    ax.set_title(f'{MODEL_NAME}: Span vs Response-Position', fontweight='bold')\n",
        "\n",
        "ax.set_xlabel('Span Diff (a-b)'); ax.set_ylabel('Last-Token Diff (a-b)')\n",
        "ax.legend(); ax.axhline(0, color='gray', linewidth=0.5); ax.axvline(0, color='gray', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_span_vs_last.png')\n",
        "plt.savefig(FIGS / 'fig_span_vs_last.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_span_vs_last\")\n",
        "\n",
        "# --- Fig 7: Ratio by demographic ---\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
        "\n",
        "ratio_matrix = np.zeros((len(top_features), len(ALL_DEMOS)))\n",
        "for i, feat in enumerate(top_features):\n",
        "    for j, demo in enumerate(ALL_DEMOS):\n",
        "        r = loc_df[(loc_df['feature'] == feat) & (loc_df['demographic'] == demo)]\n",
        "        ratio_matrix[i, j] = min(r['ratio'].iloc[0], 200) if len(r) > 0 else np.nan\n",
        "\n",
        "sns.heatmap(ratio_matrix, annot=True, fmt='.0f', cmap='YlOrRd',\n",
        "            xticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "            yticklabels=[str(f) for f in top_features],\n",
        "            ax=ax1, linewidths=0.5, mask=np.isnan(ratio_matrix),\n",
        "            cbar_kws={'label': '|Last / Span| Ratio'})\n",
        "ax1.set_title('A. Last/Span Ratio (capped 200)', fontweight='bold'); ax1.set_ylabel('Feature')\n",
        "\n",
        "demo_ratios = {}\n",
        "for demo in ALL_DEMOS:\n",
        "    sub = loc_df[(loc_df['demographic'] == demo) & (loc_df['ratio'] < 1000)]\n",
        "    demo_ratios[demo] = {'mean': sub['ratio'].mean() if len(sub) > 0 else 0,\n",
        "                          'median': sub['ratio'].median() if len(sub) > 0 else 0}\n",
        "\n",
        "means = [demo_ratios[d]['mean'] for d in ALL_DEMOS]\n",
        "medians = [demo_ratios[d]['median'] for d in ALL_DEMOS]\n",
        "ax2.bar(range(len(ALL_DEMOS)), means, color=[DEMO_COLORS[d] for d in ALL_DEMOS], alpha=0.8, label='Mean')\n",
        "ax2.scatter(range(len(ALL_DEMOS)), medians, color='black', zorder=5, s=50, marker='D', label='Median')\n",
        "ax2.set_xticks(range(len(ALL_DEMOS)))\n",
        "ax2.set_xticklabels([d.capitalize() for d in ALL_DEMOS])\n",
        "ax2.set_ylabel('|Last / Span| Ratio'); ax2.set_title('B. Mean Ratio by Demographic', fontweight='bold')\n",
        "ax2.legend(fontsize=9)\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Span vs Response-Position Ratio (Layer {LAYER})', fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGS / 'fig_location_ratio.png')\n",
        "plt.savefig(FIGS / 'fig_location_ratio.pdf')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_location_ratio\")\n",
        "\n",
        "\n",
        "# --- Multi-demographic direction ---\n",
        "\n",
        "print(\"\\nMulti-demographic encoding direction\")\n",
        "\n",
        "direction_results = []\n",
        "for feat in top_features:\n",
        "    sig_enc = enc_df[(enc_df['feature'] == feat) & (enc_df['significant'])].copy()\n",
        "    if len(sig_enc) < 2: continue\n",
        "\n",
        "    pos_demos = sig_enc[sig_enc['mean_diff'] > 0]['demographic'].tolist()\n",
        "    neg_demos = sig_enc[sig_enc['mean_diff'] < 0]['demographic'].tolist()\n",
        "\n",
        "    if pos_demos and neg_demos:\n",
        "        direction_results.append({\n",
        "            'feature': feat, 'n_sig_demos': len(sig_enc),\n",
        "            'positive_demos': ', '.join(pos_demos), 'negative_demos': ', '.join(neg_demos),\n",
        "            'opposite_direction': True,\n",
        "            'description': f\"value_a > value_b for {', '.join(pos_demos)}; value_b > value_a for {', '.join(neg_demos)}\",\n",
        "        })\n",
        "    else:\n",
        "        direction = 'value_a > value_b' if pos_demos else 'value_b > value_a'\n",
        "        all_demos = pos_demos or neg_demos\n",
        "        direction_results.append({\n",
        "            'feature': feat, 'n_sig_demos': len(sig_enc),\n",
        "            'positive_demos': ', '.join(pos_demos), 'negative_demos': ', '.join(neg_demos),\n",
        "            'opposite_direction': False,\n",
        "            'description': f\"Same direction ({direction}) for {', '.join(all_demos)}\",\n",
        "        })\n",
        "\n",
        "dir_df = pd.DataFrame(direction_results)\n",
        "\n",
        "if len(dir_df) > 0:\n",
        "    n_opposite = dir_df['opposite_direction'].sum()\n",
        "    n_same = (~dir_df['opposite_direction']).sum()\n",
        "    print(f\"  Features with 2+ sig demos: {len(dir_df)}\")\n",
        "    print(f\"  Opposite-direction: {n_opposite}, Same-direction: {n_same}\")\n",
        "    for _, row in dir_df.iterrows():\n",
        "        print(f\"  [{'OPP' if row['opposite_direction'] else 'SAME'}] Feature {row['feature']}: {row['description']}\")\n",
        "    dir_df.to_csv(DATA / 'multi_demo_direction.csv', index=False)\n",
        "else:\n",
        "    n_opposite, n_same = 0, 0\n",
        "    print(\"  No features with 2+ significant demographics\")\n",
        "\n",
        "# --- Fig 8: Multi-demographic direction profile ---\n",
        "\n",
        "multi_demo_feats = enc_df[enc_df['significant']].groupby('feature').filter(\n",
        "    lambda x: len(x) >= 2)['feature'].unique()\n",
        "\n",
        "if len(multi_demo_feats) >= 2:\n",
        "    poly_heat = np.zeros((len(multi_demo_feats), len(ALL_DEMOS)))\n",
        "    sig_mask = np.ones_like(poly_heat, dtype=bool)\n",
        "\n",
        "    for i, feat in enumerate(multi_demo_feats):\n",
        "        for j, demo in enumerate(ALL_DEMOS):\n",
        "            r = enc_df[(enc_df['feature'] == feat) & (enc_df['demographic'] == demo)]\n",
        "            if len(r) > 0 and r.iloc[0]['significant']:\n",
        "                poly_heat[i, j] = r.iloc[0]['cohens_d']\n",
        "                sig_mask[i, j] = False\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, max(4, len(multi_demo_feats) * 0.6 + 1)))\n",
        "    vmax = max(2.0, np.abs(poly_heat).max())\n",
        "    sns.heatmap(poly_heat, annot=True, fmt='+.1f', cmap='RdBu_r', center=0,\n",
        "                vmin=-vmax, vmax=vmax,\n",
        "                xticklabels=[d.capitalize() for d in ALL_DEMOS],\n",
        "                yticklabels=[str(f) for f in multi_demo_feats],\n",
        "                ax=ax, linewidths=0.5, cbar_kws={'label': \"Cohen's d (signed)\"})\n",
        "\n",
        "    for i in range(len(multi_demo_feats)):\n",
        "        for j in range(len(ALL_DEMOS)):\n",
        "            if sig_mask[i, j]:\n",
        "                ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, facecolor='lightgray', alpha=0.7))\n",
        "                ax.text(j + 0.5, i + 0.5, 'ns', ha='center', va='center', fontsize=8, color='gray')\n",
        "\n",
        "    if len(dir_df) > 0:\n",
        "        opp_feats = set(dir_df[dir_df['opposite_direction']]['feature'].values)\n",
        "        for i, feat in enumerate(multi_demo_feats):\n",
        "            if feat in opp_feats:\n",
        "                ax.add_patch(plt.Rectangle((0, i), len(ALL_DEMOS), 1,\n",
        "                             fill=False, edgecolor='gold', linewidth=2.5))\n",
        "\n",
        "    ax.set_title(f\"{MODEL_NAME}: Multi-Demographic Encoding Direction (Layer {LAYER})\\n\"\n",
        "                 f\"gray = ns, gold = opposite-direction\", fontweight='bold')\n",
        "    ax.set_ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGS / 'fig_multi_demo_direction.png')\n",
        "    plt.savefig(FIGS / 'fig_multi_demo_direction.pdf')\n",
        "    plt.close()\n",
        "    print(\"  Saved: fig_multi_demo_direction\")\n",
        "else:\n",
        "    print(\"  Skipped direction figure (< 2 multi-demo features)\")\n",
        "\n",
        "\n",
        "# --- Summary and bundle ---\n",
        "\n",
        "print(\"\\nSummary\")\n",
        "\n",
        "summary = {\n",
        "    'model': MODEL_NAME,\n",
        "    'parameters': {\n",
        "        'layer': LAYER, 'layer_depth': f\"{LAYER/N_LAYERS_TOTAL:.0%}\",\n",
        "        'n_features': N_TOP_FEATURES, 'n_pairs_encoding': N_PAIRS_ENCODING,\n",
        "        'n_pairs_location': N_PAIRS_LOCATION, 'alpha': ALPHA,\n",
        "        'features_tested': top_features,\n",
        "    },\n",
        "    'encoding': {\n",
        "        'n_tests': n_total, 'n_significant': int(n_sig),\n",
        "        'pct_significant': round(n_sig / n_total * 100, 1),\n",
        "        'mean_abs_d': round(float(enc_df['cohens_d'].abs().mean()), 2),\n",
        "    },\n",
        "    'location': {\n",
        "        'n_span_significant': int(n_span_sig), 'n_last_significant': int(n_last_sig),\n",
        "        'mean_ratio': round(mean_ratio, 1) if mean_ratio < 1000 else 'inf',\n",
        "        'median_ratio': round(median_ratio, 1) if median_ratio < 1000 else 'inf',\n",
        "    },\n",
        "    'multi_demographic': {\n",
        "        'n_multi_demo_features': len(dir_df),\n",
        "        'n_opposite': int(n_opposite), 'n_same': int(n_same),\n",
        "    },\n",
        "}\n",
        "\n",
        "for section, vals in summary.items():\n",
        "    print(f\"\\n{section}:\")\n",
        "    if isinstance(vals, dict):\n",
        "        for k, v in vals.items(): print(f\"  {k}: {v}\")\n",
        "    else: print(f\"  {vals}\")\n",
        "\n",
        "with open(DATA / 'interpretation_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "readme = f\"\"\"# Feature Interpretation — {MODEL_NAME}\n",
        "\n",
        "## Analyses\n",
        "1. Encoding Matrix: {N_TOP_FEATURES} features x {len(ALL_DEMOS)} demographics\n",
        "   - Paired t-tests, BH-FDR correction\n",
        "   - {n_sig}/{n_total} significant ({n_sig/n_total*100:.1f}%)\n",
        "\n",
        "2. Activation Location: span vs response-position\n",
        "   - Mean |last/span| ratio: {mean_ratio:.1f}x\n",
        "\n",
        "3. Multi-Demographic Direction: {len(dir_df)} features with 2+ sig demos\n",
        "   - {n_opposite} opposite-direction, {n_same} same-direction\n",
        "\n",
        "## Model\n",
        "- {MODEL_NAME}, Layer {LAYER} ({LAYER/N_LAYERS_TOTAL:.0%} depth)\n",
        "- SAE: gemma-scope 16k residual stream\n",
        "\"\"\"\n",
        "\n",
        "with open(BUNDLE / 'README.md', 'w') as f:\n",
        "    f.write(readme)\n",
        "\n",
        "shutil.make_archive(str(BUNDLE), 'zip', root_dir=BUNDLE.parent, base_dir=BUNDLE.name)\n",
        "\n",
        "n_png = len(list(FIGS.glob('*.png')))\n",
        "print(f\"\\n  {n_png} figures, {len(list(TABLES.glob('*')))} tables, {len(list(DATA.glob('*')))} data files\")\n",
        "print(f\"  Download: feature-interpretation-results-gemma.zip\")\n",
        "\n",
        "del sae\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "NMoVMY1Qx_M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concern diagnostics — extraction-side (concerns 1-6)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "ANALYSIS_LAYERS = [5, 9, 14, 18, 20, 27, 32, 36]\n",
        "LAYER_DEPTH = {5: 12, 9: 22, 14: 34, 18: 44, 20: 49, 27: 66, 32: 78, 36: 88}\n",
        "IT_LAYERS = {20}\n",
        "DEMOGRAPHICS = ['income', 'age', 'gender', 'education', 'vote']\n",
        "DOMAINS = ['climate', 'health', 'digital', 'economy', 'values']\n",
        "\n",
        "BASE_DIR = Path(\"/content\")\n",
        "OUTPUT_DIR = Path(\"/content/reviewer-diagnostics\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"figures\").mkdir(exist_ok=True)\n",
        "(OUTPUT_DIR / \"data\").mkdir(exist_ok=True)\n",
        "\n",
        "DATA_DIR = None\n",
        "for candidate in [\n",
        "    BASE_DIR / \"feature-extraction-results-gemma\" / \"data\",\n",
        "    BASE_DIR / \"feature_extraction_results_gemma\" / \"data\",\n",
        "]:\n",
        "    if candidate.exists():\n",
        "        DATA_DIR = candidate; break\n",
        "if DATA_DIR is None:\n",
        "    for match in BASE_DIR.rglob(\"feature_stats.csv\"):\n",
        "        DATA_DIR = match.parent; break\n",
        "if DATA_DIR is None:\n",
        "    raise FileNotFoundError(\"Cannot find feature extraction data directory\")\n",
        "\n",
        "print(f\"Data: {DATA_DIR}\")\n",
        "\n",
        "feat_df = pd.read_csv(DATA_DIR / \"feature_stats.csv\")\n",
        "funnel_df = pd.read_csv(DATA_DIR / \"filtering_funnel.csv\")\n",
        "behav_df = pd.read_csv(DATA_DIR / \"behavioral_effects.csv\")\n",
        "with open(DATA_DIR / \"extraction_summary.json\") as f:\n",
        "    summary = json.load(f)\n",
        "with open(DATA_DIR / \"selected_features.json\") as f:\n",
        "    selected = json.load(f)\n",
        "\n",
        "print(f\"  feature_stats: {len(feat_df)}, funnel: {len(funnel_df)}, behavioral: {len(behav_df)}\")\n",
        "\n",
        "diag = {'model': 'Gemma 2 9B IT', 'concerns': {}}\n",
        "\n",
        "plt.rcParams.update({'font.size': 10, 'axes.titlesize': 11, 'axes.labelsize': 10, 'figure.dpi': 150})\n",
        "\n",
        "\n",
        "# --- Concern 1: Extreme effect sizes ---\n",
        "\n",
        "print(\"\\nConcern 1: Extreme effect sizes\")\n",
        "\n",
        "feat_df['abs_d'] = feat_df['cohens_d'].abs()\n",
        "feat_df['abs_mean_diff'] = feat_df['mean_diff'].abs()\n",
        "feat_df['d_category'] = pd.cut(\n",
        "    feat_df['abs_d'],\n",
        "    bins=[0, 0.8, 2.0, 5.0, 10.0, np.inf],\n",
        "    labels=['medium (0.3-0.8)', 'large (0.8-2)', 'very large (2-5)',\n",
        "            'extreme (5-10)', 'implausible (>10)'])\n",
        "\n",
        "cat_counts = feat_df['d_category'].value_counts().sort_index()\n",
        "for cat, n in cat_counts.items():\n",
        "    print(f\"  {cat:<25} {n:>5} ({n/len(feat_df)*100:>5.1f}%)\")\n",
        "\n",
        "extreme_df = feat_df[feat_df['abs_d'] > 5.0]\n",
        "print(f\"\\n  Extreme (|d|>5): {len(extreme_df)} ({len(extreme_df)/len(feat_df)*100:.1f}%)\")\n",
        "\n",
        "if len(extreme_df) > 0:\n",
        "    print(f\"  Mean |mean_diff| extreme: {extreme_df['abs_mean_diff'].mean():.2f}\")\n",
        "    print(f\"  Mean |mean_diff| normal:  {feat_df[feat_df['abs_d'] <= 5]['abs_mean_diff'].mean():.2f}\")\n",
        "    extreme_multi = extreme_df.groupby(['layer', 'feature_idx'])['demographic'].nunique()\n",
        "    print(f\"  Extreme encoding 1 demo: {(extreme_multi == 1).sum()}, 2+: {(extreme_multi >= 2).sum()}\")\n",
        "\n",
        "feat_df['implied_sd'] = feat_df['abs_mean_diff'] / feat_df['abs_d'].clip(lower=0.01)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle(\"Concern 1: Extreme Effect Sizes\", fontsize=13, fontweight='bold')\n",
        "\n",
        "ax = axes[0, 0]\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    ldf = feat_df[feat_df['layer'] == layer]\n",
        "    ax.hist(ldf['abs_d'], bins=50, alpha=0.5, label=f\"L{layer}\", density=True)\n",
        "ax.axvline(5.0, color='red', linestyle='--', alpha=0.7, label='|d|=5')\n",
        "ax.set_xlabel(\"|Cohen's d|\"); ax.set_ylabel(\"Density\")\n",
        "ax.set_title(\"A. Effect size distribution by layer\")\n",
        "ax.set_xlim(0, min(feat_df['abs_d'].max() + 1, 15)); ax.legend(fontsize=7, ncol=2)\n",
        "\n",
        "ax = axes[0, 1]\n",
        "for layer in [5, 14, 27, 36]:\n",
        "    ldf = feat_df[feat_df['layer'] == layer]\n",
        "    ax.scatter(ldf['abs_mean_diff'], ldf['abs_d'], s=8, alpha=0.4, label=f\"L{layer}\")\n",
        "ax.set_xlabel(\"|Mean Activation Diff|\"); ax.set_ylabel(\"|Cohen's d|\")\n",
        "ax.set_title(\"B. Raw diff vs Cohen's d\")\n",
        "ax.axhline(5.0, color='red', linestyle='--', alpha=0.5); ax.legend(fontsize=8)\n",
        "\n",
        "ax = axes[1, 0]\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    ldf = feat_df[feat_df['layer'] == layer]\n",
        "    ax.scatter([LAYER_DEPTH[layer]] * len(ldf), ldf['implied_sd'], s=5, alpha=0.3)\n",
        "    ax.scatter(LAYER_DEPTH[layer], ldf['implied_sd'].median(), s=40, c='red', zorder=5, marker='_', linewidths=2)\n",
        "ax.set_xlabel(\"Layer Depth (%)\"); ax.set_ylabel(\"Implied Pooled SD\")\n",
        "ax.set_title(\"C. Feature activation spread by depth\"); ax.set_yscale('log')\n",
        "\n",
        "ax = axes[1, 1]\n",
        "if len(extreme_df) > 0:\n",
        "    pivot = extreme_df.groupby(['demographic', 'domain']).size().unstack(fill_value=0)\n",
        "    for demo in DEMOGRAPHICS:\n",
        "        if demo not in pivot.index: pivot.loc[demo] = 0\n",
        "    for dom in DOMAINS:\n",
        "        if dom not in pivot.columns: pivot[dom] = 0\n",
        "    pivot = pivot.reindex(DEMOGRAPHICS)[DOMAINS]\n",
        "    im = ax.imshow(pivot.values, aspect='auto', cmap='Reds')\n",
        "    ax.set_xticks(range(len(DOMAINS))); ax.set_xticklabels(DOMAINS, rotation=45, ha='right')\n",
        "    ax.set_yticks(range(len(DEMOGRAPHICS))); ax.set_yticklabels(DEMOGRAPHICS)\n",
        "    for i in range(len(DEMOGRAPHICS)):\n",
        "        for j in range(len(DOMAINS)):\n",
        "            ax.text(j, i, str(int(pivot.values[i, j])), ha='center', va='center', fontsize=9)\n",
        "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, \"No extreme features\", ha='center', va='center', transform=ax.transAxes)\n",
        "ax.set_title(\"D. Extreme features (|d|>5) by group\")\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern1_extreme_effects.png\", dpi=150, bbox_inches='tight')\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern1_extreme_effects.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_concern1_extreme_effects\")\n",
        "\n",
        "diag['concerns']['extreme_effects'] = {\n",
        "    'n_extreme_d5': int(len(extreme_df)),\n",
        "    'n_extreme_d10': int((feat_df['abs_d'] > 10).sum()),\n",
        "    'pct_extreme_d5': float(len(extreme_df) / len(feat_df) * 100),\n",
        "    'max_d': float(feat_df['abs_d'].max()),\n",
        "    'category_distribution': {str(k): int(v) for k, v in cat_counts.items()},\n",
        "}\n",
        "\n",
        "\n",
        "# --- Concern 2: Filtering funnel ---\n",
        "\n",
        "print(\"\\nConcern 2: Filtering funnel\")\n",
        "\n",
        "layer_clean_d, layer_noisy_d = {}, {}\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    lf = funnel_df[funnel_df['layer'] == layer]\n",
        "    layer_clean_d[layer] = lf['mean_abs_d_clean'].mean()\n",
        "    layer_noisy_d[layer] = lf['mean_abs_d_noisy'].mean()\n",
        "    ratio = layer_clean_d[layer] / max(layer_noisy_d[layer], 1e-6)\n",
        "    print(f\"  L{layer}: clean |d|={layer_clean_d[layer]:.4f}, noisy={layer_noisy_d[layer]:.4f}, ratio={ratio:.0f}x\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle(\"Concern 2: Feature Selection Funnel\", fontsize=13, fontweight='bold')\n",
        "\n",
        "ax = axes[0, 0]\n",
        "funnel_stages = []\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    lf = funnel_df[funnel_df['layer'] == layer]\n",
        "    funnel_stages.append({\n",
        "        'layer': f\"L{layer}\",\n",
        "        'No variance': lf['n_total_features'].iloc[0] - lf['n_has_variance'].mean(),\n",
        "        'Noisy': lf['n_has_variance'].mean() - lf['n_not_noisy'].mean(),\n",
        "        'Selected': lf['n_selected'].mean(),\n",
        "    })\n",
        "pd.DataFrame(funnel_stages).set_index('layer').plot(\n",
        "    kind='bar', stacked=True, ax=ax, color=['#d4d4d4', '#ffb3b3', '#2ecc71'])\n",
        "ax.set_ylabel(\"Feature Count\"); ax.set_title(\"A. Filtering funnel\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0); ax.legend(fontsize=8)\n",
        "\n",
        "ax = axes[0, 1]\n",
        "x = np.arange(len(ANALYSIS_LAYERS)); width = 0.35\n",
        "ax.bar(x - width/2, [layer_clean_d[l] for l in ANALYSIS_LAYERS], width, label='Selected', color='#2ecc71')\n",
        "ax.bar(x + width/2, [layer_noisy_d[l] for l in ANALYSIS_LAYERS], width, label='Rejected', color='#e74c3c')\n",
        "ax.set_xticks(x); ax.set_xticklabels([f\"L{l}\" for l in ANALYSIS_LAYERS])\n",
        "ax.set_ylabel(\"Mean |Cohen's d|\"); ax.set_title(\"B. Encoding: selected vs rejected\")\n",
        "ax.set_yscale('log'); ax.legend()\n",
        "\n",
        "ax = axes[1, 0]\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    lf = funnel_df[funnel_df['layer'] == layer]\n",
        "    ax.scatter([LAYER_DEPTH[layer]] * len(lf), lf['n_has_variance'], s=15, alpha=0.5, c='gray',\n",
        "               label='Has variance' if layer == 5 else '')\n",
        "    ax.scatter([LAYER_DEPTH[layer]] * len(lf), lf['n_selected'], s=15, alpha=0.8, c='green',\n",
        "               label='Selected' if layer == 5 else '')\n",
        "ax.set_xlabel(\"Layer Depth (%)\"); ax.set_ylabel(\"N Features\")\n",
        "ax.set_title(\"C. Variance vs selected per group\"); ax.legend()\n",
        "\n",
        "ax = axes[1, 1]\n",
        "ax.hist(funnel_df['mean_sign_agreement'], bins=30, color='steelblue', edgecolor='white')\n",
        "ax.axvline(0.7, color='red', linestyle='--', label='Threshold (0.7)')\n",
        "ax.set_xlabel(\"Mean Sign Agreement\"); ax.set_ylabel(\"Count\")\n",
        "ax.set_title(\"D. Sign agreement distribution\"); ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern2_funnel_distribution.png\", dpi=150, bbox_inches='tight')\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern2_funnel_distribution.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_concern2_funnel_distribution\")\n",
        "\n",
        "diag['concerns']['funnel'] = {\n",
        "    'mean_d_clean': float(np.mean(list(layer_clean_d.values()))),\n",
        "    'mean_d_noisy': float(np.mean(list(layer_noisy_d.values()))),\n",
        "    'clean_noisy_ratio': float(np.mean(list(layer_clean_d.values())) / max(np.mean(list(layer_noisy_d.values())), 1e-6)),\n",
        "}\n",
        "\n",
        "\n",
        "# --- Concern 3: Behavioral effects are layer-independent ---\n",
        "\n",
        "print(\"\\nConcern 3: Behavioral effects\")\n",
        "\n",
        "l5_effects = behav_df[behav_df['layer'] == 5].set_index(['demographic', 'domain'])['effect']\n",
        "l36_effects = behav_df[behav_df['layer'] == 36].set_index(['demographic', 'domain'])['effect']\n",
        "common_idx = l5_effects.index.intersection(l36_effects.index)\n",
        "\n",
        "if len(common_idx) > 0:\n",
        "    corr = l5_effects.loc[common_idx].corr(l36_effects.loc[common_idx])\n",
        "    max_diff = (l5_effects.loc[common_idx] - l36_effects.loc[common_idx]).abs().max()\n",
        "    print(f\"  L5 vs L36 correlation: {corr:.6f}\")\n",
        "    print(f\"  Max absolute difference: {max_diff:.6f}\")\n",
        "    print(f\"  Effects are {'identical' if max_diff < 0.001 else 'near-identical'} across layers\")\n",
        "\n",
        "diag['concerns']['behavioral_layer_independence'] = {\n",
        "    'l5_l36_correlation': float(corr) if len(common_idx) > 0 else None,\n",
        "    'max_layer_difference': float(max_diff) if len(common_idx) > 0 else None,\n",
        "}\n",
        "\n",
        "print(\"\\n  Canonical effects (layer-independent):\")\n",
        "for _, r in behav_df[behav_df['layer'] == ANALYSIS_LAYERS[0]].iterrows():\n",
        "    sig = \"***\" if r['p_value'] < 0.001 else \"**\" if r['p_value'] < 0.01 else \"*\" if r['p_value'] < 0.05 else \"ns\"\n",
        "    print(f\"    {r['demographic']}×{r['domain']:<14} {r['effect']:>+.4f} {sig}\")\n",
        "\n",
        "\n",
        "# --- Concern 4: Metric divergence ---\n",
        "\n",
        "print(\"\\nConcern 4: Metric divergence\")\n",
        "\n",
        "metric_summary = []\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    ldf = feat_df[feat_df['layer'] == layer]\n",
        "    metric_summary.append({\n",
        "        'layer': layer, 'depth': LAYER_DEPTH[layer],\n",
        "        'mean_abs_d': ldf['abs_d'].mean(), 'median_abs_d': ldf['abs_d'].median(),\n",
        "        'mean_abs_diff': ldf['abs_mean_diff'].mean(), 'mean_implied_sd': ldf['implied_sd'].median(),\n",
        "        'n_features': len(ldf),\n",
        "    })\n",
        "metric_df = pd.DataFrame(metric_summary)\n",
        "\n",
        "for _, r in metric_df.iterrows():\n",
        "    print(f\"  L{int(r['layer'])}: |d|={r['mean_abs_d']:.3f}, |Δ|={r['mean_abs_diff']:.2f}, SD={r['mean_implied_sd']:.2f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "fig.suptitle(\"Concern 4: Raw Activation Diff vs Cohen's d\", fontsize=13, fontweight='bold')\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.plot(metric_df['depth'], metric_df['mean_abs_d'], 'o-', color='#2c3e50', linewidth=2, label=\"|Cohen's d|\")\n",
        "ax1.set_xlabel(\"Depth (%)\"); ax1.set_ylabel(\"|Cohen's d|\", color='#2c3e50')\n",
        "ax2_twin = ax1.twinx()\n",
        "ax2_twin.plot(metric_df['depth'], metric_df['mean_abs_diff'], 's--', color='#e74c3c', linewidth=2, label=\"|Activation Δ|\")\n",
        "ax2_twin.set_ylabel(\"|Activation Δ|\", color='#e74c3c')\n",
        "ax1.set_title(\"A. Divergent metrics\")\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=8, loc='center left')\n",
        "\n",
        "ax = axes[1]\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    ldf = feat_df[feat_df['layer'] == layer]\n",
        "    parts = ax.violinplot([ldf['implied_sd'].values], positions=[LAYER_DEPTH[layer]],\n",
        "                          widths=6, showmedians=True, showextrema=False)\n",
        "    color = 'darkorange' if layer in IT_LAYERS else 'steelblue'\n",
        "    for pc in parts['bodies']: pc.set_facecolor(color); pc.set_alpha(0.6)\n",
        "    parts['cmedians'].set_color('red')\n",
        "ax.set_xlabel(\"Depth (%)\"); ax.set_ylabel(\"Implied Pooled SD\")\n",
        "ax.set_title(\"B. Activation spread increases with depth\"); ax.set_yscale('log')\n",
        "\n",
        "ax = axes[2]\n",
        "d_norm = (metric_df['mean_abs_d'] - metric_df['mean_abs_d'].min()) / (metric_df['mean_abs_d'].max() - metric_df['mean_abs_d'].min())\n",
        "diff_norm = (metric_df['mean_abs_diff'] - metric_df['mean_abs_diff'].min()) / (metric_df['mean_abs_diff'].max() - metric_df['mean_abs_diff'].min())\n",
        "sd_norm = (metric_df['mean_implied_sd'] - metric_df['mean_implied_sd'].min()) / (metric_df['mean_implied_sd'].max() - metric_df['mean_implied_sd'].min())\n",
        "ax.plot(metric_df['depth'], d_norm, 'o-', label=\"|d| (↓)\", linewidth=2)\n",
        "ax.plot(metric_df['depth'], diff_norm, 's--', label=\"|Δ| (↑)\", linewidth=2)\n",
        "ax.plot(metric_df['depth'], sd_norm, '^:', label=\"SD (↑↑)\", linewidth=2)\n",
        "ax.set_xlabel(\"Depth (%)\"); ax.set_ylabel(\"Normalized [0, 1]\")\n",
        "ax.set_title(\"C. SD grows faster than mean diff\"); ax.legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern4_metric_divergence.png\", dpi=150, bbox_inches='tight')\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern4_metric_divergence.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_concern4_metric_divergence\")\n",
        "\n",
        "diag['concerns']['metric_divergence'] = {\n",
        "    'early_mean_d': float(metric_df[metric_df['layer'].isin([5, 9, 14])]['mean_abs_d'].mean()),\n",
        "    'late_mean_d': float(metric_df[metric_df['layer'].isin([27, 32, 36])]['mean_abs_d'].mean()),\n",
        "    'early_mean_diff': float(metric_df[metric_df['layer'].isin([5, 9, 14])]['mean_abs_diff'].mean()),\n",
        "    'late_mean_diff': float(metric_df[metric_df['layer'].isin([27, 32, 36])]['mean_abs_diff'].mean()),\n",
        "}\n",
        "\n",
        "\n",
        "# --- Concern 5: L20 IT SAE dip ---\n",
        "\n",
        "print(\"\\nConcern 5: L20 (IT SAE) dip\")\n",
        "\n",
        "l18_d = metric_df[metric_df['layer'] == 18]['mean_abs_d'].values[0]\n",
        "l27_d = metric_df[metric_df['layer'] == 27]['mean_abs_d'].values[0]\n",
        "l20_d = metric_df[metric_df['layer'] == 20]['mean_abs_d'].values[0]\n",
        "interp_depth = (LAYER_DEPTH[20] - LAYER_DEPTH[18]) / (LAYER_DEPTH[27] - LAYER_DEPTH[18])\n",
        "expected_d = l18_d + interp_depth * (l27_d - l18_d)\n",
        "\n",
        "l20_n = metric_df[metric_df['layer'] == 20]['n_features'].values[0]\n",
        "expected_n = metric_df[metric_df['layer'] == 18]['n_features'].values[0] + \\\n",
        "    interp_depth * (metric_df[metric_df['layer'] == 27]['n_features'].values[0] -\n",
        "                    metric_df[metric_df['layer'] == 18]['n_features'].values[0])\n",
        "\n",
        "print(f\"  Observed |d|={l20_d:.3f}, expected={expected_d:.3f} (dev={l20_d - expected_d:+.3f})\")\n",
        "print(f\"  Observed N={l20_n:.0f}, expected={expected_n:.0f}\")\n",
        "\n",
        "demo_deviations = []\n",
        "for demo in DEMOGRAPHICS:\n",
        "    d18 = feat_df[(feat_df['layer'] == 18) & (feat_df['demographic'] == demo)]['abs_d'].mean()\n",
        "    d20 = feat_df[(feat_df['layer'] == 20) & (feat_df['demographic'] == demo)]['abs_d'].mean()\n",
        "    d27 = feat_df[(feat_df['layer'] == 27) & (feat_df['demographic'] == demo)]['abs_d'].mean()\n",
        "    demo_deviations.append(d20 - (d18 + interp_depth * (d27 - d18)))\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
        "fig.suptitle(\"Concern 5: Layer 20 (IT SAE) Investigation\", fontsize=13, fontweight='bold')\n",
        "\n",
        "ax = axes[0]\n",
        "depths = [LAYER_DEPTH[l] for l in ANALYSIS_LAYERS]\n",
        "d_vals = [metric_df[metric_df['layer'] == l]['mean_abs_d'].values[0] for l in ANALYSIS_LAYERS]\n",
        "ax.plot(depths, d_vals, 'o-', color='steelblue', linewidth=2, zorder=2)\n",
        "for i, l in enumerate(ANALYSIS_LAYERS):\n",
        "    if l in IT_LAYERS:\n",
        "        ax.scatter([depths[i]], [d_vals[i]], s=120, c='darkorange', zorder=5,\n",
        "                   edgecolors='red', linewidths=2, label='IT SAE (L20)')\n",
        "ax.axhline(expected_d, color='gray', linestyle=':', alpha=0.5, label=f'Interpolated ({expected_d:.2f})')\n",
        "ax.set_xlabel(\"Depth (%)\"); ax.set_ylabel(\"Mean |Cohen's d|\")\n",
        "ax.set_title(\"A. Encoding gradient — L20 below trend\"); ax.legend(fontsize=8)\n",
        "\n",
        "ax = axes[1]\n",
        "for demo in DEMOGRAPHICS:\n",
        "    demo_d = [feat_df[(feat_df['layer'] == l) & (feat_df['demographic'] == demo)]['abs_d'].mean()\n",
        "              if len(feat_df[(feat_df['layer'] == l) & (feat_df['demographic'] == demo)]) > 0 else 0\n",
        "              for l in ANALYSIS_LAYERS]\n",
        "    ax.plot(depths, demo_d, 'o-', label=demo, markersize=4, linewidth=1.5)\n",
        "ax.axvline(LAYER_DEPTH[20], color='darkorange', linestyle='--', alpha=0.7)\n",
        "ax.set_xlabel(\"Depth (%)\"); ax.set_ylabel(\"Mean |Cohen's d|\")\n",
        "ax.set_title(\"B. Per-demographic gradient\"); ax.legend(fontsize=7, ncol=2)\n",
        "\n",
        "ax = axes[2]\n",
        "n_feats = [metric_df[metric_df['layer'] == l]['n_features'].values[0] for l in ANALYSIS_LAYERS]\n",
        "ax.bar([f\"L{l}\" for l in ANALYSIS_LAYERS], n_feats,\n",
        "       color=['darkorange' if l in IT_LAYERS else 'steelblue' for l in ANALYSIS_LAYERS])\n",
        "ax.axhline(expected_n, color='gray', linestyle=':', alpha=0.5)\n",
        "ax.set_ylabel(\"N Features\"); ax.set_title(\"C. Feature count by layer\")\n",
        "for i, n in enumerate(n_feats):\n",
        "    ax.text(i, n + 3, str(int(n)), ha='center', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern5_l20_investigation.png\", dpi=150, bbox_inches='tight')\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern5_l20_investigation.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_concern5_l20_investigation\")\n",
        "\n",
        "diag['concerns']['l20_it_dip'] = {\n",
        "    'observed_d': float(l20_d), 'expected_d': float(expected_d),\n",
        "    'deviation': float(l20_d - expected_d),\n",
        "    'observed_n': int(l20_n), 'expected_n': float(expected_n),\n",
        "}\n",
        "\n",
        "\n",
        "# --- Concern 6: Domain specificity (proxy null baseline) ---\n",
        "\n",
        "print(\"\\nConcern 6: Domain specificity\")\n",
        "\n",
        "_sel_path = DATA_DIR / \"selected_features.json\"\n",
        "with open(_sel_path) as f:\n",
        "    _selected = json.load(f)\n",
        "\n",
        "def _get_feature_set(sel, layer_str, key):\n",
        "    if not isinstance(sel, dict): return set()\n",
        "    layer_data = sel.get(layer_str, {})\n",
        "    if not isinstance(layer_data, dict): return set()\n",
        "    entry = layer_data.get(key, {})\n",
        "    if isinstance(entry, dict): return set(entry.get('features', []))\n",
        "    elif isinstance(entry, list): return set(entry)\n",
        "    return set()\n",
        "\n",
        "domain_spec = []\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    layer_data = _selected.get(str(layer), {})\n",
        "    if not isinstance(layer_data, dict): continue\n",
        "    for demo in DEMOGRAPHICS:\n",
        "        demo_feats_by_domain = {}\n",
        "        for dom in DOMAINS:\n",
        "            demo_feats_by_domain[dom] = _get_feature_set(_selected, str(layer), f\"{demo}_{dom}\")\n",
        "\n",
        "        domains_with_feats = [d for d in DOMAINS if len(demo_feats_by_domain[d]) > 0]\n",
        "        jaccards = []\n",
        "        for i, d1 in enumerate(domains_with_feats):\n",
        "            for d2 in domains_with_feats[i+1:]:\n",
        "                s1, s2 = demo_feats_by_domain[d1], demo_feats_by_domain[d2]\n",
        "                if len(s1 | s2) > 0:\n",
        "                    jaccards.append(len(s1 & s2) / len(s1 | s2))\n",
        "\n",
        "        if len(domains_with_feats) >= 3:\n",
        "            all_dom = set.intersection(*[demo_feats_by_domain[d] for d in domains_with_feats])\n",
        "            any_dom = set.union(*[demo_feats_by_domain[d] for d in domains_with_feats])\n",
        "            pct_generic = len(all_dom) / max(len(any_dom), 1) * 100\n",
        "        else:\n",
        "            pct_generic = 0\n",
        "\n",
        "        domain_spec.append({\n",
        "            'layer': layer, 'demographic': demo,\n",
        "            'mean_jaccard': np.mean(jaccards) if jaccards else 0,\n",
        "            'pct_generic': pct_generic,\n",
        "        })\n",
        "\n",
        "spec_df = pd.DataFrame(domain_spec)\n",
        "\n",
        "for layer in ANALYSIS_LAYERS:\n",
        "    ldf = spec_df[spec_df['layer'] == layer]\n",
        "    print(f\"  L{layer}: Jaccard={ldf['mean_jaccard'].mean():.3f}, generic={ldf['pct_generic'].mean():.0f}%\")\n",
        "\n",
        "jaccard_matrices = {}\n",
        "for demo in DEMOGRAPHICS:\n",
        "    jmat = np.zeros((len(DOMAINS), len(DOMAINS)))\n",
        "    for i, d1 in enumerate(DOMAINS):\n",
        "        for j, d2 in enumerate(DOMAINS):\n",
        "            s1 = _get_feature_set(_selected, '36', f\"{demo}_{d1}\")\n",
        "            s2 = _get_feature_set(_selected, '36', f\"{demo}_{d2}\")\n",
        "            if i == j: jmat[i, j] = 1.0\n",
        "            elif len(s1 | s2) > 0: jmat[i, j] = len(s1 & s2) / len(s1 | s2)\n",
        "    jaccard_matrices[demo] = jmat\n",
        "avg_jmat = np.mean([jaccard_matrices[d] for d in DEMOGRAPHICS], axis=0)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle(\"Concern 6: Domain Specificity — Proxy Null Baseline\", fontsize=13, fontweight='bold')\n",
        "\n",
        "ax = axes[0]\n",
        "im = ax.imshow(avg_jmat, cmap='YlOrRd', vmin=0, vmax=1, aspect='auto')\n",
        "ax.set_xticks(range(len(DOMAINS))); ax.set_xticklabels(DOMAINS, rotation=45, ha='right')\n",
        "ax.set_yticks(range(len(DOMAINS))); ax.set_yticklabels(DOMAINS)\n",
        "for i in range(len(DOMAINS)):\n",
        "    for j in range(len(DOMAINS)):\n",
        "        ax.text(j, i, f\"{avg_jmat[i, j]:.2f}\", ha='center', va='center', fontsize=9)\n",
        "plt.colorbar(im, ax=ax, shrink=0.8, label='Jaccard')\n",
        "ax.set_title(\"A. Cross-domain overlap (L36)\")\n",
        "\n",
        "ax = axes[1]\n",
        "for demo in DEMOGRAPHICS:\n",
        "    ddf = spec_df[spec_df['demographic'] == demo]\n",
        "    vals = [ddf[ddf['layer'] == l]['pct_generic'].values[0]\n",
        "            if len(ddf[ddf['layer'] == l]) > 0 else 0 for l in ANALYSIS_LAYERS]\n",
        "    ax.plot([LAYER_DEPTH[l] for l in ANALYSIS_LAYERS], vals, 'o-', label=demo, markersize=4)\n",
        "ax.set_xlabel(\"Depth (%)\"); ax.set_ylabel(\"% Generic Features\")\n",
        "ax.set_title(\"B. Domain generality by depth\"); ax.legend(fontsize=8); ax.set_ylim(0, 100)\n",
        "\n",
        "ax = axes[2]\n",
        "for dom_idx, dom in enumerate(DOMAINS):\n",
        "    demo_feats = {demo: _get_feature_set(_selected, '36', f\"{demo}_{dom}\") for demo in DEMOGRAPHICS}\n",
        "    jaccards = []\n",
        "    for i, d1 in enumerate(DEMOGRAPHICS):\n",
        "        for d2 in DEMOGRAPHICS[i+1:]:\n",
        "            if len(demo_feats[d1] | demo_feats[d2]) > 0:\n",
        "                jaccards.append(len(demo_feats[d1] & demo_feats[d2]) / len(demo_feats[d1] | demo_feats[d2]))\n",
        "    ax.bar(dom_idx, np.mean(jaccards) if jaccards else 0, color='steelblue')\n",
        "ax.set_xticks(range(len(DOMAINS))); ax.set_xticklabels(DOMAINS, rotation=45, ha='right')\n",
        "ax.set_ylabel(\"Mean Jaccard\"); ax.set_title(\"C. Cross-demographic overlap (L36)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern6_domain_specificity.png\", dpi=150, bbox_inches='tight')\n",
        "fig.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern6_domain_specificity.pdf\", bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"  Saved: fig_concern6_domain_specificity\")\n",
        "\n",
        "diag['concerns']['domain_specificity'] = {\n",
        "    'mean_jaccard_l36': float(spec_df[spec_df['layer'] == 36]['mean_jaccard'].mean()),\n",
        "    'mean_pct_generic_l36': float(spec_df[spec_df['layer'] == 36]['pct_generic'].mean()),\n",
        "}\n",
        "\n",
        "# Save diagnostics\n",
        "with open(OUTPUT_DIR / \"data\" / \"concern_diagnostics.json\", 'w') as f:\n",
        "    json.dump(diag, f, indent=2)\n",
        "\n",
        "print(\"\\nExtraction-side diagnostics done.\")"
      ],
      "metadata": {
        "id": "bF0gyP0VyaHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  concern diagnostics — causal validation (concern 7)\n",
        "\n",
        "import ast\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "print(\"Concern 7: Causal validation diagnostics\")\n",
        "\n",
        "CAUSAL_DIR = None\n",
        "for candidate in [\n",
        "    BASE_DIR / \"causal_validation_final\",\n",
        "    BASE_DIR / \"outputs_gemma_replication\" / \"causal_validation_final\",\n",
        "]:\n",
        "    if candidate.exists() and (candidate / \"validation_results.csv\").exists():\n",
        "        CAUSAL_DIR = candidate; break\n",
        "if CAUSAL_DIR is None:\n",
        "    for match in BASE_DIR.rglob(\"validation_results.csv\"):\n",
        "        CAUSAL_DIR = match.parent; break\n",
        "\n",
        "if CAUSAL_DIR is None:\n",
        "    print(\"  Causal data not found — skipping\")\n",
        "else:\n",
        "    print(f\"  Causal data: {CAUSAL_DIR}\")\n",
        "\n",
        "    causal_df = pd.read_csv(CAUSAL_DIR / \"validation_results.csv\")\n",
        "    causal_top = pd.read_csv(CAUSAL_DIR / \"top_features.csv\")\n",
        "    excl_path = CAUSAL_DIR / \"exclusion_log.csv\"\n",
        "    causal_excl = pd.read_csv(excl_path) if excl_path.exists() else pd.DataFrame()\n",
        "\n",
        "    has_method = 'feature_method' in causal_df.columns\n",
        "    def per_pair(df):\n",
        "        return df[df['feature_method'] == 'per_pair'] if has_method else df\n",
        "\n",
        "    print(f\"  Results: {len(causal_df)} rows, top features: {len(causal_top)}\")\n",
        "\n",
        "    # --- 7a: Pair filtering consistency ---\n",
        "\n",
        "    print(\"\\n  7a: Pair filtering consistency\")\n",
        "    pairs_per_layer = {}\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        pairs_per_layer[l] = per_pair(causal_df[causal_df['layer'] == l])['pair_key'].nunique()\n",
        "\n",
        "    all_same = len(set(pairs_per_layer.values())) == 1\n",
        "    print(f\"    Identical across layers: {all_same} ({list(pairs_per_layer.values())[0] if all_same else pairs_per_layer})\")\n",
        "\n",
        "    if len(causal_excl) > 0:\n",
        "        excl_pivot = causal_excl.groupby(['demographic', 'domain']).size().reset_index(name='excluded')\n",
        "        total_per_cond = causal_df.groupby(['demographic', 'domain'])['pair_key'].nunique().reset_index(name='valid')\n",
        "        merged = excl_pivot.merge(total_per_cond, on=['demographic', 'domain'], how='outer').fillna(0)\n",
        "        merged['total'] = merged['excluded'] + merged['valid']\n",
        "        merged['excl_rate'] = merged['excluded'] / merged['total'] * 100\n",
        "        high_excl = merged[merged['excl_rate'] > 60]\n",
        "        if len(high_excl) > 0:\n",
        "            print(f\"    High exclusion (>60%): {len(high_excl)} conditions\")\n",
        "            for _, row in high_excl.iterrows():\n",
        "                print(f\"      {row['demographic']}×{row['domain']}: {row['excl_rate']:.0f}%\")\n",
        "        else:\n",
        "            print(\"    No conditions with >60% exclusion\")\n",
        "\n",
        "    diag['concerns']['7a_pair_filtering'] = {\n",
        "        'pairs_per_layer': pairs_per_layer,\n",
        "        'identical': all_same,\n",
        "    }\n",
        "\n",
        "    # --- 7b: Encoding–causal dissociation ---\n",
        "\n",
        "    print(\"\\n  7b: Encoding–causal dissociation\")\n",
        "\n",
        "    encoding_by_layer = {}\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        layer_feats = feat_df[feat_df['layer'] == l]\n",
        "        if len(layer_feats) > 0:\n",
        "            encoding_by_layer[l] = layer_feats['abs_d'].mean()\n",
        "\n",
        "    causal_by_layer = {}\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        vals = per_pair(causal_df[(causal_df['layer'] == l) & (causal_df['K'] == 50)])\n",
        "        patch_vals = vals[vals['condition'] == 'patch_same']['recovery_ev'].dropna()\n",
        "        if len(patch_vals) > 0:\n",
        "            causal_by_layer[l] = patch_vals.mean()\n",
        "\n",
        "    shared_layers = sorted(set(encoding_by_layer.keys()) & set(causal_by_layer.keys()))\n",
        "    enc_vals = [encoding_by_layer[l] for l in shared_layers]\n",
        "    cau_vals = [causal_by_layer[l] for l in shared_layers]\n",
        "\n",
        "    r_enc_cau, p_enc_cau = stats.pearsonr(enc_vals, cau_vals) if len(shared_layers) >= 4 else (np.nan, np.nan)\n",
        "    if not np.isnan(r_enc_cau):\n",
        "        print(f\"    Correlation: r={r_enc_cau:.3f}, p={p_enc_cau:.4f}\")\n",
        "\n",
        "    early = [l for l in shared_layers if LAYER_DEPTH[l] < 40]\n",
        "    late = [l for l in shared_layers if LAYER_DEPTH[l] > 60]\n",
        "    early_enc = np.mean([encoding_by_layer[l] for l in early]) if early else np.nan\n",
        "    late_enc = np.mean([encoding_by_layer[l] for l in late]) if late else np.nan\n",
        "    early_cau = np.mean([causal_by_layer[l] for l in early]) if early else np.nan\n",
        "    late_cau = np.mean([causal_by_layer[l] for l in late]) if late else np.nan\n",
        "\n",
        "    ratio_enc = early_enc / max(late_enc, 1e-6)\n",
        "    ratio_cau = float('inf') if early_cau < 0 else late_cau / max(early_cau, 1e-6)\n",
        "\n",
        "    print(f\"    Early (<40%): enc |d|={early_enc:.2f}, recovery={early_cau:.1%}\")\n",
        "    print(f\"    Late  (>60%): enc |d|={late_enc:.2f}, recovery={late_cau:.1%}\")\n",
        "    print(f\"    Encoding ratio (early/late): {ratio_enc:.2f}x\")\n",
        "    print(f\"    Causal ratio (late/early): {'∞' if ratio_cau == float('inf') else f'{ratio_cau:.1f}x'}\")\n",
        "\n",
        "    # Dissociation figure\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    ax = axes[0]\n",
        "    enc_max = max(enc_vals) if max(enc_vals) > 0 else 1\n",
        "    enc_norm = [v / enc_max * 100 for v in enc_vals]\n",
        "    cau_pct = [v * 100 for v in cau_vals]\n",
        "    layer_depths = [LAYER_DEPTH[l] for l in shared_layers]\n",
        "    ax.plot(layer_depths, enc_norm, 'o-', color='#3498db', linewidth=2.5, markersize=8, label='Encoding (norm)')\n",
        "    ax.plot(layer_depths, cau_pct, 's-', color='#e74c3c', linewidth=2.5, markersize=8, label='Recovery (%)')\n",
        "    for l in IT_LAYERS:\n",
        "        if l in shared_layers:\n",
        "            ax.axvline(x=LAYER_DEPTH[l], color='orange', linestyle='--', alpha=0.5)\n",
        "    ax.set_xlabel('Depth (%)'); ax.set_ylabel('Normalized Strength')\n",
        "    ax.set_title('A. Encoding vs Causal Across Depth', fontweight='bold'); ax.legend(fontsize=8)\n",
        "\n",
        "    ax = axes[1]\n",
        "    for l in shared_layers:\n",
        "        c = 'orange' if l in IT_LAYERS else '#2c3e50'\n",
        "        marker = 'D' if l in IT_LAYERS else 'o'\n",
        "        ax.scatter(encoding_by_layer[l], causal_by_layer[l] * 100, s=120, c=c, marker=marker,\n",
        "                   edgecolors='black', linewidth=1, zorder=3)\n",
        "        ax.annotate(f'L{l}', (encoding_by_layer[l], causal_by_layer[l] * 100),\n",
        "                    textcoords=\"offset points\", xytext=(6, 4), fontsize=9)\n",
        "    title = f'B. Scatter (r={r_enc_cau:.2f}, p={p_enc_cau:.3f})' if not np.isnan(r_enc_cau) else 'B. Scatter'\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xlabel('Encoding |d|'); ax.set_ylabel('Recovery (%)')\n",
        "\n",
        "    ax = axes[2]\n",
        "    x = np.arange(2); w = 0.35\n",
        "    ax.bar(x - w/2, [early_enc, late_enc], w, label='Encoding |d|', color='#3498db', edgecolor='black')\n",
        "    ax_r = ax.twinx()\n",
        "    ax_r.bar(x + w/2, [early_cau * 100, late_cau * 100], w, label='Recovery (%)', color='#e74c3c', edgecolor='black')\n",
        "    ax.set_xticks(x); ax.set_xticklabels(['Early\\n(<40%)', 'Late\\n(>60%)'])\n",
        "    ax.set_ylabel('Encoding |d|', color='#3498db')\n",
        "    ax_r.set_ylabel('Recovery (%)', color='#e74c3c')\n",
        "    ax.set_title('C. Early vs Late', fontweight='bold')\n",
        "    ax.legend(handles=[Patch(facecolor='#3498db', label='Encoding'), Patch(facecolor='#e74c3c', label='Recovery')],\n",
        "              fontsize=8, loc='upper left')\n",
        "\n",
        "    plt.suptitle('Concern 7b: Encoding–Causal Dissociation', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern7b_dissociation.png\")\n",
        "    plt.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern7b_dissociation.pdf\")\n",
        "    plt.close()\n",
        "    print(\"    Saved: fig_concern7b_dissociation\")\n",
        "\n",
        "    diag['concerns']['7b_dissociation'] = {\n",
        "        'encoding_by_layer': {str(k): float(v) for k, v in encoding_by_layer.items()},\n",
        "        'causal_by_layer': {str(k): float(v) for k, v in causal_by_layer.items()},\n",
        "        'correlation_r': float(r_enc_cau) if not np.isnan(r_enc_cau) else None,\n",
        "        'correlation_p': float(p_enc_cau) if not np.isnan(p_enc_cau) else None,\n",
        "        'early_encoding': float(early_enc), 'late_encoding': float(late_enc),\n",
        "        'early_causal': float(early_cau), 'late_causal': float(late_cau),\n",
        "    }\n",
        "\n",
        "    # --- 7c: Causal effect plausibility ---\n",
        "\n",
        "    print(\"\\n  7c: Effect plausibility\")\n",
        "\n",
        "    K = 50\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        l_df = causal_df[(causal_df['layer'] == l) & (causal_df['K'] == K)]\n",
        "        pp = per_pair(l_df)\n",
        "        same = pp[pp['condition'] == 'patch_same']['recovery_ev'].dropna()\n",
        "        rand_b = l_df[l_df['condition'] == 'patch_random_b']['recovery_ev'].dropna()\n",
        "        if len(same) < 5: continue\n",
        "        tag = \" (IT)\" if l in IT_LAYERS else \"\"\n",
        "        print(f\"    L{l}{tag}: same={same.mean():.1%}, rand_b={rand_b.mean():.1%}, gap={same.mean()-rand_b.mean():.1%}\")\n",
        "\n",
        "    all_patch = per_pair(causal_df[(causal_df['K'] == K) & (causal_df['condition'] == 'patch_same')])\n",
        "    over_100 = (all_patch['recovery_ev'] > 1.0).sum()\n",
        "    under_0 = (all_patch['recovery_ev'] < 0.0).sum()\n",
        "    total = len(all_patch)\n",
        "    print(f\"    Recovery >100%: {over_100}/{total} ({over_100/max(total,1)*100:.1f}%)\")\n",
        "    print(f\"    Recovery <0%:   {under_0}/{total} ({under_0/max(total,1)*100:.1f}%)\")\n",
        "\n",
        "    diag['concerns']['7c_plausibility'] = {\n",
        "        'over_100_pct': float(over_100 / max(total, 1) * 100),\n",
        "        'under_0_pct': float(under_0 / max(total, 1) * 100),\n",
        "    }\n",
        "\n",
        "    # --- 7d: Feature overlap (extraction vs causal) ---\n",
        "\n",
        "    print(\"\\n  7d: Feature overlap (extraction vs causal)\")\n",
        "\n",
        "    def safe_parse(x):\n",
        "        return ast.literal_eval(x) if isinstance(x, str) else x\n",
        "\n",
        "    _sel_path = DATA_DIR / \"selected_features.json\"\n",
        "    with open(_sel_path) as f:\n",
        "        _sel = json.load(f)\n",
        "\n",
        "    extraction_feats = {}\n",
        "    for layer_str, layer_data in _sel.items():\n",
        "        if not isinstance(layer_data, dict): continue\n",
        "        l = int(layer_str)\n",
        "        for key_str, feat_data in layer_data.items():\n",
        "            if isinstance(feat_data, dict) and 'features' in feat_data:\n",
        "                extraction_feats[(l, key_str)] = set(feat_data['features'][:50])\n",
        "\n",
        "    causal_top_parsed = causal_top.copy()\n",
        "    causal_top_parsed['features_list'] = causal_top_parsed['top_5_features'].apply(safe_parse)\n",
        "\n",
        "    causal_freq = defaultdict(lambda: Counter())\n",
        "    for _, row in causal_top_parsed.iterrows():\n",
        "        key = (row['layer'], f\"{row['demographic']}_{row['domain']}\")\n",
        "        for f in row['features_list']:\n",
        "            causal_freq[key][f] += 1\n",
        "\n",
        "    overlap_stats = []\n",
        "    for key in causal_freq:\n",
        "        if key not in extraction_feats: continue\n",
        "        top_causal = set([f for f, c in causal_freq[key].most_common(20)])\n",
        "        intersection = top_causal & extraction_feats[key]\n",
        "        jaccard = len(intersection) / max(len(top_causal | extraction_feats[key]), 1)\n",
        "        overlap_stats.append({'layer': key[0], 'condition': key[1],\n",
        "                              'n_intersection': len(intersection), 'jaccard': jaccard})\n",
        "\n",
        "    if overlap_stats:\n",
        "        overlap_df = pd.DataFrame(overlap_stats)\n",
        "        print(f\"    Mean Jaccard (causal top-20 vs extraction top-50): {overlap_df['jaccard'].mean():.3f}\")\n",
        "        for l in ANALYSIS_LAYERS:\n",
        "            ov = overlap_df[overlap_df['layer'] == l]\n",
        "            if len(ov) > 0:\n",
        "                print(f\"      L{l}: Jaccard={ov['jaccard'].mean():.3f}\")\n",
        "\n",
        "        diag['concerns']['7d_overlap'] = {\n",
        "            'mean_jaccard': float(overlap_df['jaccard'].mean()),\n",
        "            'by_layer': {str(l): float(overlap_df[overlap_df['layer'] == l]['jaccard'].mean())\n",
        "                         for l in ANALYSIS_LAYERS if len(overlap_df[overlap_df['layer'] == l]) > 0},\n",
        "        }\n",
        "\n",
        "    # --- 7e: Dose-response monotonicity ---\n",
        "\n",
        "    print(\"\\n  7e: Dose-response\")\n",
        "\n",
        "    k_values = [5, 10, 20, 50]\n",
        "    monotonic_count, total_layers = 0, 0\n",
        "    dose_response = {}\n",
        "\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        means = []\n",
        "        for k in k_values:\n",
        "            vals = per_pair(causal_df[(causal_df['layer'] == l) & (causal_df['K'] == k)])\n",
        "            patch = vals[vals['condition'] == 'patch_same']['recovery_ev'].dropna()\n",
        "            means.append(patch.mean() if len(patch) > 0 else np.nan)\n",
        "        dose_response[l] = means\n",
        "        valid = [m for m in means if not np.isnan(m)]\n",
        "        if len(valid) >= 3:\n",
        "            total_layers += 1\n",
        "            if all(valid[i] <= valid[i+1] for i in range(len(valid)-1)):\n",
        "                monotonic_count += 1\n",
        "\n",
        "    print(f\"    Monotonic: {monotonic_count}/{total_layers} layers\")\n",
        "\n",
        "    diag['concerns']['7e_dose_response'] = {\n",
        "        'monotonic': monotonic_count, 'total': total_layers,\n",
        "    }\n",
        "\n",
        "    # --- Combined causal figure ---\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "    # A: Recovery by layer\n",
        "    ax = axes[0, 0]\n",
        "    same_means, rand_means = [], []\n",
        "    for l in ANALYSIS_LAYERS:\n",
        "        pp = per_pair(causal_df[(causal_df['layer'] == l) & (causal_df['K'] == 50)])\n",
        "        same = pp[pp['condition'] == 'patch_same']['recovery_ev'].dropna()\n",
        "        rand = causal_df[(causal_df['layer'] == l) & (causal_df['K'] == 50) &\n",
        "                         (causal_df['condition'] == 'patch_random_a')]['recovery_ev'].dropna()\n",
        "        same_means.append(same.mean() * 100 if len(same) > 0 else 0)\n",
        "        rand_means.append(rand.mean() * 100 if len(rand) > 0 else 0)\n",
        "    x = np.arange(len(ANALYSIS_LAYERS)); w = 0.35\n",
        "    ax.bar(x - w/2, same_means, w, color=['orange' if l in IT_LAYERS else '#2ecc71' for l in ANALYSIS_LAYERS],\n",
        "           edgecolor='black', label='Same-pair')\n",
        "    ax.bar(x + w/2, rand_means, w, color='#95a5a6', edgecolor='black', label='Random-A')\n",
        "    ax.set_xticks(x); ax.set_xticklabels([f'L{l}\\n{LAYER_DEPTH[l]}%' for l in ANALYSIS_LAYERS], fontsize=7)\n",
        "    ax.set_ylabel('Recovery (%)'); ax.set_title('A. Same vs Random by Layer'); ax.legend(fontsize=7)\n",
        "\n",
        "    # B: Dose-response\n",
        "    ax = axes[0, 1]\n",
        "    for l_plot in [5, 36]:\n",
        "        if l_plot in dose_response:\n",
        "            ax.plot(k_values, [v*100 for v in dose_response[l_plot]], 'o-',\n",
        "                    label=f'L{l_plot} ({LAYER_DEPTH[l_plot]}%)', linewidth=2, markersize=7)\n",
        "    ax.set_xlabel('K'); ax.set_ylabel('Recovery (%)'); ax.set_title('B. Dose-Response'); ax.legend(fontsize=8)\n",
        "    ax.set_xticks(k_values)\n",
        "\n",
        "    # C: Recovery distribution at L36\n",
        "    ax = axes[0, 2]\n",
        "    best_patch = per_pair(causal_df[(causal_df['layer'] == 36) & (causal_df['K'] == 50) &\n",
        "                                    (causal_df['condition'] == 'patch_same')])\n",
        "    if len(best_patch) > 0:\n",
        "        rv = best_patch['recovery_ev'].dropna()\n",
        "        ax.hist(rv * 100, bins=30, color='#2ecc71', edgecolor='black', alpha=0.7)\n",
        "        ax.axvline(x=rv.mean() * 100, color='red', linestyle='--', linewidth=2, label=f'Mean={rv.mean():.1%}')\n",
        "        ax.axvline(x=0, color='black', linewidth=1); ax.axvline(x=100, color='gray', linestyle=':', linewidth=1)\n",
        "        ax.set_xlabel('Recovery (%)'); ax.set_ylabel('Count')\n",
        "        ax.set_title(f'C. Recovery Distribution (L36, n={len(rv)})'); ax.legend(fontsize=7)\n",
        "\n",
        "    # D: Exclusion rate\n",
        "    ax = axes[1, 0]\n",
        "    if len(causal_excl) > 0:\n",
        "        excl_by_demo = causal_excl.groupby('demographic').size()\n",
        "        valid_by_demo = per_pair(causal_df[causal_df['K'] == 50]).groupby('demographic')['pair_key'].nunique()\n",
        "        total_by_demo = excl_by_demo.add(valid_by_demo, fill_value=0)\n",
        "        rate_by_demo = excl_by_demo / total_by_demo * 100\n",
        "        ax.barh(rate_by_demo.index, rate_by_demo.values, color='#e74c3c', edgecolor='black')\n",
        "        ax.set_xlabel('Exclusion Rate (%)'); ax.set_xlim(0, 100)\n",
        "        for i, (demo, rate) in enumerate(rate_by_demo.items()):\n",
        "            ax.annotate(f'{rate:.0f}%', (rate, i), va='center', fontsize=9)\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No exclusion data', ha='center', va='center', transform=ax.transAxes)\n",
        "    ax.set_title('D. Exclusion Rate by Demo')\n",
        "\n",
        "    # E: Feature overlap\n",
        "    ax = axes[1, 1]\n",
        "    if overlap_stats:\n",
        "        layer_jaccards = [overlap_df[overlap_df['layer'] == l]['jaccard'].mean()\n",
        "                          if len(overlap_df[overlap_df['layer'] == l]) > 0 else 0\n",
        "                          for l in ANALYSIS_LAYERS]\n",
        "        ax.bar(range(len(ANALYSIS_LAYERS)), layer_jaccards,\n",
        "               color=['orange' if l in IT_LAYERS else '#8e44ad' for l in ANALYSIS_LAYERS], edgecolor='black')\n",
        "        ax.set_xticks(range(len(ANALYSIS_LAYERS)))\n",
        "        ax.set_xticklabels([f'L{l}' for l in ANALYSIS_LAYERS], fontsize=8)\n",
        "        ax.set_ylabel('Jaccard'); ax.set_title('E. Extraction↔Causal Overlap')\n",
        "        for i, j in enumerate(layer_jaccards):\n",
        "            ax.annotate(f'{j:.2f}', (i, j), ha='center', va='bottom', fontsize=7)\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No overlap data', ha='center', va='center', transform=ax.transAxes)\n",
        "        ax.set_title('E. Feature Overlap')\n",
        "\n",
        "    # F: Control hierarchy at L36\n",
        "    ax = axes[1, 2]\n",
        "    l36_df = causal_df[(causal_df['layer'] == 36) & (causal_df['K'] == 50)]\n",
        "    l36_pp = per_pair(l36_df)\n",
        "    hierarchy_data = []\n",
        "    for cond, label, color in [\n",
        "        ('patch_same', 'Same', '#2ecc71'), ('patch_random_a', 'Rand-A', '#7f8c8d'),\n",
        "        ('patch_cross', 'Cross', '#f39c12'), ('patch_random_b', 'Rand-B', '#bdc3c7'),\n",
        "    ]:\n",
        "        src = l36_pp if cond == 'patch_same' else l36_df\n",
        "        vals = src[src['condition'] == cond]['recovery_ev'].dropna()\n",
        "        if len(vals) > 0:\n",
        "            hierarchy_data.append((label, vals.mean(), vals.sem(), color))\n",
        "\n",
        "    if hierarchy_data:\n",
        "        labels, means, sems, colors = zip(*hierarchy_data)\n",
        "        ax.bar(range(len(means)), [m*100 for m in means], yerr=[s*100 for s in sems],\n",
        "               capsize=3, color=colors, edgecolor='black')\n",
        "        ax.set_xticks(range(len(labels))); ax.set_xticklabels(labels, fontsize=9)\n",
        "        ax.set_ylabel('Recovery (%)'); ax.axhline(y=0, color='black', linewidth=0.5)\n",
        "        for i, (_, m, _, _) in enumerate(hierarchy_data):\n",
        "            ax.annotate(f'{m:.1%}', (i, m*100), ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "    ax.set_title('F. Control Hierarchy (L36)')\n",
        "\n",
        "    plt.suptitle('Concern 7: Causal Validation Diagnostics', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern7_causal_diagnostics.png\")\n",
        "    plt.savefig(OUTPUT_DIR / \"figures\" / \"fig_concern7_causal_diagnostics.pdf\")\n",
        "    plt.close()\n",
        "    print(\"    Saved: fig_concern7_causal_diagnostics\")\n",
        "\n",
        "    # Save updated diagnostics\n",
        "    with open(OUTPUT_DIR / \"data\" / \"concern_diagnostics.json\", 'w') as f:\n",
        "        json.dump(diag, f, indent=2)\n",
        "\n",
        "    cau_str = \"∞\" if ratio_cau == float('inf') else f\"{ratio_cau:.1f}x\"\n",
        "    print(f\"\\n  Summary: dissociation enc {ratio_enc:.1f}x early / cau {cau_str} late, \"\n",
        "          f\"dose-response {monotonic_count}/{total_layers} monotonic\")\n",
        "\n",
        "print(\"\\nDiagnostics done.\")"
      ],
      "metadata": {
        "id": "ryZkkL6k0DHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}